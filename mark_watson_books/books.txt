Haskell Tutorial and Cookbook
Mark Watson
This book is for sale at http://leanpub.com/haskell-cookbook
This version was published on 2023-01-27
This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing
process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and
many iterations to get reader feedback, pivot until you have the right book and build traction once
you do.
© 2016 - 2023 Mark Watson
Contents
Cover Material, Copyright, and License . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Additional Material in the Second Edition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Request from the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Structure of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Code Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Functional Programming Requires a Different Mind Set . . . . . . . . . . . . . . . . . . . . .
eBooks Are Living Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Setting Up Your Development Environment
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Why Haskell? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Enjoy Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Section 1 - Tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Tutorial on Pure Haskell Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Interactive GHCi Shell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Introduction to Haskell Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Functions Are Pure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Parenthesis or the Special $ Character and Operator Precedence . . . . . . . . . . . .
Lazy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Understanding List Comprehensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Haskell Rules for Indenting Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Understanding let and where . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conditional do Expressions and Anonymous Functions . . . . . . . . . . . . . . . . . . . . .
Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
More on Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Comments on Dealing With Immutable Data and How to Structure Programs . . . . . . .
Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Testing Haskell Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Pure Haskell Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Tutorial on Impure Haskell Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CONTENTS
Hello IO () Monad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Note About >> and >>= Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Console IO Example with Stack Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . .
File IO
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Error Handling in Impure Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Network IO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Haskell Game Loop that Maintains State Functionally . . . . . . . . . . . . . . . . . . . . .
A More Detailed Look at Monads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Applicative Operators <$> and <*>: Finding Common Words in Files . . . . . . . . .
List Comprehensions Using the do Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Dealing With Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Debug.Trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Section 2 - Cookbook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Text Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CSV Spreadsheet Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
JSON Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Cleaning Natural Language Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Natural Language Processing Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Resolve Entities in Text to DBPedia URIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Bag of Words Classification Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Text Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Part of Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Natural Language Processing Wrap Up
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Linked Data and the Semantic Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The SPARQL Query Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
A Haskell HTTP Based SPARQL Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
Querying Remote SPARQL Endpoints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
Linked Data and Semantic Web Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Web Scraping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Using the Wreq Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Using the HandsomeSoup Library for Parsing HTML . . . . . . . . . . . . . . . . . . . . . . 111
Web Scraping Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Using Relational Databases
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
Database Access for Sqlite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
Database Access for Postgres . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Haskell Program to Play the Blackjack Card Game . . . . . . . . . . . . . . . . . . . . . . . . . 120
CONTENTS
Section 3 - Larger Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Knowledge Graph Creator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
Code Layout For the KGCreator Project and strategies for sharing Haskell code between
projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
The Main Event: Detecting Entities in Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
Utility Code for Generating RDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
Utility Code for Generating Cypher Input Data for Neo4J . . . . . . . . . . . . . . . . . . . . 145
Top Level API Code for Handling Knowledge Graph Data Generation . . . . . . . . . . . . 150
Wrapup for Automating the Creation of Knowledge Graphs
. . . . . . . . . . . . . . . . . . 152
Hybrid Haskell and Python Natural Language Processing . . . . . . . . . . . . . . . . . . . . . 153
Example Use of the Haskell NLP Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Setting up the Python NLP Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Understanding the Haskell NLP Client Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
Wrapup for Using the Python SpaCy NLP Service . . . . . . . . . . . . . . . . . . . . . . . . . 156
Hybrid Haskell and Python For Coreference Resolution . . . . . . . . . . . . . . . . . . . . . . 157
Installing the Python Coreference Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Understanding the Haskell Coreference Client Code . . . . . . . . . . . . . . . . . . . . . . . 158
Wrapup for Using the Python Coreference NLP Service . . . . . . . . . . . . . . . . . . . . . 160
Book Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
Appendix A - Haskell Tools Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Emacs Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
Do you want more of an IDE-like Development Environment? . . . . . . . . . . . . . . . . . 163
hlint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
Cover Material, Copyright, and
License
Copyright 2016 Mark Watson. All rights reserved. This book may be shared using the Creative
Commons “share and share alike, no modifications, no commercial reuse” license.
This eBook will be updated occasionally so please periodically check the leanpub.com web page for
this book¹ for updates.
Please visit the author’s website².
If you found a copy of this book on the web and find it of value then please consider buying a copy
at leanpub.com/haskell-cookbook³ to support the author and fund work for future updates.
¹https://leanpub.com/haskell-cookbook
²http://markwatson.com
³https://leanpub.com/haskell-cookbook
Preface
This is the preface to the new second edition released summer of 2019.
It took me over a year learning Haskell before I became comfortable with the language because I tried
to learn too much at once. There are two aspects to Haskell development: writing pure functional
code and writing impure code that needs to maintain state and generally deal with the world non-
deterministically. I usually find writing pure functional Haskell code to be easy and a lot of fun.
Writing impure code is sometimes a different story. This is why I am taking a different approach
to teaching you to program in Haskell: we begin techniques for writing concise, easy to read and
understand efficient pure Haskell code. I will then show you patterns for writing impure code to
deal with file IO, network IO, database access, and web access. You will see that the impure code
tends to be (hopefully!) a small part of your application and is isolated in the impure main program
and in a few impure helper functions used by the main program. Finally, we will look at a few larger
Haskell programs.
Additional Material in the Second Edition
In addition to updating the introduction to Haskell and tutorial material, I have added a few larger
projects to the second edition.
The project knowledge_graph_creator helps to automate the process of creating Knowledge
Graphs from raw text input and generates data for both the Neo4J open source graph database
as well as RDF data for use in semantic web and linked data applications.
The project HybridHaskellPythonNlp is a hybrid project: a Python web service that provides access
to the SpaCy natural language processing (NLP) library and select NLP deep learning models and a
Haskell client for accessing this service. It sometimes makes sense to develop polyglot applications
(i.e., applications written in multiple programming languages) to take advantage of language specific
libraries and frameworks. We will also use a similar hybrid example HybridHaskellPythonCore-
fAnaphoraResolution that uses another deep learning model to replace pronouns in text with the
original nouns that the pronouns refer to. This is a common processing step for systems that extract
information from text.
A Request from the Author
I spent time writing this book to help you, dear reader. I release this book under the Creative
Commons “share and share alike, no modifications, no commercial reuse” license and set the
minimum purchase price to $6.00 in order to reach the most readers. You can also read this (and
Preface
all of my books) for free on my my website⁴. Under this license you can share a PDF version of this
book with your friends and coworkers.
If you would like to support my work please consider purchasing my books on Leanpub⁵ and star
my git repositories that you find useful on GitHub⁶. You can also interact with me on social media
on Mastodon⁷ and Twitter⁸.
I enjoy writing and your support helps me write new editions and updates for my books and to
develop new book projects. Thank you!
Structure of the Book
The first section of this book contains two chapters:
• A tutorial on pure Haskell development: no side effects.
• A tutorial on impure Haskell development: dealing with the world (I/O, network access,
database access, etc.). This includes examples of file IO and network programming as well
as writing short applications: a mixture of pure and impure Haskell code.
After working through these tutorial chapters you will understand enough of Haskell development
to understand and be able to make modifications for your own use of the cookbook examples in
the second section. Some of the general topics will be covered again in the second book section
that contains longer sample applications. For example, you will learn the basics for interacting with
Sqlite and Postgres databases in the tutorial on impure Haskell code but you will see a much longer
example later in the book when I provide code that implements a natural language processing (NLP)
interface to relational databases.
The second section contains the following recipes implemented as complete programs:
• Textprocessing CSV Files
• Textprocessing JSON Files
• Natural Language Processing (NLP) interface to relational databases, including annotating
English text with Wikipedia/DBPedia URIs for entities in the original text. Entities can be
people, places, organizations, etc.
• Accessing and Using Linked Data
• Querying Semantic Web RDF Data Sources
• Web scraping data on web sites
• Using Sqlite and Postgres relational databases
• Play a simple version of Blackjack card game
A new third section (added in 2019 for the second edition) has three examples that were derived by
my own work.
⁴https://markwatson.com/books
⁵https://leanpub.com/u/markwatson
⁶https://github.com/mark-watson?tab=repositories&q=&type=public
⁷https://mastodon.social/@mark_watson
⁸https://twitter.com/mark_l_watson
Preface
Code Examples
The code examples in this book are licensed under two software licenses and you can choose the
license that works best for your needs: Apache 2 and GPL 3. To be clear, you can use the examples in
commercial projects under the Apache 2 license and if you like to write Free (Libre) software then
use the GPL 3 license.
We will use stack as a build system for all code examples. The code examples are provided as 22
separate stack based projects. These examples are found on github⁹.
Functional Programming Requires a Different Mind Set
You will learn to look at problems differently when you write functional programs. We will use
a bottom up approach in most of the examples in this book. I like to start by thinking of the
problem domain and decide how I can represent the data required for the problem at hand. I prefer
to use native data structures. This is the opposite approach to object oriented development where
considerable analysis effort and coding effort is required to define class hierachies to represent data.
In most of the code we use simple native data types like lists and maps.
Once we decide how to represent data for a program we then start designing and implementing
simple functions to operate on and transform data. If we find ourselves writing functions that are
too long or too complex, we can break up code into simpler functions. Haskell has good language
support for composing simple functions into more complex operations.
I have spent many years engaged in object oriented programming starting with CLOS for Common
Lisp, C++, Java, and Ruby. I now believe that in general, and I know it is sometimes a bad
idea to generalize too much, functional programming is a superior paradigm to object oriented
programming. Convincing you of this belief is one of my goals in writing this book!
eBooks Are Living Documents
I wrote printed books for publishers like Springer-Verlag, McGraw-Hill, and Morgan Kaufman before
I started self-publishing my own books. I prefer eBooks because I can update already published books
and update the code examples for eBooks.
I encourage you to periodically check for free updates to both this book and the code examples on
the leanpub.com web page for this book¹⁰.
⁹https://github.com/mark-watson/haskell_tutorial_cookbook_examples
¹⁰https://leanpub.com/haskell-cookbook
Preface
Setting Up Your Development Environment
I strongly recommend that you use the stack tool from the stack website¹¹. This web site has
instructions for installing stack on OS X, Windows, and Linux. If you don’t have stack installed yet
please do so now and follow the “getting started” instructions for creating a small project. Appendix
A contains material to help get you set up.
It is important for you to learn the basics of using stack before jumping into this book because I
have set up all of the example programs using stack.
The github repository for the examples in this book is located at github.com/mark-watson/haskell_-
tutorial_cookbook_examples¹².
Many of the example listings for code examples are partial or full listing of files in my github
repository. I show the file name, the listing, and the output. To experiment with the example yourself
you need to load it and execute the main function; for example, if the example file is TestSqLite1.hs
in the sub-directory Database, then from the top level directory in the git repository for the book
examples you would do the following:
$ haskell_tutorial_cookbook_examples git:(master) > cd Database
$ Database git:(master) > stack build --exec ghci
GHCi, version 7.10.3: http://www.haskell.org/ghc/
:? for help
Prelude> :l TestSqLite1
[1 of 1] Compiling Main
( TestSqLite1.hs, interpreted )
Ok, modules loaded: Main.
*Main> main
"Table names in database test.db:"
"test"
"SQL to create table 'test' in database test.db:"
"CREATE TABLE test (id integer primary key, str text)"
"number of rows in table 'test':"
"rows in table 'test':"
(1,"test string 2")
*Main>
If you don’t want to run the example in a REPL in order to experiment with it interactively you can
then just run it via stack using:
¹¹http://docs.haskellstack.org/en/stable/README.html
¹²https://github.com/mark-watson/haskell_tutorial_cookbook_examples
Preface
$ Database git:(master) > stack build --exec TestSqlite1
"Table names in database test.db:"
"test"
"SQL to create table 'test' in database test.db:"
"CREATE TABLE test (id integer primary key, str text)"
"number of rows in table 'test':"
"rows in table 'test':"
(1,"test string 2")
I include README.md files in the project directories with specific instructions.
I now use VSCode for most of my Haskell development. With the Haskell plugins VSCode offers
auto-completion while typing and highlights syntax errors. Previously I use other editor for Haskell
development. If you are an Emacs user I recommend that you follow the instructions in Appendix
A, load the tutorial files into an Emacs buffer, build an example and open a REPL frame. If one is not
already open type control-c control-l, switch to the REPL frame, and run the main function. When
you make changes to the tutorial files, doing another control-c control-l will re-build the example
in less than a second. In addition to using Emacs I occasionally use the IntelliJ Community Edition
(free) IDE with the Haskell plugin, the TextMate editor (OS X only) with the Haskell plugin, or the
GNU GEdit editor (Linux only).
Appendix A also shows you how to setup the *stack Haskell build tool.
Whether you use Emacs/VSCode or run a REPL in a terminal window (command window if you are
using Windows) the important thing is to get used to and enjoy the interactive style of development
that Haskell provides.
Why Haskell?
I have been using Lisp programming languages professionally since 1982. Lisp languages are flexible
and appropriate for many problems. Some might dissagree with me but I find that Haskell has most
of the advantages of Lisp with the added benefit of being strongly typed. Both Lisp and Haskell
support a style of development using an interactive shell (or “repl”).
What does being a strongly typed language mean? In a practical sense it means that you will often
encounter syntax errors caused by type mismatches that you will need to fix before your code will
compile (or run in the GHCi shell interpreter). Once your code compiles it will likely work, barring
a logic error. The other benefit that you can get is having to write fewer unit tests - at least that is
my experience. So, using a strongly typed language is a tradeoff. When I don’t use Haskell I tend to
use dynamic languages like Common Lisp or Python.
Preface
Enjoy Yourself
I have worked hard to make learning Haskell as easy as possible for you. If you are new to the
Haskell programming language then I have something to ask of you, dear reader: please don’t rush
through this book, rather take it slow and take time to experiment with the programming examples
that most interest you.
Acknowledgements
I would like to thank my wife Carol Watson for editing the manuscript for this book. I would like
to thank Roy Marantz, Michel Benard, and Daniel Kroni for reporting an errors.
Section 1 - Tutorial
The first section of this book contains two chapters:
• A tutorial on pure Haskell development: no side effects.
• A tutorial on impure Haskell development: dealing with the world (I/O, network access,
database access, etc.)
After working through these two tutorial chapters you will have sufficient knowledge of Haskell
development to understand the cookbook examples in the second section and be able to modify
them for your own use. Some of the general topics will be covered again in the second book section
that contains longer example programs.
Tutorial on Pure Haskell
Programming
Pure Haskell code has no side effects and if written properly is easy to read and understand. I am
assuming that you have installed stack using the directions in Appendix A. It is important to keep
a Haskell interactive repl open as you read the material in this book and experiment with the code
examples as you read. I don’t believe that you will be able to learn the material in this chapter unless
you work along trying the examples and experimenting with them in an open Haskell repl!
The directory Pure in the git repository contains the examples for this chapter. Many of the examples
contain a small bit of impure code in a main function. We will cover how this impure code works
in the next chapter but let’s look at a short example of impure code that is contained inside a main
function:
main = do
putStrLn ("1 + 2 = " ++ show (1 + 2))
The function main is the entry point of this short two line program. When the program is run, the
main function will be executed.
Here the function main uses the do notation to execute a single IO action, but do can also execute
a sequence of actions. The putStrLn function prints a string to the console. The printed string is
constructed by concatenating three parts: “1 + 2 = “, the result of the expression 1 + 2 (which is 3),
and the string representation of this result, which is obtained by calling the function show.
It’s worth noting that putStrLn writes a string to the standard output and also writes a new line
character to the console. In general, the function show is used to convert any value to a string, here
it is converting the result of 1+2 to string to concatenate it with the previous string.
Pure Haskell code performs no I/O, network access, access to shared in-memory data structures, etc.
The first time you build an example program with stack it may take a while since library
dependencies need to be loaded from the web. In each example directory, after an initial stack
build or stack ghci (to run the repl) then you should not notice this delay.
Interactive GHCi Shell
The interactive shell (often called a “repl”) is very useful for learning Haskell: understanding types
and the value of expressions. While simple expressions can be typed directly into the GHCi shell, it
is usually better to use an external text editor and load Haskell source files into the shell (repl). Let’s
get started. Assuming that you have installed stack as described in Appendix A, please try:
Tutorial on Pure Haskell Programming
~/$ cd haskell_tutorial_cookbook_examples/Pure
~/haskell_tutorial_cookbook_examples/Pure$ stack ghci
Using main module: Package `Pure' component exe:Simple with main-is file: /home/mark\
w/BITBUCKET/haskell_tutorial_cookbook_examples/Pure/Simple.hs
Configuring GHCi with the following packages: Pure
GHCi, version 7.10.3: http://www.haskell.org/ghc/
:? for help
[1 of 1] Compiling Main
( /home/markw/BITBUCKET/haskell_tutorial_cookboo\
k_examples/Pure/Simple.hs, interpreted )
Ok, modules loaded: Main.
*Main> 1 + 2
*Main> (1 + 2)
*Main> :t (1 + 2)
(1 + 2) :: Num a => a
*Main> :l Simple.hs
[1 of 1] Compiling Main
( Simple.hs, interpreted )
Ok, modules loaded: Main.
*Main> main
1 + 2 = 3
*Main>
If you are working in a repl and edit a file you just loaded with :l, you can then reload the last file
loaded using :r without specifying the file name. This makes it quick and easy to edit a Haskell file
with an external editor like Emacs or Vi and reload it in the repl after saving changes to the current
Here we have evaluated a simple expression “1 + 2” in line 10. Notice that in line 12 we can always
place parenthesis around an expression without changing its value. We will use parenthesis when
we need to change the default orders of precedence of functions and operators and make the code
more readable.
In line 14 we are using the ghci :t command to show the type of the expression (1 + 2). The type
Num is a type class (i.e., a more general purpose type that other types can inherit from) that contains
several sub-types of numbers. As examples, two subtypes of Num are Fractional (e.g., 3.5) and
Integer (e.g., 123). Type classes provide a form of function overloading since existing functions can
be redefined to handle arguments that are instances of new classes.
In line 16 we are using the ghci command :l to load the external file Simple.hs. This file contains a
function called main so we can execute main after loading the file. The contents of Simple.hs is:
Tutorial on Pure Haskell Programming
module Main where
sum2 x y = x + y
main = do
putStrLn ("1 + 2 = " ++ show (sum2 1 2))
Line 1 defines a module named Main. The rest of this file is the definition of the module. This form
of the module do expression exports all symbols so other code loading this module has access to
sum2 and main. If we only wanted to export main then we could use:
module Main (main) where
The function sum2 takes two arguments and adds them together. I didn’t define the type of this
function so Haskell does it for us using type inference.
*Main> :l Simple.hs
[1 of 1] Compiling Main
( Simple.hs, interpreted )
Ok, modules loaded: Main.
*Main> :t sum2
sum2 :: Num a => a -> a -> a
*Main> sum2 1 2
*Main> sum2 1.0 2
*Main> :t 3.0
3.0 :: Fractional a => a
*Main> :t 3
3 :: Num a => a
*Main> (toInteger 3)
*Main> :t (toInteger 3)
(toInteger 3) :: Integer
*Main>
What if you want to build a standalone executable program from the example in Smple.hs? Here is
an example:
Tutorial on Pure Haskell Programming
$ stack ghc Simple.hs
[1 of 1] Compiling Main
( Simple.hs, Simple.o )
Linking Simple ...
$ ./Simple
1 + 2 = 3
Most of the time we will use simple types built into Haskell: characters, strings, lists, and tuples.
The type Char is a single character. One type of string is a list of characters [Char]. (Another type
ByteString will be covered in later chapters.) Every element in a list must have the same type. A
Tuple is like a list but elements can be different types. Here is a quick introduction to these types,
with many more examples later:
*Main> :t 's'
's' :: Char
*Main> :t "tree"
"tree" :: [Char]
*Main> 's' : "tree"
"stree"
*Main> :t "tick"
"tick" :: [Char]
*Main> 's' : "tick"
"stick"
*Main> :t [1,2,3,4]
[1,2,3,4] :: Num t => [t]
*Main> :t [1,2,3.3,4]
[1,2,3.3,4] :: Fractional t => [t]
*Main> :t ["the", "cat", "slept"]
["the", "cat", "slept"] :: [[Char]]
*Main> ["the", "cat", "slept"] !! 0
*Main> head ["the", "cat", "slept"]
*Main> tail ["the", "cat", "slept"]
["cat","slept"]
*Main> ["the", "cat", "slept"] !! 1
*Main> :t (20, 'c')
(20, 'c') :: Num t => (t, Char)
*Main> :t (30, "dog")
(30, "dog") :: Num t => (t, [Char])
*Main> :t (1, "10 Jackson Street", 80211, 77.5)
(1, "10 Jackson Street", 80211, 77.5)
:: (Fractional t2, Num t, Num t1) => (t, [Char], t1, t2)
Tutorial on Pure Haskell Programming
The GHCi repl command :t tells us the type of any expression or function. Much of your time
developing Haskell will be spent with an open repl and you will find yourself checking types many
times during a development session.
In line 1 you see that the type of ’s‘ is ’s’ :: Char and in line 3 that the type of the string “tree”
is [Char] which is a list of characters. The abbreviation String is defined for [Char]; you can use
either. In line 9 we see the “cons” operator : used to prepend a character to a list of characters. The
cons : operator works with all types contained in any lists. All elements in a list must be of the same
The type of the list of numbers [1,2,3,4] in line 11 is [1,2,3,4] :: Num t ⇒ [t]. The type Num is a
general number type. The expression Num t ⇒ [t] is read as: “t is a type variable equal to Num and
the type of the list is [t], or a list of Num values”. It bears repeating: all elements in a list must be of
the same type. The functions head and tail used in lines 19 and 21 return the first element of a list
and return a list without the first element.
You will use lists frequently but the restriction of all list elements being the same type can be too
restrictive so Haskell also provides a type of sequence called tuple whose elements can be of different
types as in the examples in lines 25-31.
Tuples of length 2 are special because functions fst and snd are provided to access the first and
second pair value:
*Main> fst (1, "10 Jackson Street")
*Main> snd (1, "10 Jackson Street")
"10 Jackson Street"
*Main> :info fst
fst :: (a, b) -> a
-- Defined in ‘Data.Tuple’
*Main> :info snd
snd :: (a, b) -> b
-- Defined in ‘Data.Tuple’
Please note that fst and snd will not work with tuples that are not of length 2. Also note that if you
use the function length on a tuple, the result is always one because of the way tuples are defined as
Foldable types, which we will use later.
Haskell provides a concise notation to get values out of long tuples. This notation is called
destructuring:
Tutorial on Pure Haskell Programming
*Main> let geoData = (1, "10 Jackson Street", 80211, 77.5)
*Main> let (_,_,zipCode,temperature) = geoData
*Main> zipCode
*Main> temperature
Here, we defined a tuple geoData with values: index, street address, zip code, and temperature. In
line two we extract the zip code and temperature. Another reminder: we use let in lines 1-2 because
we are in a repl.
Like all programming languages, Haskell has operator precedence rules as these examples show:
*Main> 1 + 2 * 10
*Main> 1 + (2 * 10)
*Main> length "the"
*Main> length "the" + 10
*Main> (length "the") + 10
The examples in lines 1-4 illustrate that the multiplication operator has a higher precedence than
the addition operator.
*Main> :t length
length :: Foldable t => t a -> Int
*Main> :t (+)
(+) :: Num a => a -> a -> a
Note that the function length starts with a lower case letter. All Haskell functions start with a lower
case letter except for type constructor functions that we will get to later. A Foldable type can be
iterated through and be processed with map functions (which we will use shortly).
We saw that the function + acts as an infix operator. We can convert infix functions to prefix
functions by enclosing them in parenthesis:
Tutorial on Pure Haskell Programming
*Main> (+) 1 2
*Main> div 10 3
*Main> 10 `div` 3
In this last example we also saw how a prefix function div can be used infix by enclosing it in back
tick characters.
*Main> let x3 = [1,2,3]
*Main> x3
[1,2,3]
*Main> let x4 = 0 : x3
*Main> x4
[0,1,2,3]
*Main> x3 ++ x4
[1,2,3,0,1,2,3]
*Main> x4
[0,1,2,3]
*Main> x4 !! 0
*Main> x4 !! 100
*** Exception: Prelude.!!: index too large
*Main> let myfunc1 x y = x ++ y
*Main> :t myfunc1
myfunc1 :: [a] -> [a] -> [a]
*Main> myfunc1 x3 x4
[1,2,3,0,1,2,3]
Usually we define functions in files and load them as we need them. Here is the contents of the file
myfunc1.hs:
myfunc1 :: [a] -> [a] -> [a]
myfunc1 x y = x ++ y
The first line is a type signature for the function and is not required; here the input arguments are
two lists and the output is the two lists concatenated together. In line 1 note that a is a type variable
that can represent any type. However, all elements in the two function input lists and the output list
are constrained to be the same type.
Tutorial on Pure Haskell Programming
*Main> :l myfunc1.hs
[1 of 1] Compiling Main
( myfunc1.hs, interpreted )
Ok, modules loaded: Main.
*Main> myfunc1 ["the", "cat"] ["ran", "up", "a", "tree"]
["the","cat","ran","up","a","tree"]
Please note that the stack repl auto-completes using the tab character. For example, when I was
typing in “:l myfunc1.hs” I actually just typed “:l myf” and then hit the tab character to complete
the file name. Experiment with auto-completion, it will save you a lot of typing. In the following
example, for instance, after defining the variable sentence I can just type “se” and the tab character
to auto-complete the entire variable name:
*Main> let sentence = myfunc1 ["the", "cat"] ["ran", "up", "a", "tree"]
*Main> sentence
["the","cat","ran","up","a","tree"]
The function head returns the first element in a list and the function tail returns all but the first
elements in a list:
*Main> head sentence
*Main> tail sentence
["cat","ran","up","a","tree"]
We can create new functions from existing arguments by supplying few arguments, a process known
as “currying”:
*Main> let p1 = (+ 1)
*Main> :t p1
p1 :: Num a => a -> a
*Main> p1 20
In this last example the function + takes two arguments but if we only supply one argument a
function is returned as the value: in this case a function that adds 1 to an input value.
We can also create new functions by composing existing functions using the infix function . that
when placed between two function names produces a new function that combines the two functions.
Let’s look at an example that uses . to combine the partial function (+ 1) with the function length:
Tutorial on Pure Haskell Programming
*Main> let lengthp1 = (+ 1) . length
*Main> :t lengthp1
lengthp1 :: Foldable t => t a -> Int
*Main> lengthp1 "dog"
Note the order of the arguments to the inline function .: the argument on the right side is the first
function that is applied, then the function on the left side of the . is applied.
This is the second example where we have seen the type Foldable which means that a type can be
mapped over, or iterated over. We will look at Haskell types in the next section.
Introduction to Haskell Types
This is a good time to spend more time studying Haskell types. We will see more material on Haskell
types throughout this book so this is just an introduction using the data expression to define a Type
MyColors defined in the file MyColors.hs:
data MyColors = Orange | Red | Blue | Green | Silver
deriving (Show)
This code defines a new data type in Haskell named MyColors that has five values: Orange, Red,
Blue, Green or Silver. The keyword data is used to define a new data type, and the “|” symbol is
used to separate the different possible values (also known as constructors) of the type.
The deriving (Show) clause at the end of the line tells the compiler to automatically generate an
implementation of the Show type class for the MyColors type. In other words, we are asking the
Haskell compiler to automatically generate a function show that can convert a value to a string.
show is a standard function and in general we want it defined for all types. show converts an
instance to a string value. This allows instances of MyColors to be converted to strings using the
function show.
The MyColors type defined here is an enumeration (i.e., it is a fixed set of values), it’s an algebraic
data type with no associated fields. This means that the type MyColors can only take one of the five
values defined: Orange, Red, Blue, Green or Silver. There is another way to think about this. This
code defines a new data type called MyColors with five constructors Orange, Red, Blue, Green or
Silver.
Tutorial on Pure Haskell Programming
Prelude> :l colors.hs
[1 of 1] Compiling Main
( colors.hs, interpreted )
Ok, modules loaded: Main.
*Main> show Red
*Main> let c1 = Green
*Main> c1
*Main> :t c1
c1 :: MyColors
*Main> Red == Green
<interactive>:60:5:
No instance for (Eq MyColors) arising from a use of ‘==’
In the expression: Red == Green
In an equation for ‘it’: it = Red == Green
What went wrong here? The infix function == checks for equality and we did not define equality
functions for our new type. Let’s fix the definition in the file colors.hs:
data MyColors = Orange | Red | Blue | Green | Silver
deriving (Show, Eq)
Because we are deriving Eq we are also asking the compiler to generate code to see if two instances
of this class are equal. If we wanted to be able to order our colors then we would also derive Ord.
Now our new type has show, ==, and /= (inequality) defined:
Prelude> :l colors.hs
[1 of 1] Compiling Main
( colors.hs, interpreted )
Ok, modules loaded: Main.
*Main> Red == Green
*Main> Red /= Green
Let’s also now derive Ord to have the compile generate a default function compare that operates
on the type MyColors:
data MyColors = Orange | Red | Blue | Green | Silver
deriving (Show, Eq, Ord)
Because we are now deriving Ord the compiler will generate functions to calculate relative ordering
for values of type MyColors. Let’s experiment with this:
Tutorial on Pure Haskell Programming
*Main> :l MyColors.hs
[1 of 1] Compiling Main
( MyColors.hs, interpreted )
Ok, modules loaded: Main.
*Main> :t compare
compare :: Ord a => a -> a -> Ordering
*Main> compare Green Blue
*Main> compare Blue Green
*Main> Orange < Red
*Main> Red < Orange
*Main> Green < Red
*Main> Green < Silver
*Main> Green > Red
Notice that the compiler generates a compare function for the type MyColors that orders values
by the order that they appear in the data expression. What if you wanted to order them in string
sort order? This is very simple: we will remove Ord from the deriving clause and define our own
function compare for type MyColors instead of letting the compiler generate it for us:
data MyColors = Orange | Red | Blue | Green | Silver
deriving (Show, Eq)
instance Ord MyColors where
compare c1 c2 = compare (show c1) (show c2)
In line 5 I am using the function show to convert instances of MyColors to strings and then the
version of compare that is called in line 5 is the version the compiler wrote for us because we derived
Show. Now the ordering is in string ascending sort order because we are using the compare function
that is supplied for the type String:
*Main> :l MyColors.hs
[1 of 1] Compiling Main
( MyColors.hs, interpreted )
Ok, modules loaded: Main.
*Main> Green > Red
Tutorial on Pure Haskell Programming
Our new type MyColors is a simple type. Haskell also supports hierarchies of types called Type
Classes and the type we have seen earlier Foldable is an example of a type class that other types
can inherit from. For now, consider sub-types of Foldable to be collections like lists and trees that
can be iterated over.
I want you to get in the habit of using :type and :info (usually abbreviated to :t and :i) in the GHCi
repl. Stop reading for a minute now and type :info Ord in an open repl. You will get a lot of output
showing you all of the types that Ord is defined for. Here is a small bit of what gets printed:
*Main> :i Ord
class Eq a => Ord a where
compare :: a -> a -> Ordering
(<) :: a -> a -> Bool
(<=) :: a -> a -> Bool
(>) :: a -> a -> Bool
(>=) :: a -> a -> Bool
max :: a -> a -> a
min :: a -> a -> a
-- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
instance Ord MyColors -- Defined at MyColors.hs:4:10
instance (Ord a, Ord b) => Ord (Either a b)
-- Defined in ‘Data.Either’
instance Ord a => Ord [a]
-- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
instance Ord Word -- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
instance Ord Ordering -- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
instance Ord Int -- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
instance Ord Float -- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
instance Ord Double -- Defined in ‘ghc-prim-0.4.0.0:GHC.Classes’
Lines 1 through 8 show you that Ord is a subtype of Eq that defines functions compare, max, and
min as well as the four operators <, <=, >=, and >=. When we customized the compare function for
the type MyColors, we only implemented compare. That is all that we needed to do since the other
operators rely on the implementation of compare.
Once again, I ask you to experiment with the example type MyColors in an open GHCi repl:
Tutorial on Pure Haskell Programming
*Main> :t max
max :: Ord a => a -> a -> a
*Main> :t Green
Green :: MyColors
*Main> :i Green
data MyColors = ... | Green | ...
-- Defined at MyColors.hs:1:39
*Main> max Green Red
The following diagram shows a partial type hierarchy of a few types included in the standard Haskell
Prelude (this is derived from the Haskell Report at haskell.org¹³):
Example Haskell Type Hierarchy
Here you see that type Num and Ord are sub-types of type Eq, Real is a sub-type of Num, etc. We
will see the types Monad and Functor in the next chapter.
Functions Are Pure
Again, it is worth pointing out that Haskell functions do not modify their inputs values. The common
pattern is to pass immutable values to a function and modified values are returned. As a first example
of this pattern we will look at the standard function map that takes two arguments: a function that
¹³https://www.haskell.org/onlinereport/basic.html
Tutorial on Pure Haskell Programming
converts a value of any type a to another type b, and a list of type a. Functions that take other
functions as arguments are called higher order functions. The result is another list of the same
length whose elements are of type b and the elements are calulated using the function passed as the
first argument. Let’s look at a simple example using the function (+ 1) that adds 1 to a value:
*Main> :t map
map :: (a -> b) -> [a] -> [b]
*Main> map (+ 1) [10,20,30]
[11,21,31]
*Main> map (show . (+ 1)) [10,20,30]
["11","21","31"]
In the first example, types a and b are the same, a Num. The second example used a composed
function that adds 1 and then converts the example to a string. Remember: the function show
converts a Haskell data value to a string. In this second example types a and b are different because
the function is mapping a number to a string.
The directory haskell_tutorial_cookbook_examples/Pure contains the examples for this chapter. We
previously used the example file Simple.hs. Please note that in the rest of this book I will omit the
git repository top level directory name haskell_tutorial_cookbook_examples and just specify the
sub-directory name:
module Main where
sum2 x y = x + y
main = do
putStrLn ("1 + 2 = " ++ show (sum2 1 2))
For now let’s just look at the mechanics of executing this file without using the REPL (started with
stack ghci). We can simply build and run this example using stack, which is covered in some detail
in Appendix A:
stack build --exec Simple
This command builds the project defined in the configuration files Pure.cabal and stack.yaml
(the format and use of these files is briefly covered in detail in Appendix A and there is more
reference material here¹⁴). This example defines two functions: sum2 and main. sum2 is a pure
Haskell function with no state, no interaction with the outside world like file IO, etc., and no non-
determinism. main is an impure function, and we will look at impure Haskell code in some detail
in the next chapter. As you might guess the output of this code snippet is
¹⁴https://docs.haskellstack.org/en/stable/yaml_configuration/
Tutorial on Pure Haskell Programming
1 + 2 = 3
To continue the tutorial on using pure Haskell functions, once again we will use stack to start an
interactive repl during development:
markw@linux:~/haskell_tutorial_cookbook_examples/Pure$ stack ghci
*Main> :t 3
3 :: Num a => a
*Main> :t "dog"
"dog" :: [Char]
*Main> :t main
main :: IO ()
*Main>
In this last listing I don’t show the information about your Haskell environment and the packages
that were loaded. In repl listings in the remainder of this book I will continue to edit out this Haskell
environment information for brevity.
Line 4 shows the use of the repl shortcut :t to print out the type of a string which is an array of
[Char], and the type of the function main is of type IO Action, which we will explain in the next
chapter. An IO action contains impure code where we can read and write files, perform a network
operation, etc. and we will look at IO Action in the next chapter.
Using Parenthesis or the Special $ Character and
Operator Precedence
We will look at operator and function precedence and the use of the $ character to simplify using
parenthesis in expessions. By the way, in Haskell there is not much difference between operators
and function calls except operators like +, etc. which are by default infix while functions are usually
prefix. So except for infix functions that are enclosed in backticks (e.g., 10 div 3) Haskell usually
uses prefix functions: a function followed by zero or more arguments. You can also use $ that acts
as an opening parenthesis with a not-shown closing parenthesis at the end of the current expression
(which may be multi-line). Here are some examples:
Tutorial on Pure Haskell Programming
*Main> print (3 * 2)
*Main> print $ 3 * 2
*Main> last (take 10 [1..])
*Main> last $ take 10 [1..]
*Main> ((take 10 [1..]) ++ (take 10 [1000..]))
[1,2,3,4,5,6,7,8,9,10,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009]
*Main> take 10 [1..] ++ take 10 [1000..]
[1,2,3,4,5,6,7,8,9,10,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009]
*Main> 1 + 2 * (4 * 5)
*Main> 2 * 3 + 10 * 30
I use the GHCi command :info (:i is an abbreviation) to check both operator precedence and the
function signature if the operator is converted to a function by enclosing it in parenthessis:
*Main> :info *
class Num a where
(*) :: a -> a -> a
-- Defined in ‘GHC.Num’
infixl 7 *
*Main> :info +
class Num a where
(+) :: a -> a -> a
-- Defined in ‘GHC.Num’
infixl 6 +
*Main> :info `div`
class (Real a, Enum a) => Integral a where
div :: a -> a -> a
-- Defined in ‘GHC.Real’
infixl 7 `div`
*Main> :i +
class Num a where
(+) :: a -> a -> a
Tutorial on Pure Haskell Programming
-- Defined in ‘GHC.Num’
infixl 6 +
Notice how + has lower precedence than *.
Just to be clear, understand how operators are used as functions and also how functions can be used
as infix operators:
*Main> 2 * 3
*Main> (*) 2 3
*Main> 10 `div` 3
*Main> div 10 3
Especially when you are just starting to use Haskell it is a good idea to also use :info to check the
type signatures of standard functions that you use. For example:
*Main> :info last
last :: [a] -> a
-- Defined in ‘GHC.List’
*Main> :info map
map :: (a -> b) -> [a] -> [b]
-- Defined in ‘GHC.Base’
Lazy Evaluation
Haskell is refered to as a lazy language because expressions are not evaluated until they are used.
Consider the following example:
$ stack ghci
*Main> [0..10]
[0,1,2,3,4,5,6,7,8,9,10]
*Main> take 11 [0..]
[0,1,2,3,4,5,6,7,8,9,10]
*Main> let xs = [0..]
*Main> :sprint xs
xs = _
*Main> take 5 xs
[0,1,2,3,4]
Tutorial on Pure Haskell Programming
*Main> :sprint xs
xs = _
*Main>
In line 2 we are creating a list with 11 elements. In line 4 we are doing two things:
• Creating an infinitely long list containing ascending integers starting with 0.
• Fetching the first 11 elements of this infinitely long list. It is important to understand that in
line 4 only the first 11 elements are generated because that is all the take function requires.
In line 6 we are assigning another infinitely long list to the variable xs but the value of xs is
unevaluated and a placeholder is stored to calculate values as required. In line 7 we use GHCi’s
:sprint command to show a value without evaluating it. The output in line 8 _ indicated that the
expression has yet to be evaluated.
Lines 9 through 12 remind us that Haskell is a functional language: the take function used in line 9
does not change the value of its argument so xs as seen in lines 10 and 12 is still unevaluated.
Understanding List Comprehensions
Effectively using list comprehensions makes your code shorter, easier to understand, and easier to
maintain. Let’s start out with a few GHCi repl examples. You will learn a new GHCi repl trick in
this section: entering multiple line expressions by using :{ and :} to delay evaluation until an entire
expression is entered in the repl (listings in this section are reformatted to fit the page width):
*Main> [x | x <- ["cat", "dog", "bird"]]
["cat","dog","bird"]
*Main> :{
*Main| [(x,y) | x <- ["cat", "dog", "bird"],
*Main|
y <- [1..2]]
*Main| :}
[("cat",1),("cat",2),("dog",1),("dog",2),("bird",1),("bird",2)]
The list comprehension on line 1 assigns the elements of the list [“cat”, “dog”, “bird”] one at a time
to the variable x and then collects all these values of x in a list value that is the value of the list
comprehension. The list comprehension in line 1 is hopefully easy to understand but when we bind
and collect multiple variables the situation, as seen in the example in lines 4 and 5, is not as easy to
understand. The thing to remember is that the first variable gets iterated as an “outer loop” and the
second variable is iterated as the “inner loop.” List comprehensions can use many variables and the
iteration ordering rule is the same: last variable iterates first, etc.
Tutorial on Pure Haskell Programming
*Main> :{
*Main| [(x,y) | x <- [0..3],
*Main|
y <- [1,3..10]]
*Main| :}
[(0,1),(0,3),(0,5),(0,7),(0,9),(1,1),(1,3),(1,5),(1,7),
(1,9),(2,1),(2,3),(2,5),(2,7),(2,9),(3,1),(3,3),(3,5),
(3,7),(3,9)]
*Main> [1,3..10]
[1,3,5,7,9]
In this last example we are generating all combinations of [0..3] and [1,3..10] and storing the
combinations as two element tuples. You could also store then as lists:
*Main> [[x,y] | x <- [1,2], y <- [10,11]]
[[1,10],[1,11],[2,10],[2,11]]
List comprehensions can also contain filtering operations. Here is an example with one filter:
*Main> :{
*Main| [(x,y) | x <- ["cat", "dog", "bird"],
*Main|
y <- [1..10],
*Main|
y `mod` 3 == 0]
*Main| :}
[("cat",3),("cat",6),("cat",9),
("dog",3),("dog",6),("dog",9),
("bird",3),("bird",6),("bird",9)]
Here is a similar example with two filters (we are also filtering out all possible values of x that start
with the character ‘d’):
*Main> :{
*Main| [(x,y) | x <- ["cat", "dog", "bird"],
*Main|
y <- [1..10],
*Main|
y `mod` 3 == 0,
*Main|
x !! 0 /= 'd']
*Main| :}
[("cat",3),("cat",6),("cat",9),("bird",3),("bird",6),("bird",9)]
For simple filtering cases I usually use the filter function but list comprehensions are more versatile.
List comprehensions are extremely useful - I use them frequently.
Lists are instances of the class Monad that we will cover in the next chapter (check out the section
“List Comprehensions Using the do Notation”).
List comprehensions are powerful. I would like to end this section with another trick that does not
use list comprehensions for building lists of tuple values: using the zip function:
Tutorial on Pure Haskell Programming
*Main> let animals = ["cat", "dog", "bird"]
*Main> zip [1..] animals
[(1,"cat"),(2,"dog"),(3,"bird")]
*Main> :info zip
zip :: [a] -> [b] -> [(a, b)]
-- Defined in ‘GHC.List’
The function zip is often used in this way when we have a list of objects and we want to operate on
the list while knowing the index of each element.
Haskell Rules for Indenting Code
When a line of code is indented relative to the previous line of code, or several lines of code with
additional indentation, then the indented lines act as if they were on the previous line. In other
words, if code that should all be on one line must be split to multiple lines, then use indentation as
a signal to the Haskell compiler.
Indentation of continuation lines should be uniform, starting in the same column. Here are some
examples of good code, and code that will not compile:
let a = 1
-- good
-- good
-- good
-- good
-- good
-- good
in a + b + c
-- good
let a = 1
-- will not compile (bad)
-- will not compile (bad)
-- will not compile (bad)
-- will not compile (bad)
-- will not compile (bad)
-- will not compile (bad)
a = 1;
-- compiles but bad style (good)
b = 2;
-- compiles but bad style (good)
c = 3;
-- compiles but bad style (good)
Tutorial on Pure Haskell Programming
If you use C style braces and semicolons to mark end of expressions, then indenting does not matter
as seen in lines 20 through 24. Otherwise, uniform indentation is a hint to the compiler.
The same indenting rules apply to other types of do expressions which we will see throughout this
book for do, if, and other types of do expressions.
Understanding let and where
At first glance, let and where seem very similar in that they allow us to create temporary variables
used inside functions. As the examples in the file LetAndWhere.hs show, there are important
differences.
In the following code notice that when we use let in pure code inside a function, we then use in to
indicate the start of an expression to be evaluated that uses any variables defined in a let expression.
Inside a do code block the in token is not needed and will cause a parse error if you use it. do code
blocks are a syntactic sugar for use in impure Haskell code and we will use it frequently later in the
You also do not use in inside a list comprehension as seen in the function testLetComprehension
in the next code listing:
module Main where
funnySummation w x y z =
let bob = w + x
sally = y + z
in bob + sally
testLetComprehension =
[(a,b) | a <- [0..5], let b = 10 * a]
testWhereBlocks a =
z = a + 2
functionWithWhere n
(n + 1) * tenn
tenn = 10 * n
main = do
Tutorial on Pure Haskell Programming
print $ funnySummation 1 2 3 4
let n = "Rigby"
print n
print testLetComprehension
print $ testWhereBlocks 11
print $ functionWithWhere 1
Compare the let do expressions starting on line 4 and 24. The first let occurs in pure code and uses
in to define one or more do expressions using values bound in the let. In line 24 we are inside a
monad, specifically using the do notation and here let is used to define pure values that can be used
later in the do do expression.
Loading the last code example and running the main function produces the following output:
*Main> :l LetAndWhere.hs
[1 of 1] Compiling Main
( LetAndWhere.hs, interpreted )
Ok, modules loaded: Main.
*Main> main
"Rigby"
[(0,0),(1,10),(2,20),(3,30),(4,40),(5,50)]
This output is self explanatory except for line 7 that is the result of calling testLetComprehension
that retuns an example list comprehension [(a,b)|a<-[0..5],letb=10*a]
Conditional do Expressions and Anonymous Functions
The examples in the next three sub-sections can be found in haskell_tutorial_cookbook_exam-
ples/Pure/Conditionals.hs. You should read the following sub-sections with this file loaded (some
GHCi repl output removed for brevity):
haskell_tutorial_cookbook_examples/Pure$ stack ghci
*Main> :l Conditionals.hs
[1 of 1] Compiling Main
( Conditionals.hs, interpreted )
Ok, modules loaded: Main.
*Main>
Tutorial on Pure Haskell Programming
Simple Pattern Matching
We previously used the built-in functions head that returns the first element of a list and tail that
returns a list with the first element removed. We will define these functions ourselves using what
is called wild card pattern matching. It is common to append the single quote character ‘ to built-in
functions when we redefine them so we name our new functions head’ and tail’. Remember when
we used destructuring to access elements of a tuple? Wild card pattern matching is similar:
head'(x:_)
tail'(_:xs) = xs
The underscore character _ matches anything and ignores the matched value. Our head and tail
definitions work as expected:
*Main> head' ["bird","dog","cat"]
"bird"
*Main> tail' [0,1,2,3,4,5]
[1,2,3,4,5]
*Main> :type head'
head' :: [t] -> t
*Main> :t tail'
tail' :: [t] -> [t]
Of course we frequently do not want to ignore matched values. Here is a contrived example that
expects a list of numbers and doubles the value of each element. As for all of the examples in this
chapter, the following function is pure: it can not modify its argument(s) and always returns the
same value given the same input argument(s):
doubleList [] = []
doubleList (x:xs) = (* 2) x : doubleList xs
In line 1 we start by defining a pattern to match the empty list. It is necessary to define this
terminating condition because we are using recursion in line 2 and eventually we reach the end
of the input list and make the recursive call doubleList []. If you leave out line 1 you then will see
a runtime error like “Non-exhaustive patterns in function doubleList.” As a Haskell beginner you
probably hate Haskell error messages and as you start to write your own functions in source files
and load them into a GHCi repl or compile them, you will initially probably hate compilation error
messages also. I ask you to take on faith a bit of advice: Haskell error messages and warnings will
end up saving you a lot of effort getting your code to work properly. Try to develop the attitude
“Great! The Haskell compiler is helping me!” when you see runtime errors and compiler errors.
In line 2 notice how I didn’t need to use extra parenthesis because of the operator and function
application precedence rules.
Tutorial on Pure Haskell Programming
*Main> doubleList [0..5]
[0,2,4,6,8,10]
*Main> :t doubleList
doubleList :: Num t => [t] -> [t]
This function doubleList seems very unsatisfactory because it is so specific. What if we wanted to
triple or quadruple the elements of a list? Do we want to write two new functions? You might think
of adding an argument that is the multiplier like this:
bumpList n [] = []
bumpList n (x:xs) = n * x : bumpList n xs
is better, being more abstract and more general purpose. However, we will do much better.
Before generalizing the list manipuation process further, I would like to make a comment on coding
style, specifically on not using unneeded parenthesis. In the last exmple defining bumpList if you
have superfluous parenthesis like this:
bumpList n (x:xs) = (n * x) : bumpList (n xs)
then the code still works correctly and is fairly readable. I would like you to get in the habit
of avoiding extra uneeded parenthesis and one tool for doing this is running hlint (installing
hlint is covered in Appendix A) on your Haskell code. Using hlint source file will provide
warnings/suggestions like this:
haskell_tutorial_cookbook_examples/Pure$ hlint Conditionals.hs
Conditionals.hs:7:21: Warning: Redundant bracket
Found:
((* 2) x) : doubleList (xs)
Why not:
(* 2) x : doubleList (xs)
Conditionals.hs:7:43: Error: Redundant bracket
Found:
Why not:
hlint is not only a tool for improving your code but also for teaching you how to better program using
Haskell. Please note that hlint provides other suggestions for Conditionals.hs that I am ignoring that
mostly suggest that I replace our mapping operations with using the built-in map function and use
functional composition. The sample code is specifically to show examples of pattern matching and
is not as concise as it could be.
Tutorial on Pure Haskell Programming
Are you satisfied with the generality of the function bumpList? I hope that you are not! We should
write a function that will apply an arbitrary function to each element of a list. We will call this
function map’ to avoid confusing our map’ function with the built-in function map.
The following is a simple implementation of a map function (we will see Haskell’s standard map
functions in the next section):
map' f [] = []
map' f (x:xs) = f x : map' f xs
In line 2 we do not need parenthesis around f x because function application has a higher precidence
than the operator : which adds an element to the beginning of a list.
Are you pleased with how concise this definition of a map function is? Is concise code like map’
readable to you? Speaking as someone who has written hundreds of thousands of lines of Java code
for customers, let me tell you that I love the conciseness and readability of Haskell! I appreciate
the Java ecosystem with many useful libraries and frameworks and augmented like fine languages
like Clojure and JRuby, but in my opinion using Haskell is a more enjoyable and generally more
productive language and programming environment.
Let’s experiment with our map’ function:
*Main> map' (* 7) [0..5]
[0,7,14,21,28,35]
*Main> map' (+ 1.1) [0..5]
[1.1,2.1,3.1,4.1,5.1,6.1]
*Main>
map' (\x -> (x + 1) * 2) [0..5]
[2,4,6,8,10,12]
Lines 1 and 3 should be understandable to you: we are creating a partial function like (* 7) and
passing it to map’ to apply to the list [0..5].
The syntax for the function in line 5 is called an anonymous function. Lisp programers, like myself,
refer to this as a lambda expression. In any case, I often prefer using anonymous functions when a
function will not be used elsewhere. In line 5 the argement to the anonymous inline function is x
and the body of the function is (x + 1) * 2.
I do ask you to not get carried away with using too many anonymous inline functions because they
can make code a little less readable. When we put our code in modules, by default every symbol
(like function names) in the module is externally visible. However, if we explicitly export symbols
in a module do expression then only the explicitly exported symbols are visible by other code that
uses the module. Here is an example:
Tutorial on Pure Haskell Programming
module Test2 (doubler) where
map' f [] = []
map' f (x:xs) = (f x) : map' f xs
testFunc x = (x + 1) * 2
doubler xs = map' (* 2) xs
In this example map’ and testFunc are hidden: any other module that imports Test2 only has access
to doubler. It might help for you to think of the exported functions roughly as an interface for a
module.
Pattern Matching With Guards
We will cover two important concepts in this section: using guard pattern matching to make function
definitions shorter and easier to read and we will look at the Maybe type and how it is used. The
Maybe type is mostly used in non-pure Haskell code and we will use it heavily later. The Maybe
type is a Monad (covered in the next chapter). I introduce the Maybe type here since its use fits
naturally with guard patterns.
Guards are more flexible than the pattern matching seen in the last section. I use pattern matching
for simple cases of destructuring data and guards when I need the flexibility. You may want to revisit
the examples in the last section after experimenting with and understanding the examples seen here.
The examples for this section are in the file Guards.hs. As a first simple example we will implement
the Ruby language “spaceship operator”:
spaceship n
| n < 0
| n == 0
| otherwise = 1
Notice on line 1 that we do not use an = in the function definition when using guards. Each guard
starts with |, contains a condition, and a value on the right side of the = sign.
*Main> spaceship (-10)
*Main> spaceship 0
*Main> spaceship 17
Remember that a literal negative number as seen in line 1 must be wrapped in parenthesis, otherwise
the Haskell compiler will interpret - as an operator.
Tutorial on Pure Haskell Programming
Case Expressions
Case do expressions match a value against a list of possible values. It is common to use the wildcard
matching value _ at the end of a case expression which can be of any type. Here is an example in
the file Cases.hs:
module Main where
numberOpinion n =
case n of
0 -> "Too low"
1 -> "just right"
_ -> "OK, that is a number"
main = do
print $ numberOpinion 0
print $ numberOpinion 1
print $ numberOpinion 2
The code in lines 3-7 defines the function numberOpinion that takes a single argument “n”. We
use a case expression to match the value of n against several possible cases. Each of these cases is
defined using the -> operator, followed by an expression to be evaluated if the case is matched.
The first case, 0 -> ‘Too low’ matches the value of n against 0, if the value of “n” is 0, the function
will return the string “Too low”. The second case, 1 -> ‘just right’ matches the value of n against 1,
if the value of n is 1, the function will return the string “just right”. The last case is different in that
it is a catch all case using the ** as a wild card match. So, ** -> ‘OK, that is a number’ matches
any other values of n: if the value of nn is not 0 or 1 the function will return the string “OK, that is
a number”.
If Then Else expressions
Haskell has if then else syntax built into the language - if is not defined as a function. Personally I
do not use if then else in Haskell very often. I mostly use simple pattern matching and guards. Here
are some short examples from the file IfThenElses.hs:
ageToString age =
if age < 21 then "minor" else "adult"
All if statements must have both a then expression and a else expression.
Tutorial on Pure Haskell Programming
haskell_tutorial_cookbook_examples/Pure$ stack ghci
*Main> :l IfThenElses.hs
[1 of 1] Compiling Main
( IfThenElses.hs, interpreted )
Ok, modules loaded: Main.
*Main> ageToString 15
"minor"
*Main> ageToString 37
"adult"
Maps are simple to construct using a list of key-value tuples and are by default immutable. There is
an example using mutable maps in the next chapter.
We will look at the module Data.Map first in a GHCi repl, then later in a few full code examples.
There is something new in line 1 of the following listing: I am assigning a short alias M to the module
Data.Map. In referencing a function like fromList (which converts a list of tuples to a map) in the
Data.Map module I can use M.fromList instead of Data.Map.fromList. This is a common practice
so when you read someone else’s Haskell code, one of the first things you should do when reading
a Haskell source file is to make note of the module name abbreviations at the top of the file.
haskell_tutorial_cookbook_examples/Pure$ stack ghci
*Main> import qualified Data.Map as M
*Main M> :t M.fromList
M.fromList :: Ord k => [(k, a)] -> M.Map k a
*Main M> let aTestMap = M.fromList [("height", 120), ("weight", 15)]
*Main M> :t aTestMap
aTestMap :: Num a => M.Map [Char] a
*Main M> :t lookup
lookup :: Eq a => a -> [(a, b)] -> Maybe b
*Main M> :t M.lookup
M.lookup :: Ord k => k -> M.Map k a -> Maybe a
*Main M> M.lookup "weight" aTestMap
Just 15
*Main M> M.lookup "address" aTestMap
Nothing
The keys in a map must all be the same type and the values are also constrained to be of the same
type. I almost always create maps using the helper function fromList in the module Data.Maps.
We will only be using this method of map creation in later examples in this book so I am skipping
coverage of other map building functions. I refer you to the Data.Map documentation¹⁵.
¹⁵https://www.stackage.org/haddock/lts-6.17/containers-0.5.6.2/Data-Map.html
Tutorial on Pure Haskell Programming
The following example shows one way to use the Just and Nothing return values:
module MapExamples where
import qualified Data.Map as M -- from library containers
aTestMap = M.fromList [("height", 120), ("weight", 15)]
getNumericValue key aMap =
case M.lookup key aMap of
Nothing -> -1
Just value -> value
main = do
print $ getNumericValue "height" aTestMap
print $ getNumericValue "age" aTestMap
The function getNumericValue shows one way to extract a value from an instance of type Maybe.
The function lookup returns a Maybe value and in this example I use a case statement to test for
a Nothing value or extract a wrapped value in a Just instance. Using Maybe in Haskell is a better
alternative to checking for null values in C or Java.
The output from running the main function in module MapExamples is:
haskell_tutorial_cookbook_examples/Pure$ stack ghci
*Main> :l MapExamples.hs
[1 of 1] Compiling MapExamples
( MapExamples.hs, interpreted )
Ok, modules loaded: MapExamples.
*MapExamples> main
The documentation of Data.Set.Class can be found here¹⁶ and contains overloaded functions for
the types of sets defined here¹⁷.
For most of my work and for the examples later in this book, I create immutable sets from lists and
the only operation I perform is checking to see if a value is in the set. The following examples in
GHCI repl are what you need for the material in this book:
¹⁶https://www.stackage.org/haddock/lts-6.17/sets-0.0.5/Data-Set-Class.html
¹⁷https://www.stackage.org/package/sets
Tutorial on Pure Haskell Programming
*Main> import qualified Data.Set as S
*Main S> let testSet = S.fromList ["cat","dog","bird"]
*Main S> :t testSet
testSet :: S.Set [Char]
*Main S> S.member "bird" testSet
*Main S> S.member "snake" testSet
Sets and Maps are immutable so I find creating maps using a lists of key-value tuples and creating
sets using lists is fine. That said, coming from the mutable Java, Ruby, Python, and Lisp programming
languages, it took me a while to get used to immutability in Haskell.
More on Functions
In this section we will review what you have learned so far about Haskell functions and then look
at a few more complex examples.
We have been defining and using simple functions and we have seen that operators behave like infix
functions. We can make operators act as prefix functions by wrapping them in parenthesis:
*Main> 10 + 1
*Main> (+) 10 1
and we can make functions act as infix operators:
*Main> div 100 9
*Main> 100 `div` 9
This back tick function to operator syntax works with functions we write also:
Tutorial on Pure Haskell Programming
*Main> let myAdd a b = a + b
*Main> :t myAdd
myAdd :: Num a => a -> a -> a
*Main> myAdd 1 2
*Main> 1 `myAdd` 2
Because we are working in a GHCi repl, in line 1 we use let to define the function myAdd. If you
defined this function in a file and then loaded it, you would not use a let.
In the map examples where we applied a function to a list of values, so far we have used functions
that map input values to the same return type, like this (using both partial function evaluation and
anonymous inline function):
*Main> map (* 2) [5,6]
[10,12]
*Main> map (\x -> 2 * x) [5,6]
[10,12]
We can also map to different types; in this example we map from a list of Num values to a list
containing sub-lists of Num values:
*Main> let makeList n = [0..n]
*Main> makeList 3
[0,1,2,3]
*Main> map makeList [2,3,4]
[[0,1,2],[0,1,2,3],[0,1,2,3,4]]
As usual, I recommend that when you work in a GHCi repl you check the types of functions and
values you are working with:
*Main> :t makeList
makeList :: (Enum t, Num t) => t -> [t]
*Main> :t [1,2]
[1,2] :: Num t => [t]
*Main> :t [[0,1,2],[0,1,2,3],[0,1,2,3,4]]
[[0,1,2],[0,1,2,3],[0,1,2,3,4]] :: Num t => [[t]]
*Main>
In line 2 we see that for any type t the function signature is t -> [t] where the compiler determines
that t is constrained to be a Num or Enum by examining how the input variable is used as a range
parameter for constructing a list. Let’s make a new function that works on any type:
Tutorial on Pure Haskell Programming
*Main> let make3 x = [x,x,x]
*Main> :t make3
make3 :: t -> [t]
*Main> :t make3 "abc"
make3 "abc" :: [[Char]]
*Main> make3 "abc"
["abc","abc","abc"]
*Main> make3 7.1
[7.1,7.1,7.1]
*Main> :t make3 7.1
make3 7.1 :: Fractional t => [t]
Notice in line 3 that the function make3 takes any type of input and returns a list of elements the
same type as the input. We used makes3 both with a string argument and a fractional (floating
point) number) argument.
Comments on Dealing With Immutable Data and How
to Structure Programs
If you program in other programming languages that use mutable data then expect some feelings of
disorientation initially when starting to use Haskell. It is common in other languages to maintain the
state of a computation in an object and to mutate the value(s) in that object. While I cover mutable
state in the next chapter the common pattern in Haskell is to create a data structure (we will use lists
in examples here) and pass it to functions that return a new modified copy of the data structure as
the returned value from the function. It is very common to keep passing the modified new copy of a
data structure through a series of function calls. This may seem cumbersome when you are starting
to use Haskell but quickly feels natural.
The following example shows a simple case where a list is constructed in the function main and
passed through two functions doubleOddElements and times10Elements:
module ChainedCalls where
doubleOddElements =
map (\x -> if x `mod` 2 == 0 then x else 2 * x)
times10Elements = map (* 10)
main = do
print $ doubleOddElements [0,1,2,3,4,5,6,7,8]
let aList = [0,1,2,3,4,5]
let newList = times10Elements $ doubleOddElements aList
Tutorial on Pure Haskell Programming
print newList
let newList2 = (times10Elements . doubleOddElements) aList
print newList2
Notice that the expressions being evaluated in lines 11 and 13 are the same. In line 11 we are
applying function doubleOddElements to the value of aList and passing this value to the outer
function times10Elements. In line 13 we are creating a new function from composing two existing
functions: times10Elements . doubleOddElements. The parenthesis in line 13 are required because
the . operator has lower precedence than the application of function doubleOddElements so without
the parenthesis line 13 would evaluate as times10Elements (doubleOddElements aList) which is
not what I intended and would throw an error.
The output is:
haskell_tutorial_cookbook_examples/Pure$ stack ghci
*Main> :l ChainedCalls.hs
[1 of 1] Compiling ChainedCalls
( ChainedCalls.hs, interpreted )
Ok, modules loaded: ChainedCalls.
*ChainedCalls> main
[0,2,2,6,4,10,6,14,8]
[0,20,20,60,40,100]
[0,20,20,60,40,100]
Using immutable data takes some getting used to. I am going to digress for a minute to talk about
working with Haskell. The steps I take when writing new Haskell code are:
• Be sure I understand the problem
• How will data be represented - in Haskell I prefer using built-in types when possible
• Determine which Haskell standard functions, modules, and 3rd party modules might be useful
• Write and test the pure Haskell functions I think that I need for the application
• Write an impure main function that fetches required data, calls the pure functions (which are
no longer pure in the sense they are called from impure code), and saves the processed data.
I am showing you many tiny examples but please keep in mind the entire process of writing longer
programs.
Error Handling
We have seen examples of handling soft errors when no value can be calculated: use Maybe, Just,
and Nothing. In bug free pure Haskell code, runtime exceptions should be very rare and I usually
do not try to trap them.
Using Maybe, Just, and Nothing is much better than, for example, throwing an error using the
standard function error:
Tutorial on Pure Haskell Programming
*Main> error "test error 123"
*** Exception: test error 123
and then, in impure code catching the errors, here is the documentation¹⁸ for your reference.
In impure code that performs IO or accesses network resources that could possibly run out of
memory, etc., runtime errors can occur and you could use the same try catch coding style that you
have probably used in other programming languages. I admit this is my personal coding style but I
don’t like to catch runtime errors. I spent a long time writing Java applications and when possible I
preferred using uncaught exceptions and I usually do the same when writing impure Haskell code.
Because of Haskell’s type safety and excellent testing tools, it is possible to write nearly error free
Haskell code. Later when we perform network IO we will rely on library support to handle errors
and timeouts in a clean “Haskell like” way.
Testing Haskell Code
The example in this section is found in the directory haskell_tutorial_cookbook_examples/Testing-
Haskell.
If you use stack to create a new project then the framework for testing is generated for you:
$ stack new TestingHaskell
$ cd TestingHaskell
LICENSE
Setup.hs
TestingHaskell.cabal
stack.yaml
TestingHaskell//app:
Main.hs
TestingHaskell//src:
Lib.hs
TestingHaskell//test:
Spec.hs
$ cat test/Spec.hs
main :: IO ()
main = putStrLn "Test suite not yet implemented"
$ stack setup
$ stack build
¹⁸https://wiki.haskell.org/Exception
Tutorial on Pure Haskell Programming
This stack generated project is more complex than the project I created manually in the directory
haskell_tutorial_cookbook_examples/Pure. The file Setup.hs is a placeholder and uses any module
named Main in the app directory. This module, defined in app/Main.hs, imports the module Lib
defined in src/Lib.hs.
The generated test does not do anything, but let’s run it anyway:
$ stack test
Registering TestingHaskell-0.1.0.0...
TestingHaskell-0.1.0.0: test (suite: TestingHaskell-test)
Progress: 1/2 Test suite not yet implemented
Completed 2 action(s).
In the generated project, I made a few changes:
• removed src/Lib.hs
• added src/MyColors.hs providing the type MyColors that we defined earlier
• modified app/Main.hs to use the MyColors type
• added tests to test/Spec.hs
Here is the contents of TestingHaskell/src/MyColors.hs:
module MyColors where
data MyColors = Orange | Red | Blue | Green | Silver
deriving (Show, Eq)
instance Ord MyColors where
compare c1 c2 = compare (show c1) (show c2)
And the new test/Spec.hs file:
Tutorial on Pure Haskell Programming
import Test.Hspec
import MyColors
main :: IO ()
main = hspec spec
spec :: Spec
spec = do
describe "head" $ do
it "test removing first list element" $ do
head [1,2,3,4] `shouldBe` 1
head ["the", "dog", "ran"] `shouldBe` "dog" -- should fail
describe "MyColors tests" $ do
it "test custom 'compare' function" $ do
MyColors.Green < MyColors.Red `shouldBe` True
Red > Silver `shouldBe` True
-- should fail
Notice how two of the tests are meant to fail as an example. Let’s run the tests:
$ stack test
TestingHaskell-0.1.0.0: test (suite: TestingHaskell-test)
Progress: 1/2
test removing first list element FAILED [1]
MyColors tests
test custom 'compare' function FAILED [2]
Failures:
test/Spec.hs:13:
1) head test removing first list element
expected: "dog"
but got: "the"
test/Spec.hs:17:
2) MyColors tests test custom 'compare' function
expected: True
but got: False
Randomized with seed 1233887367
Tutorial on Pure Haskell Programming
Finished in 0.0139 seconds
2 examples, 2 failures
Completed 2 action(s).
Test suite failure for package TestingHaskell-0.1.0.0
TestingHaskell-test:
exited with: ExitFailure 1
Logs printed to console
In line one with stack test we are asking stack to run app tests in the subdirectory test. All Haskell
source files in subdirectory test are assumed to be test files. In the listing for file test/Spec.hs we have
two tests that fail on purpose and you see the output for the failed tests at lines 12-15 and 17-20.
Because the Haskell compiler does such a good job at finding type errors I have fewer errors in my
Haskell code compared to languages like Ruby and Common Lisp. As a result I find myself writing
fewer tests for my Haskell code than I would write in other languages. Still, I recommend some tests
for each of your projects; decide for yourself how much relative effort you want to put into writing
tests.
Pure Haskell Wrap Up
I hope you are starting to get an appreciation for using composition of functions and higher order
functions to enable us to compose programs from smaller pieces that can be joined together.
This composition is made easier when using pure functions that always return the same value when
called with the same type of arguments.
We will continue to see examples of how lazy evaluation simplifies code because we can use infinitely
large lists with the assurance that values are not calculated until they are needed.
In addition to Haskell code generally having fewer errors (after it gets by the compiler!) other
advantages of functional programming include more concise code that is easy to read and understand
once you get some experience with the language.
Tutorial on Impure Haskell
Programming
One of the great things about Haskell is that the language encourages us to think of our code in two
parts:
• Pure functional code (functions have no side effects) that is easy to write and test. Functional
code tends to be shorter and less likely to be imperative (i.e., more functional, using maps and
recursion, and less use of loops as in Java or C++).
• Impure code that deals with side effects like file and network IO, maintaining state in a typesafe
way, and isolate imperative code that has side effects.
In his excellent Functional Programming with Haskell class at eDX¹⁹ Erik Meijer described pure code
as being islands in the ocean and the ocean representing impure code. He says that it is a design
decision how much of your code is pure (islands) and how much is impure (the ocean). This model
of looking at Haskel programs works for me.
My use the word “impure” is common for refering to Haskell code with side effects. Haskell is a
purely functional language and side effects like I/O are best handled in a pure functional way using
by wrapping pure values in Mondads.
In addition to showing you reusable examples of impure code that you will likely need in your own
programs, a major theme of this chapter is handling impure code in a convenient type safe fashion.
Any Monad, which wraps a single value, is used to safely manage state. I will introduce you to
using Monad types as required for the examples in this chapter. This tutorial style introduction will
prepare you for understanding the sample applications later.
Hello IO () Monad
I showed you many examples of pure code in the last chapter but most examples in source files (as
opposed to those shown in a GHCi repl) had a bit of impure code in them: the main function like
the following that simply writes a string of characters to standard output:
main = do
print "hello world"
The type of function main is:
¹⁹http://edx.org
Tutorial on Impure Haskell Programming
*Main> :t main
main :: IO ()
The IO () monad is an IO value wrapped in a type safe way. Because Haskell is a lazy evaluation
language, the value is not evaluated until it is used. Every IO () action returns exactly one value.
Think of the word “mono” (or “one”) when you think of Monads because they always return one
value. Monads are also used to connnect together parts of a program.
What is it about the function main in the last example that makes its type an IO ()? Consider the
simple main function here:
module NoIO where
main = do
let i = 1 in
and its type:
*Main> :l NoIO
[1 of 1] Compiling NoIO
( NoIO.hs, interpreted )
Ok, modules loaded: NoIO.
*NoIO> main
*NoIO> :t main
main :: Integer
*NoIO>
OK, now you see that there is nothing special about a main function: it gets its type from the type
of value returned from the function. It is common to have the return type depend on the function
argument types. The first example returns a type IO () because it returns a print do expression:
*Main> :t print
print :: Show a => a -> IO ()
*Main> :t putStrLn
putStrLn :: String -> IO ()
The function print shows the enclosing quote characters when displaying a string while putStrLn
does not. In the first example, what happens when we stitch together several expressions that have
type IO ()? Consider:
Tutorial on Impure Haskell Programming
main = do
print 1
print "cat"
Function main is still of type IO (). You have seen do expressions frequently in examples and now
we will dig into what the do expression is and why we use it.
The do notation makes working with monads easier. There are alternatives to using do that we will
look at later.
One thing to note is that if you are doing bindings inside a do expression using a let with a in
expression, you need to wrap the bindings in a new (inner) do expression if there is more than one
line of code following the let statement. The way to avoid requiring a nested do expression is to not
use in in a let expression inside a do block of code. Yes, this sounds complicated but let’s clear up
any confusion by looking at the examples found in the file ImPure/DoLetExample.hs (you might also
want to look at the similar example file ImPure/DoLetExample2.hs that uses bind operators instead
of a do statement; we will look at bind operators in the next section):
module DoLetExample where
example1 = do
-- good style
putStrLn "Enter an integer number:"
s <- getLine
let number = (read s :: Int) + 2
putStrLn $ "Number plus 2 = " ++ (show number)
example2 = do
-- avoid using "in" inside a do statement
putStrLn "Enter an integer number:"
s <- getLine
let number = (read s :: Int) + 2 in
putStrLn $ "Number plus 2 = " ++ (show number)
example3 = do
-- avoid using "in" inside a do statement
putStrLn "Enter an integer number:"
s <- getLine
let number = (read s :: Int) + 2 in
do -- this do is required since we have two dependent statements:
putStrLn "Result is:"
putStrLn $ "Number plus 2 = " ++ (show number)
main = do
example1
example2
example3
Tutorial on Impure Haskell Programming
You should use the pattern in function example1 and not the pattern in example2. The do expression
is syntactic sugar that allows programmers to string together a sequence of operations that can mix
pure and impure code.
To be clear, the left arrow <- is used when the expression on the right side is some type of IO () that
needs to be lifted before being used. A let do expression is used when the right side expression is a
pure value.
On lines 6 and 12 we are using function read to converting a string read out of IO String () to an
integer value. Remember that the value of s (from calling readLine) is an IO () so in the same way
you might read from a file, in this example we are reading a value from an IO () value.
A Note About >> and >>= Operators
So far in this book I have been using the syntactic sugar of the do expression to work with Monads
like IO () and I will usually use this syntactic sugar for the rest of this book.
Even though I find it easier to write and read code using do, many Haskell programmers prefer >>
and >>= so let’s go over these operators so you won’t be confused when reading Haskell code that
uses them. Also, when we use do expressions in code the compiler generates similar code using these
>> and >>= operators.
The Monad type class defines the operators >>= and return. We turn to the GHCi repl to experiment
with and learn about these operators:
*Main> :t (>>)
(>>) :: Monad m => m a -> m b -> m b
*Main> :t (>>=)
(>>=) :: Monad m => m a -> (a -> m b) -> m b
*Main> :t return
return :: Monad m => a -> m a
We start with the return function type return :: Monad m ⇒ a -> m a which tells us that for a
monad m the function return takes a value and wraps it in a monad. We will see examples of the
return function used to return a wrapped value from a function that returns IO () values. The bind
operator (>>) is used to evaluate two expressions in sequence. As an example, we can replace this
do expression:
main = do
example1
example2
example3
with the following:
Tutorial on Impure Haskell Programming
main = example1 >> example2 >> example3
The operator >>= is similar to >> except that it evaluates the left hand expression and pipes its value
into the right hand side expression. The left hand side expression is evaluated to some type of IO
() and the expression on the right hand side typically reads from the input IO (). An example will
make this simpler to understand:
module DoLetExample3 where
example3 =
putStrLn "Enter an integer number:" >>
getLine
example4 mv = do
let number = (read mv :: Int) + 2
putStrLn $ "Number plus 2 = " ++ (show number)
main = example3 >>= example4
Note that I could have used a do statement to define function example3 but used a bind operator
instead. Let’s run this example and look at the function types. Please don’t just quickly read through
the following listing; when you understand what is happening in this example then for the rest of
your life programming in Haskell things will be easier for you:
*DoLetExample3> main
Enter an integer number:
Number plus 2 = 3
*DoLetExample3> :t example3
example3 :: IO String
*DoLetExample3> :t example4
example4 :: String -> IO ()
*DoLetExample3> :t main
main :: IO ()
*DoLetExample3> let x = example3
*DoLetExample3> x
Enter an integer number:
*DoLetExample3> :t x
x :: IO String
*DoLetExample3> x >>= example4
Enter an integer number:
Number plus 2 = 5
Tutorial on Impure Haskell Programming
The interesting part starts at line 11 when we define x to be the returned value from calling example3.
Remember that Haskell is a lazy language: evaluation is postponed until a value is actually used.
Working inside a GHCi repl is like working interactively inside a do expression. When we evaluate
x in line 12 then the code in function example3 is actually executed (notice this is where the user
prompt to enter a number occurs). In line 18 we are re-evaluationg the value in x and passing the
resulting IO String () value to the function example4.
Haskell is a “piecemeal” programming language as are the Lisp family of languages where a repl
is used to write little pieces code that are collected into programs. For simple code in Haskell (and
Lisp languages) I do sometimes directly enter code into a text editor but very ofter I start in a repl,
experiment, debug, refine, and then copy into an edited file.
Console IO Example with Stack Configuration
The directory CommandLineApps contains two simple applications that interact with STDIO, that
is to write to the console and read from the keyboard. The first example can be found in file
CommandLineApp/CommandLine1.hs:
module Main where
import System.IO
import Data.Char (toUpper)
main = do
putStrLn "Enter a line of text for test 1:"
s <- getLine
putStrLn $ "As upper case:\t" ++ (map toUpper s)
Lines 3 and 4 import the entire System.IO module (that is, import all exported symbols from
System.IO) and just the function toUpper from module Data.Char. System.IO is a standard Haskell
module and we do not have to do anything special to import it. The Data.Char is stored in the
package text. The package text is contained in the library package base which is specified in the
CommandLineApp.cabal configuration file that we will look at soon.
Use of the <- assignment in line 8 in the last Haskell listing is important to understand. It might
occur to you to leave out line 8 and just place the getLine function call directly in line 9, like this:
putStrLn $ "As upper case:\t" ++ (map toUpper getLine)
If you try this (please do!) you will see compilation errors like:
Tutorial on Impure Haskell Programming
Couldn't match expected type ‘[Char]’ with actual type ‘IO String’
In the second argument of ‘map’, namely ‘getLine’
In the second argument of ‘(++)’, namely ‘(map toUpper getLine)’
The type of getLine is an IO () that is a wrapped IO call. The value is not computed until it is used.
The <- assignment in line 8 evaluates the IO call and unwraps the result of the IO operation so that
it can be used.
I don’t spend much time covering stack project configuration files in this book but I do recommend
that as you work through examples to also look for a file in each example directory ending with the
file extension .cabal that specified which packages need to be loaded. For some examples it might
take a while to download and configure libraries the first time you run either stack build or stack
ghci in an example directory.
The Haskell stack project in the CommandLineApp directory has five target applications as we
can see in the CommandLineApp.cabal file. I am not going to go into much detail about the project
cabal and stack.yaml files generated by stack when you create a new project except for configuration
data that I had to add manually; in this case, I added two executable targets at the end of the cabal
file (note: the project in the github repository for this book has more executable targets, I just show
a few here):
executable CommandLine1
hs-source-dirs:
main-is:
CommandLine1.hs
default-language:
Haskell2010
build-depends:
base >= 4.7 && < 5
executable CommandLine2
hs-source-dirs:
main-is:
CommandLine2.hs
default-language:
Haskell2010
build-depends:
base >= 4.7 && < 5
executable ReadTextFile
hs-source-dirs:
main-is:
ReadTextFile.hs
default-language:
Haskell2010
build-depends:
base >= 4.7 && < 5
executable GameLoop1
hs-source-dirs:
main-is:
GameLoop1.hs
default-language:
Haskell2010
build-depends:
base >= 4.7 && < 5, time
Tutorial on Impure Haskell Programming
executable GameLoop2
hs-source-dirs:
main-is:
GameLoop2.hs
default-language:
Haskell2010
build-depends:
base >= 4.7 && < 5, random
The executable name determines the compiled and linked executable file name. For line 1, an
executable file “CommandLine1” (or “CommandLine1.exe”” on Windows) will be generated. The
parameter hs-source-dirs is a comma separated list of source file directories. In this simple example
all Haskell source files are in the project’s top level directory “../”. The build-depends is a comma
separated list of module libraries; here we only use the base built-in modules packaged with Haskell.
Let’s use a GHCi repl to poke at this code and understand it better. The project defined in
CommandLineApp/CommandLineApp.cabal contains many executable targets so when we enter
a GHCi repl, the available targets are shown and you can choose one; in this case I am selecting the
first target defined in the cabal file. In later GHCi repl listings, I will edit out this output for brevity:
$ stack ghci
* * * * * * * *
The main module to load is ambiguous. Candidates are:
1. Package `CommandLineApp' component exe:CommandLine1 with main-is file: /Users/mar\
kw/GITHUB/haskell_tutorial_cookbook_examples/CommandLineApp/CommandLine1.hs
2. Package `CommandLineApp' component exe:CommandLine2 with main-is file: /Users/mar\
kw/GITHUB/haskell_tutorial_cookbook_examples/CommandLineApp/CommandLine2.hs
3. Package `CommandLineApp' component exe:ReadTextFile with main-is file: /Users/mar\
kw/GITHUB/haskell_tutorial_cookbook_examples/CommandLineApp/ReadTextFile.hs
You can specify which one to pick by:
* Specifying targets to stack ghci e.g. stack ghci CommandLineApp:exe:CommandLine1
* Specifying what the main is e.g. stack ghci --main-is CommandLineApp:exe:CommandL\
* Choosing from the candidate above [1..3]
* * * * * * * *
Specify main module to use (press enter to load none): 1
Loading main module from cadidate 1, --main-is /Users/markw/GITHUB/haskell_tutorial_\
cookbook_examples/CommandLineApp/CommandLine1.hs
Configuring GHCi with the following packages: CommandLineApp
GHCi, version 7.10.3: http://www.haskell.org/ghc/
:? for help
Ok, modules loaded: none.
[1 of 1] Compiling Main
( /Users/markw/GITHUB/haskell_tutorial_cookbook_\
Tutorial on Impure Haskell Programming
examples/CommandLineApp/CommandLine1.hs, interpreted )
Ok, modules loaded: Main.
*Main> :t main
main :: IO b
*Main> :info main
main :: IO b
-- Defined at /Users/markw/GITHUB/haskell_tutorial_cookbook_examples/CommandLineApp/\
CommandLine1.hs:6:1
*Main> :t getLine
getLine :: IO String
*Main> :t putStrLn
putStrLn :: String -> IO ()
*Main> main
Enter a line of text for test 1:
line 1
As upper case:
LINE 1
Enter a line of text for test 1:
line 2
As upper case:
LINE 2
Enter a line of text for test 1:
^C Interrupted.
*Main>
In line 36 the function getLine is of type getLine :: IO String which means that calling getLine
returns a value that is a computation to get a line of text from stdio but the IO operation is not
performed until the value is used.
Please note that it is unusual to put five executable targets in a project’s cabal file. I am only doing so
here because I wanted to group five similar examples together in this subdirectory of the github repo
for this book²⁰. This repo has 16 example subdirectories, and the number would be much greater if
I didn’t collect similar examples together.
We will use the example in file CommandLine2.hs in the next section which is similar to this example
but also appends the user input to a text file.
File IO
We will now look at a short example of doing file IO. We will write Haskell simple string values
to a file. If you are using the more efficient Haskell Text values, the code is the same. Text values
are more efficient than simple string values when dealing with a lot of data and we will later use
a compiler setting to automatically convert between the underlying formats. The following listing
shows CommandLineApp/CommandLine2.hs:
²⁰https://github.com/mark-watson/haskell_tutorial_cookbook_examples
Tutorial on Impure Haskell Programming
module Main where
import System.IO
import Data.Char (toUpper)
main = do
putStrLn "Enter a line of text for test2:"
s <- getLine
putStrLn $ "As upper case:\t" ++ (map toUpper s)
appendFile "temp.txt" $ s ++ "\n"
Note the use of recursion in line 11 to make this program loop forever until you use a COntrol-c to
stop the program.
In line 10 we are using function appendFile to open a file, append a string to it, and then close the
file. appendFile is of type appendFile :: FilePath -> String -> IO (). It looks like we are passing a
simple string as a file name instead of type FilePath but if you look up the definition of FilePath
you will see that it is just an alias for string: type FilePath = String.
Running this example in a GHCi repl, with much of the initial printout from running stack ghci not
shown:
$ stack ghci
CommandLineApp-0.1.0.0: configure
Specify main module to use (press enter to load none): 2
Ok, modules loaded: Main.
*Main> main
Enter a line of text for test2:
line 1
As upper case:
LINE 1
Enter a line of text for test2:
line 2
As upper case:
LINE 2
Enter a line of text for test2:
^C Interrupted.
*Main>
The file temp.txt was just created.
The next example used ReadTextFile.hs to read the file temp.txt and process the text by finding all
words in the file:
Tutorial on Impure Haskell Programming
module Main where
import System.IO
import Control.Monad
main = do
entireFileAsString <- readFile "temp.txt"
print entireFileAsString
let allWords = words entireFileAsString
print allWords
readFile is a high-level function because it manages for you reading a file and closing the file handle
it uses internally. The built in function words splits a string on spaces and returns a list of strings
[String] that are printed on line 7:
$ stack ghci
CommandLineApp-0.1.0.0: build
Specify main module to use (press enter to load none): 3
Ok, modules loaded: ReadTextFile.
*ReadTextFile> main
"line 1\nline 2\n"
["line","1","line","2"]
*ReadTextFile>
*ReadTextFile> :t readFile
readFile :: FilePath -> IO String
*ReadTextFile> :type words
words :: String -> [String]
What if the function readFile encounters an error? That is the subject for the next section.
Error Handling in Impure Code
I know you have been patiently waiting to see how we handle errors in Haskell code. Your wait is
over! We will look at several common types of runtime errors and how to deal with them. In the last
section we used the function readFile to read the contents of a text file temp.txt. What if temp.txt
does not exist? Well, then we get an error like the following when running the example program in
ReadTextFile.hs:
*Main> main
*** Exception: temp.txt: openFile: does not exist (No such file or directory)
Tutorial on Impure Haskell Programming
Let’s modify this last example in a new file ReadTextFileErrorHandling.hs that catches a file not
found error. The following example is derived from the first example in Michael Snoyman’s article
Catching all exceptions²¹. This example does not work inside threads; if you need to catch errors
inside a thread then see the second example in Michael’s article.
module Main where
import System.IO
import Control.Exception
-- catchAny by Michael Snoyman:
catchAny :: IO a -> (SomeException -> IO a) -> IO a
catchAny = Control.Exception.catch
safeFileReader :: FilePath -> IO String
safeFileReader fPath = do
entireFileAsString <- catchAny (readFile "temp.txt") $ \error -> do
putStrLn $ "Error: " ++ show error
return ""
return entireFileAsString
main :: IO ()
main = do
fContents <- safeFileReader "temp.txt"
print fContents
print $ words fContents
I will run this twice: the first time without the file temp.txt present and a second time with temp.txt
in the current durectory:
*Main> :l ReadTextFileErrorHandling.hs
[1 of 1] Compiling Main
( ReadTextFileErrorHandling.hs, interpreted )
Ok, modules loaded: Main.
*Main> main
Error: temp.txt: openFile: does not exist (No such file or directory)
*Main> main
"line 1\nline 2\n"
["line","1","line","2"]
²¹https://www.schoolofhaskell.com/user/snoyberg/general-haskell/exceptions/catching-all-exceptions
Tutorial on Impure Haskell Programming
Until you need to handle runtime errors in a multi-threaded Haskell program, following this example
should be sufficient. In the next section we look at Network IO.
Network IO
We will experiment with three network IO examples in this book:
• A simple socket client/server example in this section.
• Reading web pages in the chapter “Web Scraping”
• Querying remote RDF endpoints in the chapter “Linked Data and the Semantic Web”
We start by using a high level library, network-simple for both the client and serve examples in
the next two sub-sections. The client and sever examples are in the directory haskell_tutorial_-
cookbook_examples/ClientServer in the files Client.hs and Server.hs.
Server Using network-simple Library
The Haskell Network and Network.Simple modules use strings represented as Data.ByteString.Char8
data so as seen in line 1 I set the language type OverloadedStrings. The following example in file
ClientServer/Server.hs is derived from an example in the network-simple project:
{-# LANGUAGE OverloadedStrings #-}
module Server where
import Control.Monad
import qualified Data.ByteString.Char8 as B
import qualified Network.Simple.TCP as T
reverseStringLoop sock = do
mbs <- T.recv sock 4096
case mbs of
Just bs -> T.send sock (B.reverse bs) >> reverseStringLoop sock
Nothing -> return ()
main :: IO ()
main = T.withSocketsDo $ do -- derived from library example
T.listen "*" "3000" $ \(lsock, laddr) -> do
putStrLn $ "Listening at " ++ show laddr
forever . T.acceptFork lsock $ \(sock, addr) -> do
putStrLn $ "Connection from " ++ show addr
reverseStringLoop sock
Tutorial on Impure Haskell Programming
The server accepts a string, reverses the string, and returns the reversed string to the client.
I am assuming that you have done some network programming and are familiar with sockets, etc.
The function reverseStringLoop defined in lines 9-13 accepts a socket as a parameter and returns
a value of type MonadIO that wraps a byte-string value. In line 10 we use the T.recv function that
takes two arguments: a socket and the maximum number of bytes to received from the client. The
case expression reverses the received byte string, sends the reversed string back to the client, and
recursively calls itself waiting for new data from the client. If the client breaks the socket connection,
then the function retuns an empty MonadIO().
The main function defined in lines 15-21 listens on port 3000 for new client socket connections. In
line 19, the function T.acceptFork accepts as an argument a socket value and a function to execute;
the complete type is:
*Main> :t T.acceptFork
T.acceptFork
:: transformers-0.4.2.0:Control.Monad.IO.Class.MonadIO m =>
T.Socket
-> ((T.Socket, T.SockAddr) -> IO ()) -> m GHC.Conc.Sync.ThreadId
Don’t let line 3 scare you; the GHCi repl is just showing you where this type of MonadIO is defined.
The return type refers to a thread ID that is passed to the function forever :: Monad m ⇒ m a -> m
b that is defined in the module Control.Monad and lets the thread run until it teminates.
The network-simple package is fairly high level and relatively simple to use. If you are interested
you can find many client/server examples on the web that use the lower-level network package.
We will develop a client application to talk with this server in the next section but if you want to
immediately try the server, start it and then run telnet in another terminal window:
Prelude> :l Server
[1 of 1] Compiling Server
( Server.hs, interpreted )
Ok, modules loaded: Server.
*Main> main
Listening at 0.0.0.0:3000
And run telnet:
Tutorial on Impure Haskell Programming
$ telnet localhost 3000
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
The dog ran down the street
teerts eht nwod nar god ehT
In the next section we write a simple client to talk with this service example.
Client Using network-simple Library
I want to use automatic conversion between strings represented as Data.ByteString.Char8 data and
regular [Char] strings so as seen in line 1 I set the language type OverloadedStrings in the example
in file Client.hs:
{-# LANGUAGE OverloadedStrings #-}
module Client where
import Control.Monad
import qualified Network.Simple.TCP as T
main = do
T.connect "127.0.0.1" "3000" $ \(connectionSocket, remoteAddr) -> do
putStrLn $ "Connection established to " ++ show remoteAddr
T.send connectionSocket "test123"
response <- T.recv connectionSocket 100
case response of
Just s -> putStrLn $ "Response: " ++ show s
Nothing -> putStrLn "No response from server"
The function T.connect in line 9 accepts arguments for a host name, a port, and a function to call
with the connection socket to the server and the server’s address. The body of this inline function,
defined in in the middle on line 9 and continuing in lines 10-15, prints the server address, sends a
string “test123” to the server, and waits for a response back from the server (T.recv in line 12). The
server response is printed, or a warning that no response was received.
While the example in file Server.hs is running in another terminal, we can run the client interactively:
Tutorial on Impure Haskell Programming
Prelude> :l Client.hs
[1 of 1] Compiling Client
( Client.hs, interpreted )
Ok, modules loaded: Client.
*Main main
Connection established to 127.0.0.1:3000
Response: "321tset"
A Haskell Game Loop that Maintains State
Functionally
The example in this section can be found in the file GameLoop2.hs in the directory haskell_tuto-
rial_cookbook_examples/CommandLineApp. This example uses the random package to generate
a seed random number for a simple number guessing game. An alternative implementation in
GameLoop1.hs, which I won’t discuss, uses the system time to generate a seed.
This is an important example because it demonstrates one way to maintain state in a functional
way. We have a read-only game state value that is passed to the function gameLoop which modifies
the read-only game state passed as an argument and returns a newly constructed game state as the
function’s returned value. This is a common pattern that we will see again later when we develop an
application to play a simplified version of the card game Blackjack in the chapter “Haskell Program
to Play the Blackjack Card Game.”
module GameLoop2 where
import System.Random
data GameState = GameState { numberToGuess::Integer, numTries::Integer}
deriving (Show)
gameLoop :: GameState -> IO GameState
gameLoop gs = do
print $ numberToGuess gs
putStrLn "Enter a number:"
s <- getLine
let num = read s :: Integer
if num == numberToGuess gs then
return gs
else gameLoop $ GameState (numberToGuess gs) ((numTries gs) + 1)
main = do
pTime <- randomRIO(1,4)
Tutorial on Impure Haskell Programming
let gameState = GameState pTime 1
print "Guess a number between 1 and 4"
gameLoop gameState
You notice in line 12 that since we are inside of a do expression we can lift (or unwrap) the IO String
() value returned from getLine to a string value that we can use directly. This is a pattern we will
use repeatedly. The value returned from getLine is not used until line 13 when we use function read
to extract the value from the IO String () value getLine returned.
In the if expression in lines 14-16 we check if the user has input the correct value and can then
simply return the input game state to the calling main function. If the user has not guessed the
correct number then in line 16 we create a new game state value and call the function gameLoop
recursively with the newly constructed game state.
The following listing shows a sample session playing the number guessing game.
Prelude> :l GameLoop2.hs
[1 of 1] Compiling GameLoop2
( GameLoop2.hs, interpreted )
Ok, modules loaded: GameLoop2.
*GameLoop2> main
"Guess a number between 1 and 4"
Enter a number:
Enter a number:
Enter a number:
GameState {numberToGuess = 4, numTries = 3}
*GameLoop2> main
"Guess a number between 1 and 4"
Enter a number:
Enter a number:
GameState {numberToGuess = 2, numTries = 2}
*GameLoop2>
We will use this pattern for maintaining state in a game in the later chapter “Haskell Program to
Play the Blackjack Card Game.”
Efficiency of Haskell Strings
Except for the Client/Server example, so far we have been mostly using simple String values where
String is a list of characters [Char]. For longer strings it is much more efficient to use the module
Tutorial on Impure Haskell Programming
Data.Text²² that is defined in package text (so text needs to be added to the dependencies in your
cabal file).
Many Haskell libraries use the simple String type but the use of Data.Text is also common, especially
in applications handling large amounts of string data. We have already seen examples of this in the
client/server example programs. Fortunately Haskell is a strongly typed language that supports a
language extension for automatically handling both simple strings and the more efficient text types.
This language extension, as we have seen in a previous example, is activated by adding the following
near the top of a Haskell source file:
{-# LANGUAGE OverloadedStrings
As much as possible I am going to use simple strings in this book and when we need both simple
strings and byte strings I will then use OverloadedStrings for automatic conversion. This conversion
is performed by knowing the type signatures of data and functions in surrounding code. The compiler
figures out what type of string is expected and does the conversion for you.
A More Detailed Look at Monads
We have been casually using different types of IO () monads. In this section I will introduce you to
the State monad and then we will take a deeper look at IO (). While we will be just skimming the
surface of the topic of monads, my goal in this section is to teach you enough to work through the
remaining examples in this book.
Monads are types belonging to the Monad type class that specifies one operator and one function:
class Monad m where
(>>=) :: m a -> (a -> m b) -> m b
return :: a -> m a
The >>= operator takes two arguments: a monad wrapping a value (type a in the above listing) and
a function taking the same type a and returning a monad wrapping a new type b. The return value
of >>= is a new monad wrapping a value of type b.
The Monad type class function return takes any value and wraps it in a new monad. The naming
of return is confusing because it does not alter the flow of execution in a program like a return
statement in Java, rather, it wraps a value in a monad.
State Monad
The definition for the constructor of a State monad is:
²²https://www.stackage.org/nightly-2016-09-18/package/text-1.2.2.1
Tutorial on Impure Haskell Programming
newtype State s a = State { runState :: s -> (a, s) }
So far we have been using data to define new types and newtype is similar except newtype acts
during compile time and no type information is present at runtime. All monads contain a value and
for the State monad this value is a function. The >>= operator is called the bind operator.
The accessor function runState provides the means to access the value in the state. The following
example is in the file StateMonad/State1.hs. In this example, incrementState is a state monad that
increases its wrapped integer value by one when it is executed. Remember that the return function
is perhaps poorly named because it does not immediately “return” from a computation block as it
does in other languages; return simply wraps a value as a monad without redirecting the execution
In order to make the following example more clear, I implement the increment state function twice,
once using the do notation that you are already familiar with and once using the >>= bind operator:
module Main where
import Control.Monad.State
incrementState :: State Int Int
incrementState = do
n <- get
put (n + 1)
return n
-- same state monad without using a 'do' expression:
incrementState2 :: State Int Int
incrementState2 = get >>= \a ->
put (a + 1) >>= \b ->
return a
bumpVals (a,b) = (a+1, b+2)
main = do
print $ runState incrementState 1
-- (1,2) == (return value, final state)
print $ runState incrementState2 1 -- (1,2) == (return value, final state)
print $ runState (mapState bumpVals incrementState) 1 -- (2,4)
print $ evalState incrementState 1
-- 1 == return value
print $ execState incrementState 1
-- 2 == final state
Here we have used two very different looking, yet equivalent, styles for accessing and modifying
state monad values. In lines 6-9 we are using the do notation. The function get in line 7 returns one
value: the value wrapped in a state monad. Function put in line 8 replaces the wrapped value in the
Tutorial on Impure Haskell Programming
state monad, in this example by incrementing its numeric value. Finally return wraps the value in
a monad.
I am using the runState function defined in lines 20-24 that returns a tuple: the first tuple value is
the result of the computation performed by the function passed to runState (incrementState and
incrementState2 in these examples) and the second tuple value is the final wrapped state.
In lines 12-15 I reimplemented increment state using the bind function (>>=). We have seen before
that >>= passes the value on its left side to the computation on its right side, that is function calls in
lines 13-15:
\a -> put (a + 1)
\b -> return a
It is a matter of personal taste whether to code using bind or do. I almost always use the do notation
in my own code but I wanted to cover bind both in case you prefer that notation and so you can
also read and understand Haskell code using bind. We continue looking at alternatives to the do
notation in the next section.
Using Applicative Operators <$> and <*>: Finding
Common Words in Files
My goal in this book is to show you a minimal subset of Haskell that is relatively easy to understand
and use for coding. However, a big part of using a language is reading other people’s code so I do
need to introduce a few more constructs that are widely used: applicative operators.
Before we begin I need to introduce you to a new term: Functor which is a typeclass that defines
only one method fmap. fmap is used to map a function over an IO action and has the type signature:
fmap :: Functor f => (a -> b) -> f a -> f b
fmap can be used to apply a pure function like (a -> b) to an IO a and return a new IO b without
unwrapping the original IO (). The following short example (in file ImPure/FmapExample.hs) will
let you play with this idea:
Tutorial on Impure Haskell Programming
module FmapExample where
fileToWords fileName = do
fileText <- readFile fileName
return $ words fileText
main = do
words1 <- fileToWords "text1.txt"
print $ reverse words1
words2 <- fmap reverse $ fileToWords "text1.txt"
print words2
In lines 8-9 I am unwrapping the result of the IO [String] returned by the function fileToWords
and then applying the pure function words to the unwrapped value. Wouldn’t it be nice to operate
on the words in the file without unwrapping the [String] value? You can do this using fmap as seen
in lines 10-11. Please take a moment to understand what line 10 is doing. Here is line 10:
words2 <- fmap reverse $ fileToWords "text1.txt"
First we read the words in a file into an IO [String] monad:
fileToWords "text1.txt"
Then we apply the pure function reverse to the values inside the IO [String] monad, creating a new
fmap reverse $ fileToWords "text1.txt"
Note that from the type of the fmap function, the input monad and output monad can wrap different
types. For example, if we applied the function head to an IO [String] we would get an outut of IO
[Char].
Finally we unwrap the [String] value inside the monad and set words2 to this unwrapped value:
words2 <- fmap reverse $ fileToWords "text1.txt"
In summary, the Functor typeclass defines one method fmap that is useful for operating on data
wrapped inside a monad.
We will now implement a small application that finds common words in two text files, implementing
the primary function three times, using:
• The do notation.
• The >>= bind operator.
• The Applicative operators <$> and <*>
Let’s look at the types for these operators:
Tutorial on Impure Haskell Programming
(<$>) :: Functor f => (a -> b) -> f a -> f b
(<*>) :: Applicative f => f (a -> b) -> f a -> f b
We will use both <$> and <*> in the function commonWords3 in this example and I will explain
how these operators work after the following program listing.
This practical example will give you a chance to experiment more with Haskell (you do have a GHCi
repl open now, right?). The source file for this example is in the file ImPure/CommonWords.hs:
module CommonWords where
import Data.Set (fromList, toList, intersection)
import Data.Char (toLower)
fileToWords fileName = do
fileText <- readFile fileName
return $ (fromList . words) (map toLower fileText)
commonWords file1 file2 = do
words1 <- fileToWords file1
words2 <- fileToWords file2
return $
toList $ intersection words1 words2
commonWords2 file1 file2 =
fileToWords file1 >>= \f1 ->
fileToWords file2 >>= \f2 ->
return $
toList $ intersection f1 f2
commonWords3 file1 file2 =
(\f1 f2 -> toList $ intersection f1 f2)
<$> fileToWords file1
<*> fileToWords file2
main = do
cw <- commonWords "text1.txt" "text2.txt"
print cw
cw2 <- commonWords "text1.txt" "text2.txt"
print cw2
cw3 <- commonWords "text1.txt" "text2.txt"
print cw3
The function fileToWords defined in lines 6-8 simply reads a file, as in the last example, maps
contents of the file to lower case, uses words to convert a String to a [String] list of individual
Tutorial on Impure Haskell Programming
words, and uses the function Data.Set.fromList to create a set from a list of words that in general
will have duplicates. We are retuning an IO (Data.Set.Base.Set String) value so we can later perform
a set intersection operation. In other applications you might want to apply Data.Set.toList before
returning the value from fileToWords so the return type of the function would be IO [String].
The last listing defines three similar functions commonWords, commonWords2, and common-
Words3.
commonWords defined in lines 10-13 should hopefully look routine and familiar to you now. We
set the local variables with the unwrapped (i.e., extracted from a monad) contents of the unique
words in two files, and then return monad wrapping the intersection of the words in both files.
The function commonWords2 is really the same as commonWords except that it uses the bind >>=
operator instead of the do notation.
The interesting function in this example is commonWords3 in lines 20-23 which uses the applicative
operators <$> and <*>. Notice the pure function defined inline in line 21: it takes two arguments of
type set and returns the set intersection of the arguments. The operator <$> takes a function on
the left side and a monad on the right side which contains the wrapped value to be passed as the
argument f1. <*> supplies the value for the inline function arguments f2. To rephrase how lines 21-23
work: we are calling fileToWords twice, both times getting a monad. These two wrapped monad
values are passed as arguments to the inline function in line 21 and the result of evaluating this
inline function is returned as the value of the function commonWords3.
I hope that this example has at least provided you with “reading knowledge” of the Applicative
operators <$> and <*> and has also given you one more example of replacing the do notation with
the use of the bind >>= operator.
List Comprehensions Using the do Notation
We saw examples of list comprehensions in the last chapter on pure Haskell programming. We can
use return to get lists values that are instances of type Monad:
*Prelude> :t (return [])
(return []) :: Monad m => m [t]
*Prelude> :t (return [1,2,3])
(return [1,2,3]) :: (Monad m, Num t) => m [t]
*Prelude> :t (return ["the","tree"])
(return ["the","tree"]) :: Monad m => m [[Char]]
We can get list comprehension behavior from the do notation (here I am using the GHCi repl :{ and
:} commands to enter multiple line examples):
Tutorial on Impure Haskell Programming
*Main> :{
*Main| do num <- [1..3]
*Main|
animal <- ["parrot", "ant", "dolphin"]
*Main|
return (num, animal)
*Main| :}
[(1,"parrot"),(1,"ant"),(1,"dolphin"),
(2,"parrot"),(2,"ant"),(2,"dolphin"),
(3,"parrot"),(3,"ant"),(3,"dolphin")]
I won’t use this notation further but you now will recognize this pattern if you read it in other
people’s code.
Dealing With Time
In the example in this section we will see how to time a block of code (using two different methods)
and how to set a timeout for code that runs in an IO ().
The first way we time a block of code uses getPOSIXTime and can be used to time pure or impure
code. The second method using timeIt takes an IO () as an argument; in the following example I
wrapped pure code in a print function call which returns an IO () as its value. The last example in
the file TimerTest.hs shows how to run impure code wrapped in a timeout.
module Main where
import Data.Time.Clock.POSIX -- for getPOSIXTime
import System.TimeIt
-- for timeIt
import System.Timeout
-- for timeout
anyCalculationWillDo n =
-- a function that can take a while to run
take n $ sieve [2..]
sieve (x:xs) =
x:sieve [y | y <- xs, rem y x > 0]
main = do
startingTime <- getPOSIXTime
print startingTime
print $ last $ take 20000001 [0..]
endingTime <- getPOSIXTime
print endingTime
print (endingTime - startingTime)
timeIt $ print $ last $ anyCalculationWillDo 2000
Tutorial on Impure Haskell Programming
let somePrimes = anyCalculationWillDo 3333 in
timeIt $ print $ last somePrimes
-- 100000 microseconds timeout tests:
timeout 100000 $ print "simple print **do** expression did not timeout"
timeout 100000 $ print $ last $ anyCalculationWillDo 4
timeout 100000 $ print $ last $ anyCalculationWillDo 40
timeout 100000 $ print $ last $ anyCalculationWillDo 400
timeout 100000 $ print $ last $ anyCalculationWillDo 4000
timeout 100000 $ print $ last $ anyCalculationWillDo 40000
print $ anyCalculationWillDo 5
I wanted a function that takes a while to run so for anyCalculationWillDo (lines 7 to 11) I
implemented an inefficient prime number generator.
When running this example on my laptop, the last two timeout calls (lines 26 and 31) are terminated
for taking more than 100000 microseconds to execute.
The last line 32 of code prints out the first 5 prime numbers greater than 1 so you can see the results
of calling the time wasting test function anyCalculationWillDo.
$ stack build --exec TimerTest
1473610528.2177s
20000000
1473610530.218574s
2.000874s
CPU time:
CPU time:
"simple print **do** expression did not timeout"
[2,3,5,7,11]
The timeout function is useful for setting a maximum time that you are willing to wait for a
calculation to complete. I mostly use timeout for timing out operations fetching data from the web.
Using Debug.Trace
Inside an IO you can use print statements to understand what is going on in your code when
debugging. You can not use print statements inside pure code but the Haskell base library contains
Tutorial on Impure Haskell Programming
the trace functions that internally perform impure writes to stdout. You do not want to use these
debug tools in production code.
As an example, I have rewritten the example from the last section to use Debug.Trace.trace and
Debug.Trace.traceShow:
module Main where
import Debug.Trace
(trace, traceShow) -- for debugging only!
anyCalculationWillDo n =
("+++ anyCalculationWillDo: " ++ show n) $
anyCalculationWillDo' n
anyCalculationWillDo' n =
take n $ trace ("
-- sieve n:" ++ (show n)) $ sieve [2..]
sieve (x:xs) =
traceShow ("
-- inside sieve recursion") $
x:sieve [y | y <- xs, rem y x > 0]
main = do
print $ anyCalculationWillDo 5
In line 3 we import the trace and showTrace functions:
*Main> :info trace
trace :: String -> a -> a
-- Defined in ‘Debug.Trace’
*Main> :info traceShow
traceShow :: Show a => a -> b -> b
-- Defined in ‘Debug.Trace’
trace takes two arguments: the first is a string that that is written to stdout and the second is a
function call to be evaluated. traceShow is like *trace except that the first argument is cnverted to
a tstring. The output from running this example is:
Tutorial on Impure Haskell Programming
+++ anyCalculationWillDo: 5
-- sieve n:5
-- inside sieve recursion"
-- inside sieve recursion"
-- inside sieve recursion"
-- inside sieve recursion"
-- inside sieve recursion"
[2,3,5,7,11]
I don’t usually like using the trace functions because debugging with them involves slightly
rewriting my code. My preference is to get low level code written interactively in the GHCI repl
so it does not need to be debugged. I very frequently use print statement inside IOs since adding
them requires no significant modification of my code.
Wrap Up
I tried to give you a general fast-start in this chapter for using monads and in general writing impure
Haskell code. This chapter should be sufficient for you to be able to understand and experiment with
the examples in the rest of this book.
This is the end of the first section. We will now look at a variety of application examples using the
Haskell language.
While I expect you to have worked through the previous chapters in order, for the rest of the book
you can skip around and read the material in any order that you wish.
Section 2 - Cookbook
Now that you have worked through the pure and impure Haskell coding tutorials in the first two
chapters we will look at a “cookbook” of techniques and sample applications to solve some common
programming tasks as well as implement a program to play the card game Blackjack.
I expect you, dear reader, to have studied and absorbed the tutorial material on pure and impure
Haskell programming in the first two chapters. If you are new to Haskell, or don’t have much
experience yet, carefully working through these tutorial chapters is a requirement for understanding
the material in the rest of this book.
This section contains the following “recipe” applications:
• Textprocessing CSV Files
• Textprocessing JSON Files
• Using sqlite and Postgres databases
• REST Server Providing JSON Data
• REST Client
• Accessing and Using Linked Data
• Querying Semantic Web RDF Data Sources
• Annotating English text with Wikipedia/DBPedia URIs for entities in the original text. Entities
can be people, places, organizations, etc.
• Play the Blackjack card game
• Machine Learning
• Probabilistic Graph Models
Text Processing
In my work in data science and machine learning, processing text is a core activity. I am a practitioner,
not a research scientist, and in a practical sense, I spend a fair amount of time collecting data (e.g.,
web scraping and using semantic web/linked data sources), cleaning it, and converting it to different
formats.
We will cover three useful techniques: parsing and using CSV (comma separated values) spreadsheet
files, parsing and using JSON data, and cleaning up natural language text that contains noise
characters.
CSV Spreadsheet Files
The comma separated values (CSV) format is a plain text format that all spreadsheet applications
support. The following example illustrates two techniques that we haven’t covered yet:
• Extracting values from the Either type.
• Using destructuring to concisely extract parts of a list.
The Either type Either a b contains either a Left a or a Right b value and is usually used to return
an error in Left or a value in Right. We will using the Data.Either.Unwrap module to unwrap the
Right part of a call to the Text.CSV.parseCSVFromFile function that reads a CSV file and returns
a Left error or the data in the spreadsheet in a list as the Right value.
The destructuring trick in line 15 in the following listing lets us separate the head and rest of a list
in one operation; for example:
*TestCSV> let z = [1,2,3,4,5]
*TestCSV> z
[1,2,3,4,5]
*TestCSV> let x:xs = z
*TestCSV> x
*TestCSV> xs
[2,3,4,5]
Here is how to read a CSV file:
Text Processing
module TestCSV where
import Text.CSV (parseCSVFromFile, CSV)
import Data.Either.Unwrap (fromRight)
readCsvFile :: FilePath -> CSV
readCsvFile fname = do
c <- parseCSVFromFile fname
return $ fromRight c
main = do
c <- readCsvFile "test.csv"
-- includes header and data rows
print $ map head c
-- print header
let header:rows = c -- destructure
print header
print rows
Function readCsvFile reads from a file and returns a CSV. What is a CSV type? You could search
the web for documentation, but dear reader, if you have worked this far learning Haskell, by now
you know to rely on the GHCi repl:
*TestCSV> :i CSV
type CSV = [Text.CSV.Record]
-- Defined in ‘Text.CSV’
*TestCSV> :i Text.CSV.Record
type Text.CSV.Record = [Text.CSV.Field]
-- Defined in ‘Text.CSV’
*TestCSV> :i Text.CSV.Field
type Text.CSV.Field = String
-- Defined in ‘Text.CSV’
So, a CSV is a list of records (rows in the spreadsheet file), each record is a list of fields (i.e., a string
value).
The output when reading the CVS file test.csv is:
Text Processing
Prelude> :l TestCSV
[1 of 1] Compiling TestCSV
( TestCSV.hs, interpreted )
Ok, modules loaded: TestCSV.
*TestCSV> main
[["name"," email"," age"],["John Smith"," jsmith@acmetools.com"," 41"],["June Jones"\
," jj@acmetools.com"," 38"]]
["name","John Smith","June Jones"]
["name"," email"," age"]
[["John Smith"," jsmith@acmetools.com"," 41"],["June Jones"," jj@acmetools.com"," 38\
JSON Data
JSON is the native data format for the Javascript language and JSON has become a popular
serialization format for exchanging data between programs on a network. In this section I will
demonstrate serializing a Haskell type to a string with JSON encoding and then perform the opposite
operation of deserializing a string containing JSON encoded data back to an object.
The first example uses the module Text.JSON.Generic (from the json library) and the second
example uses module Data.Aeson (from the aeson library).
In the first example, we set the language type to include DeriveDataTypeable so a new type definition
can simply derive Typeable which allows the compiler to generate appropriate encodeJSON and
decodeJSON functions for the type Person we define in the example:
{-# LANGUAGE DeriveDataTypeable #-}
module TestTextJSON where
import Text.JSON.Generic
data Person = Person {name::String, email::String}
deriving (Show, Data, Typeable)
main = do
let a = encodeJSON $ Person "Sam" "sam@a.com"
print a
let d = (decodeJSON a :: Person)
print d
print $ name d
print $ email d
Text Processing
Notice that in line 14 that I specified the expected type in the decodeJSON call. This is not strictly
required, the Haskell GHC compiler knows what to do in this case. I specified the type for code
readability. The Haskell compiler wrote the name and email functions for me and I use these
functions in lines 16 and 17 to extract these fields. Here is the output from running this example:
Prelude> :l TestTextJSON.hs
[1 of 1] Compiling TestTextJSON
( TestTextJSON.hs, interpreted )
Ok, modules loaded: TestTextJSON.
*TestTextJSON> main
"{\"name\":\"Sam\",\"email\":\"sam@a.com\"}"
Person {name = "Sam", email = "sam@a.com"}
"sam@a.com"
The next example uses the Aeson library and is similar to this example.
Using Aeson, we set a language type DeriveGeneric and in this case have the Person class derive
Generic. The School of Haskell has an excellent Aeson tutorial²³ that shows a trick I use in this
example: letting the compiler generate required functions for types FromJSON and ToJSON as seen
in lines 12-13.
{-# LANGUAGE DeriveGeneric #-}
module TestJSON where
import Data.Aeson
import GHC.Generics
import Data.Maybe
data Person = Person {name::String, email::String } deriving (Show, Generic)
-- nice trick from School Of Haskell tutorial on Aeson:
instance FromJSON Person
-- DeriveGeneric language setting allows
instance ToJSON Person
-- automatic generation of instance of
-- types deriving Generic.
main = do
let a = encode $ Person "Sam" "sam@a.com"
print a
let (Just d) = (decode a :: Maybe Person)
print d
print $ name d
print $ email d
²³https://www.schoolofhaskell.com/school/starting-with-haskell/libraries-and-frameworks/text-manipulation/json
Text Processing
I use a short cut in line 19, assuming that the Maybe object returned from decode (which the
compiler wrote automatically for the type FromJSON) contains a Just value instead of an empty
Nothing value. So in line 19 I directly unwrap the Just value.
Here is the output from running this example:
Prelude> :l TestAESON.hs
[1 of 1] Compiling TestJSON
( TestAESON.hs, interpreted )
Ok, modules loaded: TestJSON.
*TestJSON> main
"{\"email\":\"sam@a.com\",\"name\":\"Sam\"}"
Person {name = "Sam", email = "sam@a.com"}
"sam@a.com"
Line 5 shows the result of printing the JSON encoded string value created by the call to encode in
line 17 of the last code example. Line 6 shows the decoded value of type Person, and lines 7 and 8
show the inner wrapped values in the Person data.
Cleaning Natural Language Text
I spend a lot of time working with text data because I have worked on NLP (natural language
processing) projects for over 25 years. We will jump into some interesting NLP applications in the
next chapter. I will finish this chapter with strategies for cleaning up text which is often a precursor
to performing NLP.
You might be asking why we would need to clean up text. Here are a few common use cases:
• Text fetched from the web frequently contains garbage characters.
• Some types of punctuation need to be removed.
• Stop words (e.g., the, a, but, etc.) need to be removed.
• Special unicode characters are not desired.
• Sometimes we want white space around punctuation to make tokenizing text easier.
Notice the module statement on line 1 of the following listing: I am exporting functions cleanText
and removeStopWords so they will be visible and available for use by any other modules that import
this module. In line 6 we import intercalate which constructs a string from a space character and an
[String] (i.e., a list of strings); here is an example where instead of adding a space character between
the strings joined together, I add “*” characters:
Text Processing
*CleanText> intercalate "*" ["the", "black", "cat"]
"the*black*cat"
The function cleanText removes garbage characters and makes sure that any punctuation characters
are surrounded by white space (this makes it easier, for example, to determine sentence boundaries).
Function removeStopWords removes common words like “a”, “the”, etc. from text.
module CleanText (cleanText, removeStopWords)
import Data.List.Split (splitOn)
import Data.List (intercalate)
import Data.Char as C
import Data.List.Utils (replace)
noiseCharacters = ['[', ']', '{', '}', '\n', '\t', '&', '^',
'@', '%', '$', '#', ',']
substituteNoiseCharacters :: [Char] -> [Char]
substituteNoiseCharacters =
map (\x -> if elem x noiseCharacters then ' ' else x)
cleanText s =
intercalate
filter
(\x -> length x > 0) $
splitOn " " $ substituteNoiseCharacters $
(replace "." " . "
(replace "," " , "
(replace ";" " ; " s)))
stopWords = ["a", "the", "that", "of", "an"]
toLower' :: [Char] -> [Char]
toLower' s = map (\x -> if isLower x then x else (C.toLower x)) s
removeStopWords :: String -> [Char]
removeStopWords s =
intercalate
filter
(\x -> notElem (toLower' x) stopWords) $
words s
Text Processing
main = do
let ct = cleanText "The[]@] cat, and all dog, escaped&^. They were caught."
print ct
let nn = removeStopWords ct
print nn
This example should be extended with additional noise characters and stop words, depending on
your application. The function cleanText simply uses substring replacements.
Let’s look more closely at removeStopWords that takes a single argument s, which is expected to
be a string. removeStopWords uses a combination of several functions to remove stop words from
the input string. The function words is used to split the input string s into a list of words. Then, the
function filter is used to remove any words that match a specific condition. Here the condition is
defined as a lambda function, which is passed as the first argument to the filter function. The lambda
function takes a single argument x and returns a Boolean value indicating whether the word should
be included in the output or not. The lambda function uses function notElem to check whether the
lowercased version of the word x is present in a predefined list of stop words. Finally, we use the
function intercalate to join the remaining words back into a single string. The first argument to
function ** intercalate** is the separator that should be used to join the words, in this case, it’s a
single space.
Here is the output from this example:
*TestCleanText> :l CleanText.hs
[1 of 1] Compiling TestCleanText
( CleanText.hs, interpreted )
Ok, modules loaded: TestCleanText.
*TestCleanText> main
"The cat and all dog escaped . They were caught ."
"cat dog escaped . They were caught ."
We will continue working with text in the next chapter.
Natural Language Processing Tools
The tools developed in this chapter are modules you can reuse in your programs. We will develop
a command line program that reads a line of text from STDIN and writes sematic information as
output to STDOUT. I have used this in a Ruby program by piping input text data to a forked process
and reading the output which is a semantic representation of the input text.
We will be using this example as an external dependency to a later example in the chapter
Knowledge Graph Creator.
A few of the data files I provide in this example are fairly large. As an example the file PeopleDb-
Pedia.hs which builds a map from people’s names to the Wikipedia/DBPedia URI for information
about them, is 2.5 megabytes in size. The first time you run stack build in the project directory it
will take a while, so you might want to start building the project in the directory NlpTool and let it
run while you read this chapter.
Here are three examples using the NlpTool command line application developed in this chapter:
Enter text (all on one line)
Canada and England signed a trade deal.
category:
economics
summary:
Canada and England signed a trade deal.
countries:
[["Canada","<http://dbpedia.org/resource/Canada>"],
["England","<http://dbpedia.org/resource/England>"]]
Enter text (all on one line)
President George W Bush asked Congress for permission to invade Iraq.
category:
news_war
summary:
President George W Bush asked Congress for permission to invade Iraq.
people:
[["George W Bush","<http://dbpedia.org/resource/George_W._Bush>"]]
countries:
[["Iraq",""]]
Enter text (all on one line)
The British government is facing criticism from business groups over statements sugg\
esting the U.K. is heading for a hard divorce from the European Union â€” and pressu\
re from lawmakers who want Parliament to have a vote on the proposed exit terms. The\
government's repeated emphasis on controlling immigration sent out "signs that the \
door is being closed, to an extent, on the open economy, that has helped fuel invest\
ment," the head of employers' group the Confederation of British Industry, Carolyn F\
airbairn, said in comments published Monday. Prime Minister Theresa May said last we\
ek that Britain would seek to retain a close relationship with the 28-nation bloc, w\
ith continued free trade in goods and services. But she said the U.K. wouldn't cede \
control over immigration, a conflict with the EU's principle of free movement among \
Natural Language Processing Tools
member states.
category:
economics
summary:
Prime Minister Theresa May said last week that Britain would seek to retain\
a close relationship with the 28-nation bloc, with continued free trade in goods an\
d services.
credit: news text from abcnews.com
Resolve Entities in Text to DBPedia URIs
The code for this application is in the directory NlpTool.
The software and data in this chapter can be used under the terms of either the GPL version 3 license
or the Apache 2 license.
There are several automatically generated Haskell formatted data files that I created using Ruby
scripts operating the Wikipedia data. For the purposes of this book I include these data-specific files
for your use and enjoyment but we won’t spend much time discussing them. These files are:
• BroadcastNetworkNamesDbPedia.hs
• CityNamesDbpedia.hs
• CompanyNamesDbpedia.hs
• CountryNamesDbpedia.hs
• PeopleDbPedia.hs
• PoliticalPartyNamesDbPedia.hs
• TradeUnionNamesDbPedia.hs
• UniversityNamesDbPedia.hs
As an example, let’s look at a small sample of data in PeopleDbPedia.hs:
module PeopleDbPedia (peopleMap) where
import qualified Data.Map as M
peopleMap = M.fromList [
("Aaron Sorkin", "<http://dbpedia.org/resource/Aaron_Sorkin>"),
("Bill Clinton", "<http://dbpedia.org/resource/Bill_Clinton>"),
("George W Bush", "<http://dbpedia.org/resource/George_W_Bush>"),
There are 35,146 names in the file PeopleDbPedia.hs. I have built for eight different types of entity
names: Haskell maps that take entity names (String) and maps the entity names into relevant
Natural Language Processing Tools
DBPedia URIs. Simple in principle, but a lot of work preparing the data. As I mentioned, we will use
these data-specific files to resolve entity references in text.
The next listing shows the file Entities.hs. In lines 5-7 I import the entity mapping files I just described.
In this example and later code I make heavy use of the Data.Map and Data.Set modules in the
collections library (see the NlpTools.cabal file).
The operator isSubsetOf defined in line 39 tests to see if a value is contained in a collection. The
built-in function all applies a function or operator to all elements in a collection and returns a true
value if the function or operator returns true applied to each element in the collection.
The local utility function namesHelper defined in lines 41-53 is simpler than it looks. The function
filter in line 42 applies the inline function in lines 43-45 (this function returns true for Maybe values
that contain data) to a second list defined in lines 48-55. This second list is calculated by mapping
an inline function over the input argument ngrams. The inline function looks up an ngram in a
DBPedia map (passed as the second function argument) and returns the lookup value if it is not
empty and if it is empty looks up the same ngram in a word map (last argument to this function).
The utility function namesHelper is then used to define functions to recognize company names,
country names, people names, city names, broadcast network names, political party names, trade
union names, and university names:
-- Copyright 2014 by Mark Watson. All rights reserved. The software and data in this\
project can be used under the terms of either the GPL version 3 license or the Apac\
he 2 license.
module Entities (companyNames, peopleNames,
countryNames, cityNames, broadcastNetworkNames,
politicalPartyNames, tradeUnionNames, universityNames) where
import qualified Data.Map as M
import qualified Data.Set as S
import Data.Char (toLower)
import Data.List (sort, intersect, intersperse)
import Data.Set (empty)
import Data.Maybe (isJust)
import Utils (splitWords, bigram, bigram_s, splitWordsKeepCase,
trigram, trigram_s, removeDuplicates)
import FirstNames (firstNames)
import LastNames (lastNames)
import NamePrefixes (namePrefixes)
import PeopleDbPedia (peopleMap)
Natural Language Processing Tools
import CountryNamesDbpedia (countryMap)
import CountryNames (countryNamesOneWord, countryNamesTwoWords, countryNamesThreeWor\
import CompanyNamesDbpedia (companyMap)
import CompanyNames (companyNamesOneWord, companyNamesTwoWords, companyNamesThreeWor\
import CityNamesDbpedia (cityMap)
import BroadcastNetworkNamesDbPedia (broadcastNetworkMap)
import PoliticalPartyNamesDbPedia (politicalPartyMap)
import TradeUnionNamesDbPedia (tradeUnionMap)
import UniversityNamesDbPedia (universityMap)
xs `isSubsetOf` ys = all (`elem` ys) xs
namesHelper ngrams dbPediaMap wordMap =
filter
(\x -> case x of
(_, Just x) -> True
_ -> False) $
map (\ngram -> (ngram,
let v = M.lookup ngram dbPediaMap in
if isJust v
then return (ngram, v)
else if S.member ngram wordMap
then Just (ngram, Just "")
else Nothing))
ngrams
helperNames1W = namesHelper
helperNames2W wrds = namesHelper (bigram_s wrds)
helperNames3W wrds =
namesHelper (trigram_s wrds)
companyNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds companyMap companyNamesOneWord ++
helperNames2W wrds companyMap companyNamesTwoWords ++
helperNames3W wrds companyMap companyNamesThreeWords in
map (\(s, Just (a,Just b)) -> (a,b)) cns
Natural Language Processing Tools
countryNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds countryMap countryNamesOneWord ++
helperNames2W wrds countryMap countryNamesTwoWords ++
helperNames3W wrds countryMap countryNamesThreeWords in
map (\(s, Just (a,Just b)) -> (a,b)) cns
peopleNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds peopleMap Data.Set.empty ++
helperNames2W wrds peopleMap Data.Set.empty ++
helperNames3W wrds peopleMap Data.Set.empty in
map (\(s, Just (a,Just b)) -> (a,b)) cns
cityNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds cityMap Data.Set.empty ++
helperNames2W wrds cityMap Data.Set.empty ++
helperNames3W wrds cityMap Data.Set.empty in
map (\(s, Just (a,Just b)) -> (a,b)) cns
broadcastNetworkNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds broadcastNetworkMap Data.Set.empty ++
helperNames2W wrds broadcastNetworkMap Data.Set.empty ++
helperNames3W wrds broadcastNetworkMap Data.Set.empty in
map (\(s, Just (a,Just b)) -> (a,b)) cns
politicalPartyNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds politicalPartyMap Data.Set.empty ++
helperNames2W wrds politicalPartyMap Data.Set.empty ++
helperNames3W wrds politicalPartyMap Data.Set.empty in
map (\(s, Just (a,Just b)) -> (a,b)) cns
tradeUnionNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds tradeUnionMap Data.Set.empty ++
helperNames2W wrds tradeUnionMap Data.Set.empty ++
helperNames3W wrds tradeUnionMap Data.Set.empty in
map (\(s, Just (a,Just b)) -> (a,b)) cns
Natural Language Processing Tools
universityNames wrds =
let cns = removeDuplicates $ sort $
helperNames1W wrds universityMap Data.Set.empty ++
helperNames2W wrds universityMap Data.Set.empty ++
helperNames3W wrds universityMap Data.Set.empty in
map (\(s, Just (a,Just b)) -> (a,b)) cns
main = do
let s = "As read in the San Francisco Chronicle, the company is owned by John Sm\
ith, Bill Clinton, Betty Sanders, and Dr. Ben Jones. Ben Jones and Mr. John Smith ar\
e childhood friends who grew up in Brazil, Canada, Buenos Aires, and the British Vir\
gin Islands. Apple Computer relased a new version of OS X yesterday. Brazil Brazil B\
razil. John Smith bought stock in ConocoPhillips, Heinz, Hasbro, and General Motors,\
Fox Sports Radio. I listen to B J Cole. Awami National Party is a political party. \
ALAEA is a trade union. She went to Brandeis University."
--print $ humanNames s
print $ peopleNames $ splitWordsKeepCase s
print $ countryNames $ splitWordsKeepCase s
print $ companyNames $ splitWordsKeepCase s
print $ cityNames $ splitWordsKeepCase s
print $ broadcastNetworkNames $ splitWordsKeepCase s
print $ politicalPartyNames $ splitWordsKeepCase s
print $ tradeUnionNames $ splitWordsKeepCase s
print $ universityNames $ splitWordsKeepCase s
The following output is generated by running the test main function defined at the bottom of the
file app/NlpTool.hs:
$ stack build --fast --exec NlpTool-exe
Building all executables for `NlpTool' once. After a successful build of all of them\
, only specified executables will be rebuilt.
NlpTool> build (lib + exe)
Preprocessing library for NlpTool-0.1.0.0..
Building library for NlpTool-0.1.0.0..
Preprocessing executable 'NlpTool-exe' for NlpTool-0.1.0.0..
Building executable 'NlpTool-exe' for NlpTool-0.1.0.0..
[1 of 2] Compiling Main
[2 of 2] Compiling Paths_NlpTool
Linking .stack-work/dist/x86_64-osx/Cabal-2.4.0.1/build/NlpTool-exe/NlpTool-exe ...
NlpTool> copy/register
Installing library in /Users/markw/GITHUB/haskell_tutorial_cookbook_examples_private\
_new_edition/NlpTool/.stack-work/install/x86_64-osx/7a2928fbf8188dcb20f165f77b37045a\
Natural Language Processing Tools
5c413cc7f63913951296700a6b7e292d/8.6.5/lib/x86_64-osx-ghc-8.6.5/NlpTool-0.1.0.0-DXKb\
ucyA0S0AKOAcZGDl2H
Installing executable NlpTool-exe in /Users/markw/GITHUB/haskell_tutorial_cookbook_e\
xamples_private_new_edition/NlpTool/.stack-work/install/x86_64-osx/7a2928fbf8188dcb2\
0f165f77b37045a5c413cc7f63913951296700a6b7e292d/8.6.5/bin
Registering library for NlpTool-0.1.0.0..
Enter text (all on one line)
As read in the San Francisco Chronicle, the company is owned by John Smith, Bill Cli\
nton, Betty Sanders, and Dr. Ben Jones. Ben Jones and Mr. John Smith are childhood f\
riends who grew up in Brazil, Canada, Buenos Aires, and the British Virgin Islands. \
Apple Computer relased a new version of OS X yesterday. Brazil Brazil Brazil. John S\
mith bought stock in ConocoPhillips, Heinz, Hasbro, and General Motors, Fox Sports R\
adio. I listen to B J Cole. Awami National Party is a political party. ALAEA is a tr\
ade union. She went to Brandeis University.
category:
news_politics
summary:
ALAEA is a trade union. Apple Computer relased a new version of OS X yester\
people:
[["B J Cole","<http://dbpedia.org/resource/B._J._Cole>"]]
companies:
[["Apple","<http://dbpedia.org/resource/Apple>"],["ConocoPhillips","<http\
://dbpedia.org/resource/ConocoPhillips>"],["Hasbro","<http://dbpedia.org/resource/Ha\
sbro>"],["Heinz","<http://dbpedia.org/resource/Heinz>"],["San Francisco Chronicle","\
<http://dbpedia.org/resource/San_Francisco_Chronicle>"]]
countries:
[["Brazil","<http://dbpedia.org/resource/Brazil>"],["Canada","<http://dbp\
edia.org/resource/Canada>"]]
Enter text (all on one line)
Note that entities that are not recognized as Wikipedia objects don’t get recognized.
Bag of Words Classification Model
The file Categorize.hs contains a simple bag of words classification model. To prepare the classi-
fication models, I collected a large set of labelled text. Labels were “chemistry”, “computers”, etc.
I ranked words based on how often they appeared in training texts for a classification category,
normalized by how often they appeared in all training texts. This example uses two auto-generated
and data-specific Haskell files, one for single words and the other for two adjacent word pairs:
• Category1Gram.hs
• Category2Gram.hs
In NLP work, single words are sometimes called 1grams and two word adjacent pairs are referred
to as 2grams. Here is a small amount of data from Category1Gram.hs:
Natural Language Processing Tools
module Category1Gram (**onegrams**) where
import qualified Data.Map as M
chemistry = M.fromList [("chemical", 1.15), ("atoms", 6.95),
("reaction", 6.7), ("energy", 6.05),
computers = M.fromList [("software", 4.6), ("network", 4.65),
("linux", 3.6), ("device", 3.55), ("computers", 3.05),
("storage", 2.7), ("disk", 2.3),
Here is a small amount of data from Category2Gram.hs:
module Category2Gram (**twograms**) where
import qualified Data.Map as M
chemistry = M.fromList [("chemical reaction", 1.55),
("atoms molecules", 0.6),
("periodic table", 0.5),
("chemical reactions", 0.5),
("carbon atom", 0.5),
computers = M.fromList [("computer system", 0.9),
("operating system", 0.75),
("random memory", 0.65),
("computer science", 0.65),
("computer program", 0.6),
It is very common to use term frequencies for single words for classification models. One problem
with using single words is that the evidence that any word gives for a classification is independent of
the surrounding words in text being evaluated. By also using word pairs (two word combinations are
often called 2grams or two-grams) we pick up patterns like “not good” giving evidence for negative
sentiment even with the word “good” in text being evaluated. For my own work, I have a huge
corpus of 1gram, 2gram, 3gram, and 4gram data sets. For the purposes of the following example
program, I am only using 1gram and 2gram data.
The following listing shows the file Categorize.hs. Before looking at the entire example, let’s focus
in on some of the functions I have defined for using the word frequency data to categorized text.
Natural Language Processing Tools
*Categorize> :t stemWordsInString
stemWordsInString :: String -> [Char]
*Categorize> stemWordsInString "Banking industry is sometimes known for fraud."
"bank industri is sometim known for fraud"
stemScoredWordList is used to create a 1gram to word relevance score for each category. The keys
are word stems.
*Categorize> stemScoredWordList onegrams
[("chemistri",fromList [("acid",1.15),("acids",0.8),("alcohol",0.95),("atom",4.45)
Notice that “chemistri” is the stemmed version of “chemistry”, “bank” for “banks”, etc. stem2 is a
2gram frequency score by category mapping where the keys are word stems:
*Categorize> stem2
[("chemistry",fromList [("atom molecul",0.6),("carbon atom",0.5),("carbon carbon",0.\
stem1 is like stem2, but for stemmed 1grams, not 2grams:
*Categorize> stem1
[("chemistry",fromList [("acid",0.8),("chang",1.05),("charg",0.95),("chemic",1.15),(\
"chemistri",1.45),
score is called with a list or words and a word value mapping. Here is an example:
*Categorize> :t score
:: (Enum t, Fractional a, Num t, Ord a, Ord k) =>
[k] -> [(t1, M.Map k a)] -> [(t, a)]
*Categorize> score ["atom", "molecule"] onegrams
[(0,8.2),(25,2.4)]
This output is more than a little opaque. The pair (0, 8.2) means that the input words [“atom”,
“molecule”] have a score of 8.2 for category indexed at 0 and the pair (25,2.4) means that the input
words have a score of 2.4 for the category at index 25. The category at index 0 is chemistry and the
category at index 25 is physics as we can see by using the higher level function bestCategories1
that caluculates categories for a word sequence using 1gram word data:
Natural Language Processing Tools
*Categorize> :t bestCategories1
bestCategories1 :: [[Char]] -> [([Char], Double)]
*Categorize> bestCategories1 ["atom", "molecule"]
[("chemistry",8.2),("physics",2.4)]
The top level function bestCategories uses 1gram data. Here is an example for using it:
*Categorize> splitWords "The chemist made a periodic table and explained a chemical \
reaction"
["the","chemist","made","a","periodic","table","and","explained","a","chemical","rea\
ction"]
*Categorize> bestCategories1 $ splitWords "The chemist made a periodic table and exp\
lained a chemical reaction"
[("chemistry",11.25),("health_nutrition",1.2)]
Notice that these words were also classified as category “health_nutrition” but with a low score of
1.2. The score for “chemistry” is almost an order of magnitude larger. bestCategories sorts return
values in “best first” order.
splitWords is used to split a string into word tokens before calling bestCategories.
Here is the entire example in file Categorize.hs:
module Categorize (bestCategories, splitWords, bigram) where
import qualified Data.Map as M
import Data.List (sortBy)
import Category1Gram (onegrams)
import Category2Gram (twograms)
import Sentence (segment)
import Stemmer (stem)
import Utils (splitWords, bigram, bigram_s)
catnames1 = map fst onegrams
catnames2 = map fst twograms
stemWordsInString s = init $ concatMap ((++ " ") . stem) (splitWords s)
stemScoredWordList = map (\(str,score) -> (stemWordsInString str, score))
Natural Language Processing Tools
stem2 = map (\(category, swl) ->
(category, M.fromList (stemScoredWordList (M.toList swl))))
twograms
stem1 = map (\(category, swl) ->
(category, M.fromList (stemScoredWordList (M.toList swl))))
onegrams
scoreCat wrds amap =
sum $ map (\x ->
M.findWithDefault 0.0 x amap) wrds
score wrds amap =
filter (\(a, b) -> b > 0.9) $ zip [0..] $ map (\(s, m) -> scoreCat wrds m) amap
cmpScore (a1, b1) (a2, b2) = compare b2 b1
bestCategoriesHelper wrds ngramMap categoryNames=
let tg = bigram_s wrds in
map (first (categoryNames !!)) $ sortBy cmpScore $ score wrds ngramMap
bestCategories1 wrds =
take 3 $ bestCategoriesHelper wrds onegrams catnames1
bestCategories2 wrds =
take 3 $ bestCategoriesHelper (bigram_s wrds) twograms catnames2
bestCategories1stem wrds =
take 3 $ bestCategoriesHelper wrds stem1 catnames1
bestCategories2stem wrds =
take 3 $ bestCategoriesHelper (bigram_s wrds) stem2 catnames2
bestCategories :: [String] -> [(String, Double)]
bestCategories wrds =
let sum1 = M.unionWith (+) (M.fromList $ bestCategories1 wrds) ( M.fromList $ best\
Categories2 wrds)
sum2 = M.unionWith (+) (M.fromList $ bestCategories1stem wrds) ( M.fromList $ \
bestCategories2stem wrds)
in sortBy cmpScore $ M.toList $ M.unionWith (+) sum1 sum2
main = do
let s = "The sport of hocky is about 100 years old by ahdi dates. American Footb\
all is a newer sport. Programming is fun. Congress passed a new budget that might he\
Natural Language Processing Tools
lp the economy. The frontier initially was a value path. The ai research of john mcc\
arthy."
print $ bestCategories1 (splitWords s)
print $ bestCategories1stem (splitWords s)
print $ score (splitWords s) onegrams
print $ score (bigram_s (splitWords s)) twograms
print $ bestCategories2 (splitWords s)
print $ bestCategories2stem (splitWords s)
print $ bestCategories (splitWords s)
Here is the output:
$ stack ghci
:l Categorize.hs
*Categorize> main
[("computers_ai",17.900000000000002),("sports",9.75),("computers_ai_search",6.2)]
[("computers_ai",18.700000000000003),("computers_ai_search",8.1),("computers_ai_lear\
ning",5.7)]
[(2,17.900000000000002),(3,1.75),(4,5.05),(6,6.2),(9,1.1),(10,1.2),(21,2.7),(26,1.1)\
,(28,1.6),(32,9.75)]
[(2,2.55),(6,1.0),(32,2.2)]
[("computers_ai",2.55),("sports",2.2),("computers_ai_search",1.0)]
[("computers_ai",1.6)]
[("computers_ai",40.75000000000001),("computers_ai_search",15.3),("sports",11.95),("\
computers_ai_learning",5.7)]
Given that the variable s contains some test text, line 4 of this output was generated by evaluating
bestCategories1 (splitWords s), lines 5-6 by evaluating bestCategories1stem (splitWords s), lines
7-8 from score (splitWords s) onegrams, line 9 from core (bigram_s (splitWords s)) twograms,
line 10 from bestCategories2 (splitWords s), line 11 from bestCategories2stem (splitWords s),
and lines 12-13 from bestCategories (splitWords s).
I called all of the utility fucntions in function main to demonstrate what they do but in practice I
just call function bestCategories in my applications.
Text Summarization
This application uses both the Categorize.hs code and the 1gram data from the last section. The
algorithm I devised for this example is based on a simple idea: we categorize text and keep track of
which words provide the strongest evidence for the highest ranked categories. We then return a few
sentences from the original text that contain the largest numbers of these important words.
Natural Language Processing Tools
module Summarize (summarize, summarizeS) where
import qualified Data.Map as M
import Data.List.Utils (replace)
import Data.Maybe (fromMaybe)
import Categorize (bestCategories)
import Sentence (segment)
import Utils (splitWords, bigram_s, cleanText)
import Category1Gram (onegrams)
import Category2Gram (twograms)
scoreSentenceHelper words scoreMap = -- just use 1grams for now
sum $ map (\word ->
M.findWithDefault 0.0 word scoreMap) words
safeLookup key alist =
fromMaybe 0 $ lookup key alist
scoreSentenceByBestCategories words catDataMaps bestCategories =
map (\(category, aMap) ->
(category, safeLookup category bestCategories *
scoreSentenceHelper words aMap)) catDataMaps
scoreForSentence words catDataMaps bestCategories =
sum $ map snd $ scoreSentenceByBestCategories words catDataMaps bestCategories
summarize s =
let words = splitWords $ cleanText s
bestCats = bestCategories words
sentences = segment s
result1grams = map (\sentence ->
(sentence,
scoreForSentence (splitWords sentence)
onegrams bestCats))
sentences
result2grams = map (\sentence ->
(sentence,
scoreForSentence (bigram_s (splitWords sentence))
twograms bestCats))
sentences
mergedResults = M.toList $ M.unionWith (+)
(M.fromList result1grams) (M.fromList result1grams)
Natural Language Processing Tools
c400 = filter (\(sentence, score) -> score > 400) mergedResults
c300 = filter (\(sentence, score) -> score > 300) mergedResults
c200 = filter (\(sentence, score) -> score > 200) mergedResults
c100 = filter (\(sentence, score) -> score > 100) mergedResults
c000 = mergedResults in
if not (null c400) then c400 else if not (null c300) then c300 else if not (null c\
200) then c200 else if not (null c100) then c100 else c000
summarizeS s =
let a = replace "\"" "'" $ concatMap (\x -> fst x ++ " ") $ summarize s in
if not (null a) then a else safeFirst $ segment s where
safeFirst x
| length x > 1 = head x ++ x !! 1
| not (null x)
= head x
| otherwise
main = do
let s = "Plunging European stocks, wobbly bonds and grave concerns about the healt\
h of Portuguese lender Banco Espirito Santo SA made last week feel like a rerun of t\
he euro crisis, but most investors say it was no more than a blip for a resurgent re\
gion. Banco Espirito Santo has been in investorsâ€™ sights since December, when The \
Wall Street Journal first reported on accounting irregularities at the complex firm.\
Nerves frayed on Thursday when Banco Espirito Santo's parent company said it wouldn\
't be able to meet some short-term debt obligations."
print $ summarize s
print $ summarizeS s
Lazy evaluation allows us in function summarize to define summaries of various numbers of
sentences, but not all of these possible summaries are calculated.
$ stack ghci
*Main ... > :l Summarize.hs
*Summarize> main
[("Nerves frayed on Thursday when Banco Espirito Santo's parent company said it woul\
dn't be able to meet some short-term debt obligations.",193.54500000000002)]
"Nerves frayed on Thursday when Banco Espirito Santo's parent company said it wouldn\
't be able to meet some short-term debt obligations. "
Part of Speech Tagging
We close out this chapter with the Haskell version of my part of speech (POS) tagger that I originally
wrote in Common Lisp, then converted to Ruby and Java. The file LexiconData.hs is similar to the
Natural Language Processing Tools
lexical data files seen earlier: I am defining a map where keys a words and map values are POS
tokens like NNP (proper noun), RB (adverb), etc. The file README.md contains a complete list of
POS tag definitions.
The example code and data for this section is in the directory FastTag.
This listing shows a tiny representative part of the POS definitions in LexiconData.hs:
lexicon = M.fromList [("AARP", "NNP"), ("Clinic", "NNP"), ("Closed", "VBN"),
("Robert", "NNP"), ("West-German", "JJ"),
("afterwards", "RB"), ("arises", "VBZ"),
("attacked", "VBN"), ...]
Before looking at the code example listing, let’s see how the functions defined in fasttag.hs work in
a GHCi repl:
*Main LexiconData> bigram ["the", "dog", "ran",
"around", "the", "tree"]
[["the","dog"],["dog","ran"],["ran","around"],
["around","the"],["the","tree"]]
*Main LexiconData> tagHelper "car"
["car","NN"]
*Main LexiconData> tagHelper "run"
["run","VB"]
*Main LexiconData> substitute ["the", "dog", "ran", "around",
"the", "tree"]
[[["the","DT"],["dog","NN"]],[["dog","NN"],["ran","VBD"]],
[["ran","VBD"],["around","IN"]],[["around","IN"],["the","DT"]],
[["the","DT"],["tree","NN"]]]
*Main LexiconData> fixTags $
substitute ["the", "dog", "ran",
"around", "the", "tree"]
["NN","VBD","IN","DT","NN"]
Function bigram takes a list or words and returns a list of word pairs. We need the word pairs
because parts of the tagging algorithm needs to see a word with its preceeding word. In an imperative
language, I would loop over the words and for a word at index i I would have the word at index i -
1. In a functional language, we avoid using loops and in this case create a list of adjacent word pairs
to avoid having to use an explicit loop. I like this style of functional programming but if you come
from years of using imperative language like Java and C++ it takes some getting used to.
tagHelper converts a word into a list of the word and its likely tag. substitute applies tagHelper to
a list of words, getting the most probable tag for each word. The function fixTags will occasionally
override the default word tags based on a few rules that are derived from Eric Brill’s paper A Simple
Rule-Based Part of Speech Tagger²⁴.
²⁴http://aclweb.org/anthology/A92-1021
Natural Language Processing Tools
Here is the entire example:
module Main where
import qualified Data.Map as M
import Data.Strings (strEndsWith, strStartsWith)
import Data.List (isInfixOf)
import LexiconData (lexicon)
bigram :: [a] -> [[a]]
bigram [] = []
bigram [_] = []
bigram xs = take 2 xs : bigram (tail xs)
containsString word substring = isInfixOf substring word
fixTags twogramList =
-- in the following inner function, [last,current] might be bound,
-- for example, to [["dog","NN"],["ran","VBD"]]
(\[last, current] ->
-- rule 1: DT, {VBD | VBP} --> DT, NN
if last !! 1 == "DT" && (current !! 1 == "VBD" ||
current !! 1 == "VB" ||
current !! 1 == "VBP")
then "NN"
-- rule 2: convert a noun to a number (CD) if "." appears in the word
if (current !! 1) !! 0 == 'N' && containsString (current !! 0) "."
then "CD"
-- rule 3: convert a noun to a past participle if
words.get(i) ends with "ed"
if (current !! 1) !! 0 == 'N' && strEndsWith (current !! 0) "ed"
then "VBN"
-- rule 4: convert any type to adverb if it ends in "ly"
if strEndsWith (current !! 0) "ly"
then "RB"
-- rule 5: convert a common noun (NN or NNS) to an
adjective if it ends with "al"
Natural Language Processing Tools
if strStartsWith (current !! 1) "NN" &&
strEndsWith (current !! 1) "al"
then "JJ"
-- rule 6: convert a noun to a verb if the preceeding
word is "would"
if strStartsWith (current !! 1) "NN" &&
(last !! 0) == "would" -- should be case insensitive
then "VB"
-- rule 7: if a word has been categorized as a
common noun and it ends with "s",
then set its type to plural common noun (NNS)
if strStartsWith (current !! 1) "NN" &&
strEndsWith (current !! 0) "s"
then "NNS"
-- rule 8: convert a common noun to a present
participle verb (i.e., a gerand)
if strStartsWith (current !! 1) "NN" &&
strEndsWith (current !! 0) "ing"
then "VBG"
else (current !! 1))
twogramList
substitute tks = bigram $ map tagHelper tks
tagHelper token =
let tags = M.findWithDefault [] token lexicon in
if tags == [] then [token, "NN"] else [token, tags]
tag tokens = fixTags $ substitute ([""] ++ tokens)
main = do
let tokens = ["the", "dog", "ran", "around", "the", "tree", "while",
"the", "cat", "snaked", "around", "the", "trunk",
"while", "banking", "to", "the", "left"]
print $ tag tokens
print $ zip tokens $ tag tokens
Natural Language Processing Tools
*Main LexiconData> main
["DT","NN","VBD","IN","DT","NN","IN","DT","NN","VBD","IN","DT",
"NN","IN","VBG","TO","DT","VBN"]
[("the","DT"),("dog","NN"),("ran","VBD"),("around","IN"),
("the","DT"),("tree","NN"),("while","IN"),("the","DT"),
("cat","NN"),("snaked","VBD"),("around","IN"),("the","DT"),
("trunk","NN"),("while","IN"),("banking","VBG"),("to","TO"),
("the","DT"),("left","VBN")]
The README.md file contains definitions of the POS definitions. Here are the ones used in this
example:
DT Determiner
the,some
NN noun
dog,cat,road
VBD verb, past tense
ate,ran
IN Preposition
of,in,by
Natural Language Processing Wrap Up
NLP is a large topic. I have attempted to show you just the few tricks that I use often and are simple
to implement. I hope that you reuse the code in this chapter in your own projects when you need to
detect entities, classify text, summarize text, and assign part of speech tags to words in text.
Linked Data and the Semantic Web
I am going to show you how to query semantic web data sources on the web and provide examples
for how you might use this data in applications. I have written two previous books on the semantic
web, one covering Common Lisp and the other covering JVM languages Java, Scala, Clojure, and
Ruby. You can get free PDF versions on the book page of www.markwatson.com²⁵. If you enjoy the
light introduction in this chapter then please do download a free copy of my semantic web book for
more material on RDF, RDFS, and SPARQL.
I like to think of the semantic web and linked data resources as:
• A source of structured data on the web. These resources are called SPARQL endpoints.
• Data is represented by data triples: subject, predicate, and object. The subject of one triple can
be the object of another triple. Predicates are relationships; a few examples: “owns”, “is part
of”, “author of”, etc.
• Data that is accessed via the SPARQL query language.
• A source of data that may or may not be available. SPARQL endpoints are typically available
for free use and they are sometimes unavailable. Although not covered here, I sometimes work
around this problem by adding a caching layer to SPARQL queries (access key being a SPARQL
query string, the value being the query results). This caching speeds up development and
running unit tests, and sometimes saves a customer demo when a required SPARQL endpoint
goes offline at an inconvenient time.
DBPedia is the semantic web version of Wikipedia²⁶. The many millions of data triples that make
up DBPedia are mostly derived from the structured “info boxes” on Wikipedia pages.
As you are learning SPARQL use the DBPedia SPARQL endpoint²⁷ to practice. As a practitioner who
uses linked data, for any new project I start by identifying SPARQL endpoints for possibly useful
data. I then interactively experiment with SPARQL queries to extract the data I need. Only when
I am satisfied with the choice of SPARQL endpoints and SPARQL queries do I write any code to
automatically fetch linked data for my application.
Pro tip: I mentioned SPARQL query caching. I sometimes cache query results in a local database,
saving the returned RDF data indexed by the SPARQL query. You can also store the cache timestamp
and refresh the cache every few weeks as needed. In addition to making development and unit testing
faster, your applications will be more resilient.
In the last chapter “Natural Language Processing Tools” we resolved entities in natural language text
to DBPedia (semantic web SPAQL endpoint for Wikipedia) URIs. Here we will use some of these
URIs to demonstrate fetching real world knowledge that you might want to use in applications.
²⁵http://www.markwatson.com/books/
²⁶http://wiki.dbpedia.org/
²⁷http://dbpedia.org/sparql
Linked Data and the Semantic Web
The SPARQL Query Language
Example RDF N3 triples (subject, predicate, object) might look like:
<http://www.markwatson.com>
<http://dbpedia.org/ontology/owner>
"Mark Watson" .
Element of triples can be URIs or string constants. Triples are often written all on one line; I split it
to three lines to fit the page width. Here the subject is the URI for my web site, the predicate is a
URI defining an ownership relationship, and the object is a string literal.
If you want to see details for any property or other URI you see, then “follow your nose” and open the
URI in a web browser. For example remove the brackets from the owner property URI http://dbpedia.
org/ontology/owner²⁸ and open it in a web browser. For working with RDF data programatically, it
is convenient using full URI. For humans reading RDF, the N3 notation is better because it supports
defining URI standard prefixes for use as abbreviations; for example:
prefix ontology: <http://dbpedia.org/ontology/>
<http://www.markwatson.com>
ontology:owner
"Mark Watson" .
If you wanted to find all things that I own (assuming this data was in a public RDF repository, which
it isn’t) then we might think to match the pattern:
prefix ontology: <http://dbpedia.org/ontology/>
?subject ontology:owner "Mark Watson"
And return all URIs matching the variable ?subject as the query result. This is the basic idea of
making SPARQL queries.
The following SPARQL query will be implemented later in Haskell using the HSparql library:
²⁸http://dbpedia.org/ontology/owner
Linked Data and the Semantic Web
prefix resource: <http://dbpedia.org/resource/>
prefix dbpprop: <http://dbpedia.org/property/>
prefix foaf: <http://xmlns.com/foaf/0.1/>
SELECT *
WHERE {
?s dbpprop:genre resource:Web_browser .
?s foaf:name ?name .
} LIMIT 5
In this last SPARQL query example, the triple patterns we are trying to match are inside a WHERE
clause. Notice that in the two triple patterns, the subject field of each is the variable ?s. The first
pattern matches all DBPedia triples with a predicate http://dbpedia.org/property/genre and an object
equal to http://dbpedia.org/resource/Web_browser. We then find all triples with the same subject
but with a predicate equal to http://xmlns.com/foaf/0.1/name.
Each result from this query will contain two values for variables ?s and ?name: a DBPedia URI for
some thing and the name for that thing. Later we will run this query using Haskell code and you
can see what the output might look like.
Sometimes when I am using a specific SPARQL query in an application, I don’t bother defining
prefixes and just use URIs in the query. As an example, suppose I want to return the Wikipedia (or
DBPedia) abstract for IBM. I might use a query such as:
select * where {
<http://dbpedia.org/resource/IBM>
<http://dbpedia.org/ontology/abstract>
FILTER langMatches(lang(?o), "EN")
} LIMIT 100
If you try this query using the web interface for DBPedia SPARQL queries²⁹ you get just one result
because of the FILTER option that only returns English language results. You could also use FR for
French results, GE for German results, etc.
A Haskell HTTP Based SPARQL Client
One approach to query the DBPedia SPARQL endpoint is to build a HTTP GET request, send it to
the SPARQL endpoint server, and parse the returned XML response. We will start with this simple
approach. You will recognize the SPARQL query from the last section:
²⁹http://dbpedia.org/sparql/
Linked Data and the Semantic Web
{-# LANGUAGE OverloadedStrings #-}
module HttpSparqlClient where
import Network.HTTP.Conduit (simpleHttp)
import Network.HTTP.Base (urlEncode)
import Text.XML.HXT.Core
import Text.HandsomeSoup
import qualified Data.ByteString.Lazy.Char8 as B
buildQuery :: String -> [Char]
buildQuery sparqlString =
"http://dbpedia.org/sparql/?query=" ++ urlEncode sparqlString
main :: IO ()
main = do
let query = buildQuery "select * where {<http://dbpedia.org/resource/IBM> <http://\
dbpedia.org/ontology/abstract> ?o . FILTER langMatches(lang(?o), \"EN\")} LIMIT 100"
res <- simpleHttp query
let doc = readString []
(B.unpack res)
putStrLn "\nAbstracts:\n"
abstracts <- runX $ doc >>> css "binding" >>>
(getAttrValue "name" &&& (deep getText))
print abstracts
The function buildQuery defined in lined 11-13 takes any SPARQL query, URL encodes it so it can
be passed as part of a URI, and builds a query string for the DBPedia SPARQL endpoint. The returned
data is in XML format. In lines 23-24 I am using the XHT parsing library to extract the names (values
bound to the variable ?o in the query in line 17). I covered the use of the HandsomeSoup parsing
library in the chapter Web Scraping.
We use runX to execute a series of operations on an XML document (the doc variable). We first
select all elements in doc that have the CSS class binding using the css function. Next we extract
the value of the name attribute from each selected element using getAttrValue and also extract
the text inside the element using the function deep. The &&& operator is used to combine the two
values for the name attribute and the element text into a tuple.
In the main function, we use the utility function simpleHttp in line 20 to fetch the results as a
ByteString and in line 21 we unpack this to a regular Haskell String.
Linked Data and the Semantic Web
Prelude> :l HttpSparqlClient.hs
[1 of 1] Compiling HttpSparqlClient ( HttpSparqlClient.hs, interpreted )
Ok, modules loaded: HttpSparqlClient.
*HttpSparqlClient> main
Abstracts:
[("o","International Business Machines Corporation (commonly referred to as IBM) is \
an American multinational technology and consulting corporation, with corporate head\
quarters in Armonk, New York.
Querying Remote SPARQL Endpoints
We will write some code in this section to make the example query to get the names of web browsers
from DBPedia. In the last section we made a SPARQL query using fairly low level Haskell libraries.
We will be using the high level library HSparql to build SPARQL queries and call the DBPedia
SPARQL endpoint.
The example in this section can be found in SparqlClient/TestSparqlClient.hs. In the main function
notice how I have commented out printouts of the raw query results. Because Haskell is type safe,
extracting the values wrapped in query results requires knowing RDF element return types. I will
explain this matching after the program listing:
-- simple experiments with the excellent HSparql library
module Main where
import Database.HSparql.Connection (BindingValue(Bound))
import Data.RDF hiding (triple)
import Database.HSparql.QueryGenerator
import Database.HSparql.Connection (selectQuery)
webBrowserSelect :: Query SelectQuery
webBrowserSelect = do
resource <- prefix "dbprop" (iriRef "http://dbpedia.org/resource/")
dbpprop
<- prefix "dbpedia" (iriRef "http://dbpedia.org/property/")
<- prefix "foaf" (iriRef "http://xmlns.com/foaf/0.1/")
<- var
name <- var
triple x (dbpprop .:. "genre") (resource .:. "Web_browser")
Linked Data and the Semantic Web
triple x (foaf .:. "name") name
return SelectQuery { queryVars = [name] }
companyAbstractSelect :: Query SelectQuery
companyAbstractSelect = do
resource <- prefix "dbprop" (iriRef "http://dbpedia.org/resource/")
ontology <- prefix "ontology" (iriRef "http://dbpedia.org/ontology/")
o <- var
triple (resource .:. "Edinburgh_University_Press") (ontology .:. "abstract") o
return SelectQuery { queryVars = [o] }
companyTypeSelect :: Query SelectQuery
companyTypeSelect = do
resource <- prefix "dbprop" (iriRef "http://dbpedia.org/resource/")
ontology <- prefix "ontology" (iriRef "http://dbpedia.org/ontology/")
o <- var
triple (resource .:. "Edinburgh_University_Press") (ontology .:. "type") o
return SelectQuery { queryVars = [o] }
main :: IO ()
main = do
sq1 <- selectQuery "http://dbpedia.org/sparql" companyAbstractSelect
--putStrLn "\nRaw results of company abstract SPARQL query:\n"
--print sq1
putStrLn "\nWeb browser names extracted from the company abstract query results:\n"
case sq1 of
Just a -> print $ map (\[Bound (LNode (PlainLL s _))] -> s) a
Nothing -> putStrLn "nothing"
sq2 <- selectQuery "http://dbpedia.org/sparql" companyTypeSelect
--putStrLn "\nRaw results of company type SPARQL query:\n"
--print sq2
putStrLn "\nWeb browser names extracted from the company type query results:\n"
case sq2 of
Just a -> print $ map (\[Bound (UNode
s)] -> s) a
Nothing -> putStrLn "nothing"
sq3 <- selectQuery "http://dbpedia.org/sparql" webBrowserSelect
--putStrLn "\nRaw results of SPARQL query:\n"
--print sq3
putStrLn "\nWeb browser names extracted from the query results:\n"
case sq3 of
Just a -> print $ map (\[Bound (LNode (PlainLL s _))] -> s) a
Nothing -> putStrLn "nothing"
Linked Data and the Semantic Web
Notes on matching result types of query results:
You will notice how I have commented out print statements in the last example. When trying new
queries you need to print out the results in order to know how to extract the wrapped query results.
Let’s look at a few examples:
If we print the value for sq1:
Raw results of company abstract SPARQL query:
Just [[Bound (LNode (PlainLL "Edinburgh University Press ...
we see that inside a Just we have a list of lists. Each inner list is a Bound wrapping types defined
in HSparql. We would unwrap sq1 using:
case sq1 of
Just a -> print $ map (\[Bound (LNode (PlainLL s _))] -> s) a
Nothing -> putStrLn "nothing"
In a similar way I printed out the values of sq2 and sq3 to see the form os case statement I would
need to unwrap them.
The output from this example with three queries to the DBPedia SPARQL endpoint is:
Web browser names extracted from the company abstract query results in sq1:
["Edinburgh University Press \195\168 una casa editrice scientifica di libri accadem\
ici e riviste, con sede a Edimburgo, in Scozia.","Edinburgh University Press \195\16\
9 uma editora universit\195\161ria com base em Edinburgh, Esc\195\179cia.","Edinburg\
h University Press is a scholarly publisher of academic books and journals, based in\
Edinburgh, Scotland."]
The type of company is extracted from the company type query results in sq2:
["http://dbpedia.org/resource/Publishing"]
Web browser names extracted from the query results in sq3:
["Grail","ViolaWWW","Kirix Strata","SharkWire Online","MacWeb","Camino","eww","TenFo\
urFox","WiseStamp","X-Smiles","Netscape Navigator 2","SimpleTest","AWeb","IBrowse","\
iCab","ANT Fresco","Netscape Navigator 9.0","HtmlUnit","ZAC Browser","ELinks","ANT G\
alio","Nintendo DSi Browser","Nintendo DS Browser","Netscape Navigator","NetPositive\
","OmniWeb","Abaco","Flock","Steel","Kazehakase","GNU IceCat","FreeWRL","UltraBrowse\
r","AMosaic","NetCaptor","NetSurf","Netscape Browser","SlipKnot","ColorZilla","Inter\
Linked Data and the Semantic Web
net Channel","Obigo Browser","Swiftfox","BumperCar","Swiftweasel","Swiftdove","IEs4L\
inux","MacWWW","IBM Lotus Symphony","SlimBrowser","cURL","FoxyTunes","Iceweasel","Me\
nuBox","Timberwolf web browser","Classilla","Rockmelt","Galeon","Links","Netscape Na\
vigator","NCSA Mosaic","MidasWWW","w3m","PointerWare","Pogo Browser","Oregano","Avan\
t Browser","Wget","NeoPlanet","Voyager","Amaya","Midori","Sleipnir","Tor","AOL Explo\
Linked Data and Semantic Web Wrap Up
If you enjoyed the material on linked data and DBPedia then please do get a free copy of one of my
semantic web books on my website book page³⁰ as well as other SPARQL and linked data tutorials
on the web.
Structured and sematically labelled data, when it is available, is much easier to process and use
effectively than raw text and HTML collected from web sites.
³⁰http://www.markwatson.com/books/
Web Scraping
In my past work I usually used the Ruby scripting language for web scraping but as I use the Haskell
language more often for projects both large and small I am now using Haskell for web scraping, data
collection, and data cleaning tasks. If you worked through the tutorial chapter on impure Haskell
programming then you already know most of what you need to understand this chapter. Here we
will walk through a few short examples for common web scraping tasks.
Before we start a tutorial about web scraping I want to point out that much of the information on the
web is copyright and the first thing that you should do is to read the terms of service for web sites to
insure that your use of web scraped data conforms with the wishes of the persons or organizations
who own the content and pay to run scraped web sites.
As we saw in the last chapter on linked data there is a huge amount of structured data available on the
web via web services, semantic web/linked data markup, and APIs. That said, you will frequently
find text (usually HTML) that is useful on web sites. However, this text is often at least partially
unstructured and in a messy and frequently changing format because web pages are meant for
human consumption and making them easy to parse and use by software agents is not a priority of
web site owners.
Note: It takes a while to fetch all of the libraries in the directory WebScraping so please do a stack
build now to get these examples ready to experiment with while you read this chapter.
Using the Wreq Library
The Wreq library³¹ is an easy way to fetch data from the web. The example in this section fetches
DBPedia (i.e., the semantic web version of Wikipedia) data in JSON and RDF N3 formats, and also
fetches the index page from my web site. I will introduce you to the Lens library for extracting data
from data structures, and we will also use Lens in a later chapter when writing a program to play
Backjack.
We will be using function get in the Network.Wreq module that has a type signature:
get::String -> IO (Response Data.ByteString.Lazy.Internal.ByteString)
We will be using the OverloadedStrings language extension to facilitate using both [Char] strings
and ByteString data types. Note: In the GHCi repl you can use :set -XOverloadedStrings.
We use function get to return JSON data; here is a bit of the JSON data returned from calling get
using the URI for my web site:
³¹http://www.serpentine.com/wreq/tutorial.html
Web Scraping
Response {responseStatus = Status {statusCode = 200, statusMessage = "OK"},
responseVersion = HTTP/1.1,
responseHeaders =
[("Date","Sat, 15 Oct 2016 16:00:59 GMT"),
("Content-Type","text/html"),
("Transfer-Encoding","chunked"),
("Connection","keep-alive")],
responseBody = "<!DOCTYPE html>\r\n<html>\r\n<head><title>Mark Watson: con\
sultant specializing in artificial intelligence, natural language processing, and ma\
chine\r\n
learning</title>\r\n
<meta name=\"viewport\" content=\"width=device-\
width, initial-scale=1.0\">\r\n
<meta name=\"msvalidate.01\" content=\"D980F894E9\
4AA6335FB595676DFDD5E6\"/>\r\n
<link href=\"/css/bootstrap.min.css\" rel=\"styles\
heet\" type=\"text/css\">\r\n
<link href=\"/css/bootstrap-theme.min.css\" rel=\"s\
tylesheet\" type=\"text/css\">\r\n
<link href=\"/css/mark.css\" rel=\"stylesheet\\
" type=\"text/css\">\r\n
<link rel=\"manifest\" href=\"/manifest.json\">\r\n
style type=\"text/css\">
body {\r\n
padding-top: 60px;\r\n
}</style>\r\n\r\n
<link rel\
=\"canonical\" href=https://www.markwatson.com/ />\r\n</head>\r\n<body
href=\"http:\
//blog.markwatson.com\">Blog</a></li>\r\n
<li class=\"\"><a href=\"/books/\">My Books</a>
As an example, the Lens expression for extracting the response status code is (r is the IO Response
data returned from calling get):
(r ^. responseStatus . statusCode)
responseStatus digs into the top level response structure and statusCode digs further in to fetch
the code 200. To get the actual contents of the web page we can use the responseBody function:
(r ^. responseBody)
Here is the code for the entire example:
Web Scraping
{-# LANGUAGE OverloadedStrings #-}
-- reference: http://www.serpentine.com/wreq/tutorial.html
module HttpClientExample where
import Network.Wreq
import Control.Lens
-- for ^. ^?
import Data.Maybe (fromJust)
fetchURI uri = do
putStrLn $ "\n\n***
Fetching " ++ uri
r <- get uri
putStrLn $ "status code: " ++ (show (r ^. responseStatus . statusCode))
putStrLn $ "content type: " ++ (show (r ^? responseHeader "Content-Type"))
putStrLn $ "respose body: " ++ show (fromJust (r ^? responseBody))
main :: IO ()
main = do
-- JSON from DBPedia
fetchURI "http://dbpedia.org/data/Sedona_Arizona.json"
-- N3 RDF from DBPedia
fetchURI "http://dbpedia.org/data/Sedona_Arizona.n3"
-- my web site
fetchURI "http://markwatson.com"
This example produces a lot of printout, so I a just showing a small bit here (the text from the body
is not shown):
*Main> :l HttpClientExample
[1 of 1] Compiling HttpClientExample ( HttpClientExample.hs, interpreted )
Ok, modules loaded: HttpClientExample.
*HttpClientExample> main
Fetching http://dbpedia.org/data/Sedona_Arizona.json
status code: 200
content type: Just "application/json"
respose body: "{\n
\"http://en.wikipedia.org/wiki/Sedona_Arizona\" : { \"http://xml\
ns.com/foaf/0.1/primaryTopic\" : [ { \"type\" : \"uri\", \"value\" : \"http://dbpedi\
a.org/resource/Sedona_Arizona\" } ] } ,\n
\"http://dbpedia.org/resource/Sedona_Ariz\
ona\" : { \"http://www.w3.org/2002/07/owl#sameAs\" : [ { \"type\" : \"uri\", \"value\
\" : \"http://dbpedia.org/resource/Sedona_Arizona\" } ] ,\n
\"http://www.w3.org/2\
000/01/rdf-schema#label\" : [ { \"type\" : \"literal\", \"value\" : \"Sedona Arizona\
Web Scraping
\" , \"lang\" : \"en\" } ] ,\n
\"http://xmlns.com/foaf/0.1/isPrimaryTopicOf\" : [\
{ \"type\" : \"uri\", \"value\" : \"http://en.wikipedia.org/wiki/Sedona_Arizona\" }\
\"http://www.w3.org/ns/prov#wasDerivedFrom\" : [ { \"type\" : \"uri\", \"v\
alue\" : \"http://en.wikipedia.org/wiki/Sedona_Arizona?oldid=345939723\" } ] ,\n
\"http://dbpedia.org/ontology/wikiPageID\" : [ { \"type\" : \"literal\", \"value\" :\
11034313 , \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\" } ] ,\n
"http://dbpedia.org/ontology/wikiPageRevisionID\" : [ { \"type\" : \"literal\", \"va\
lue\" : 345939723 , \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\" } ] \
\"http://dbpedia.org/ontology/wikiPageRedirects\" : [ { \"type\" : \"uri\", \\
"value\" : \"http://dbpedia.org/resource/Sedona,_Arizona\" } ] }\n}\n"
Fetching http://dbpedia.org/data/Sedona_Arizona.n3
status code: 200
content type: Just "text/n3; charset=UTF-8"
respose body: "@prefix foaf:\t<http://xmlns.com/foaf/0.1/> .\n@prefix wikipedia-en:\\
t<http://en.wikipedia.org/wiki/> .\n@prefix dbr:\t<http://dbpedia.org/resource/> .\n\
wikipedia-en:Sedona_Arizona\tfoaf:primaryTopic\tdbr:Sedona_Arizona .\n@prefix owl:\t\
<http://www.w3.org/2002/07/owl#> .\ndbr:Sedona_Arizona\towl:sameAs\tdbr:Sedona_Arizo\
na .\n@prefix rdfs:\t<http://www.w3.org/2000/01/rdf-schema#> .\ndbr:Sedona_Arizona\t\
rdfs:label\t\"Sedona Arizona\"@en ;\n\tfoaf:isPrimaryTopicOf\twikipedia-en:Sedona_Ar\
izona .\n@prefix prov:\t<http://www.w3.org/ns/prov#> .\ndbr:Sedona_Arizona\tprov:was\
DerivedFrom\t<http://en.wikipedia.org/wiki/Sedona_Arizona?oldid=345939723> .\n@prefi\
x dbo:\t<http://dbpedia.org/ontology/> .\ndbr:Sedona_Arizona\tdbo:wikiPageID\t110343\
13 ;\n\tdbo:wikiPageRevisionID\t345939723 ;\n\tdbo:wikiPageRedirects\t<http://dbpedi\
a.org/resource/Sedona,_Arizona> ."
Fetching http://markwatson.com
status code: 200
content type: Just "text/html"
respose body: "<!DOCTYPE html>\r\n<html>\r\n<head><title>Mark Watson: consultant spe\
cializing in
You might want to experiment in the GHCi repl with the get function and Lens. If so, this will get
you started:
Web Scraping
*Main> :set -XOverloadedStrings
*Main> r <- get "http://dbpedia.org/data/Sedona_Arizona.json"
*Main> :t r
r :: Response ByteString
*Main> (r ^. responseStatus . statusCode)
*Main> (r ^? responseHeader "Content-Type")
Just "application/json"
*Main> fromJust (r ^? responseHeader "Content-Type")
"application/json"
*Main> (fromJust (r ^? responseBody))
\"http://en.wikipedia.org/wiki/Sedona_Arizona\" : { ... not shown ... \"
In the following section we will use the HandsomeSoup library for parsing HTML.
Using the HandsomeSoup Library for Parsing HTML
We will now use the Handsome Soup³² library to parse HTML. Handsome Soup allows us to use CSS
style selectors to extract specific elements from the HTML from a web page. The HXT lower level
library provides modeling HTML (and XML) as a tree structure and an Arrow³³ style interface for
traversing the tree structures and extract data. Arrows are a generalization of monads to manage
calculations given a context. I will touch upon just enough material on Arrows for you to understand
the examples in this chapter. Handsome Soup also provides a high level utility function fromUrl to
fetch web pages; the type of fromUrl is:
fromUrl
:: String -> IOSArrow b (Data.Tree.NTree.TypeDefs.NTree XNode)
We will not work directly with the tree structure of the returned data, we will simply use the accessor
functions to extract the data we need. Before looking at the example code listing, let’s look at this
extraction process (doc is the tree structured data returned from calling fromUrl):
links <- runX $ doc >>> css "a" ! "href"
The runX function runs arrow computations for us. doc is a tree data structure, css allows us to
pattern match on specific HTML elements.
Here we are using CSS style selection for all “a” anchor HTML elements and digging into the element
to return the element attribute “href” value for each “a” anchor element. In a similar way, we can
select all “img” image elements and dig down into the matched elements to fetch the “src” attributes:
³²https://github.com/egonSchiele/HandsomeSoup
³³https://wiki.haskell.org/Arrow
Web Scraping
imageSrc <- runX $ doc >>> css "img" ! "src"
We can get the full body text:
allBodyText <- runX $ doc >>> css "body" //> getText
The operator //> applied to the function getText will get all text in all nested elements inside the
body element. If we had used the operator /> then we would only have fetched the text at the top
level of the body element.
Here is the full example source listing:
{-# LANGUAGE OverloadedStrings #-}
-- references: https://github.com/egonSchiele/HandsomeSoup
http://adit.io/posts/2012-04-14-working_with_HTML_in_haskell.html
module Main where
import Text.XML.HXT.Core
import Text.HandsomeSoup
main :: IO ()
main = do
let doc = fromUrl "http://markwatson.com/"
putStrLn "\n\n ** LINKS:\n"
links <- runX $ doc >>> css "a" ! "href"
mapM_ putStrLn links
h2 <- runX $ doc >>> css "h2" ! "href"
putStrLn "\n\n ** ALL H2 ELEMENTS::\n"
mapM_ putStrLn h2
imageSrc <- runX $ doc >>> css "img" ! "src"
putStrLn "\n\n ** ALL IMG ELEMENTS:\n"
mapM_ putStrLn imageSrc
allBodyText <- runX $ doc >>> css "body" //> getText
putStrLn "\n\n ** TEXT FROM BODY ELEMENT:\n"
mapM_ putStrLn allBodyText
pText <- runX $ doc >>> css "p" //> getText -- //> gets all contained text
gets only directly
contained text
putStrLn "\n\n ** ALL P ELEMENTS:\n"
mapM_ putStrLn pText
This example prints out several hundred lines; here is the first bit of output:
Web Scraping
*Main> :l HandsomeSoupTest.hs
[1 of 1] Compiling HandsomeSoupTest ( HandsomeSoupTest.hs, interpreted )
Ok, modules loaded: HandsomeSoupTest.
*HandsomeSoupTest> main
** LINKS:
/consulting/
http://blog.markwatson.com
/books/
/opensource/
https://github.com/mark-watson
https://plus.google.com/117612439870300277560
https://twitter.com/mark_l_watson
https://www.wikidata.org/wiki/Q18670263
http://markwatson.com/index.rdf
http://markwatson.com/index.ttl
** ALL IMG ELEMENTS:
/pictures/Markws.jpg
** TEXT FROM BODY ELEMENT:
I find HandsomeSoup to be very convenient for picking apart HTML data fetched from web pages.
Writing a good spider for any given web site is a process of understanding how the HTML for the
web site is structured and what information you need to collect. I strongly suggest that you work
with the web page to be spider open in a web browser with “show source code” in another browser
tab. Then open an interactive GHCi repl and experiment using the HandsomeSoup APIs to get the
data you need.
Web Scraping Wrap Up
There are many Haskell library options for web scraping and cleaning data. In this chapter I showed
you just what I use in my projects.
The material in this chapter and the chapters on text processing and linked data should be sufficient
to get you started using online data sources in your applications.
Using Relational Databases
We will see how to use popular libraries for accessing the sqlite and Postgres (sometimes also called
PostgeSQL) databases in this chapter. I assume that you are already familiar with SQL.
Database Access for Sqlite
We will use the sqlite-simple³⁴ library in this section to access Sqlite databases and use the similar
library postgresql-simple³⁵ in the next section for use with Postgres.
There are other good libraries for database connectivity like Persistent³⁶ but I like sqlite-simple and
it has a gentle learning curve so that is what we will use here. You will learn the basics of database
connectivity in this and the next section. Setting up and using sqlite is easy because the sqlite-
simple library includes the compiled code for sqlite so configuration requires only the file path to
the database file.
{-# LANGUAGE OverloadedStrings #-}
module Main where
import Database.SQLite.Simple
Create sqlite database:
sqlite3 test.db "create table test (id integer primary key, str text);"
This example is derived from the example at github.com/nurpax/sqlite-simple
main :: IO ()
main = do
conn <- open "test.db"
-- start by getting table names in database:
r <- query_ conn
"SELECT name FROM sqlite_master WHERE type='table'" :: IO [Only String]
³⁴https://hackage.haskell.org/package/sqlite-simple
³⁵https://hackage.haskell.org/package/postgresql-simple
³⁶https://www.stackage.org/package/persistent
Using Relational Databases
print "Table names in database test.db:"
mapM_ (print . fromOnly) r
-- get the metadata for table test in test.db:
r <- query_ conn
"SELECT sql FROM sqlite_master WHERE type='table' and name='test'" ::
IO [Only String]
print "SQL to create table 'test' in database test.db:"
mapM_ (print . fromOnly) r
-- add a row to table 'test' and then print out the rows in table 'test':
execute conn "INSERT INTO test (str) VALUES (?)"
(Only ("test string 2" :: String))
r2 <- query_ conn "SELECT * from test" :: IO [(Int, String)]
print "number of rows in table 'test':"
print (length r2)
print "rows in table 'test':"
mapM_ print
close conn
The type Only used in line 20 acts as a container for a single value and is defined in the simple-sqlite
library. It can also be used to pass values for queries like:
r <- query_ conn "SELECT name FROM customers where id = ?" (Only 4::Int)
To run this example start by creating a sqlite database that is stored in the file test.db:
sqlite3 test.db "create table test (id integer primary key, str text);"
Then build and run the example:
stack build --exec TestSqLite1
Database Access for Postgres
Setting up and using a database in the last section was easy because the sqlite-simple library includes
the compiled code for sqlite so configuration only requires the file path the the database file. The
Using Relational Databases
Haskel examples for Postgres will be similar to those for Sqlite. There is some complication in setting
up Postgres if you do not already have it installed and configured.
In any case, you will need to have Postgres installed and set up with a user account for yourself.
When I am installing and configuring Postgres on my Linux laptop, I create a database role markw.
You will certainly create a different role/account name so subsitute your role name for markw in
the following code examples.
If you are using Ubuntu you can install Postgres and create a role using:
sudo apt-get update
sudo apt-get install postgresql postgresql-contrib postgresql-server-dev-9.5
sudo -u postgres createuser --interactive
Enter name of role to add: markw
Shall the new role be a superuser? (y/n) y
We will need to install postgresql-server-dev-9.5 in order to use the Haskell Postgres bindings. Note
that your version of Ubuntu Linux may have a different version of the server dev package which
you can find using:
aptitude search postgresql-dev
If you are using Mac OS X you can then install Postgres as an application which is convenient for
development. A role is automatically created with the same name as your OS X “short name.” You
can use the “Open psql” button on the interface to open a command line shell that functions like the
psql command on Ubuntu (or other Linux distributions).
We will need to install postgresql-server-dev-9.5 in order to use the Haskell Postgres bindings. Note
that your version of Ubuntu Linux may have a different version of the server dev package which
you can find using:
aptitude search postgresql-dev
You will then want to create a database named haskell and set the password for role/account markw
to test1 for running the example in this section:
Using Relational Databases
createdb haskell
sudo -u postgres psql
postgres=# alter user markw encrypted password 'test1';
postgres=# \q
psql -U markw haskell
psql (9.5.4)
Type "help" for help.
haskell=# create table customers (id int, name text, email text);
CREATE TABLE
haskell=#
insert into customers values (1, 'Acme Cement', 'info@acmecement.com');
INSERT 0 1
haskell=# \q
If you are not familiar with using Postgres then take a minute to experiment with using the psql
command line utility to connect to the database you just created and peform practice queries:
markw=# \c haskell
You are now connected to database "haskell" as user "markw".
haskell=# \d
List of relations
Schema |
| Type
| Owner
--------+-----------+-------+-------
public | customers | table | markw
public | links
| table | markw
public | products
| table | markw
(3 rows)
haskell=# select * from customers;
----+-----------------+---------------------
1 | Acme Cement
| info@acmecement.com
2 | Biff Home Sales | info@biff.com
3 | My Pens
| info@mypens.com
(3 rows)
haskell=# select * from products;
| cost
----+---------------+------
1 | Cement bag
2 | Cheap Pen
3 | Expensive Pen | 14.5
Using Relational Databases
(3 rows)
haskell=# select * from links;
id | customer_id | productid
----+-------------+-----------
(3 rows)
haskell=#
You can change default database settings using ConnectInfo:
ConnectInfo
connectHost :: String
connectPort :: Word16
connectUser :: String
connectPassword :: String
connectDatabase :: String
In the following example on lines 9-10 I use defaultConnectInfo that lets me override just some
settings, leaving the rest set at default values. The code to access a database using simple-postgresql
is similar to that in the last section, with a few API changes.
{-# LANGUAGE OverloadedStrings #-}
module Main where
import Database.PostgreSQL.Simple
main :: IO ()
main = do
conn <- connect defaultConnectInfo { connectDatabase = "haskell",
connectUser = "markw" }
-- start by getting table names in database:
r <- query_ conn "SELECT name FROM customers" :: IO [(Only String)]
print "names and emails in table 'customers' in database haskell:"
mapM_ (print . fromOnly) r
-- add a row to table 'test' and then print out the rows in table 'test':
Using Relational Databases
let rows :: [(Int, String, String)]
rows = [(4, "Mary Smith", "marys@acme.com")]
executeMany conn
"INSERT INTO customers (id, name, email) VALUES (?,?,?)" rows
r2 <- query_ conn "SELECT * from customers" :: IO [(Int, String, String)]
print "number of rows in table 'customers':"
print (length r2)
print "rows in table 'customers':"
mapM_ print
close conn
The type Only used in line 20 acts as a container for a single value and is defined in the simple-
postgresql library. It can also be used to pass values for queries like:
r <- query_ conn "SELECT name FROM customers where id = ?" (Only 4::Int)
The monad mapping function mapM_ using in line 22 is like mapM but is used when we do not
need the resulting collection from executing the map operation. mapM_ is used for side effects, in
this case extracting the value for a collection of Only values and printing them. I removed some
output from building the example in the following listing:
$ Database-postgres git:(master) > stack build --exec TestPostgres1
TestDatabase-0.1.0.0: build
Preprocessing executable 'TestPostgres1' for TestDatabase-0.1.0.0...
[1 of 1] Compiling Main
( TestPostgres1.hs,
"names and emails in table 'customers' in database haskell:"
"Acme Cement"
"Biff Home Sales"
"My Pens"
"number of rows in table 'customers':"
"rows in table 'customers':"
(1,"Acme Cement","info@acmecement.com")
(2,"Biff Home Sales","info@biff.com")
(3,"My Pens","info@mypens.com")
(4,"Mary Smith","marys@acme.com")
Postgres is my default database and I use it unless there is a compelling reason not to. While work
for specific customers has mandated using alternative data stores (e.g., BigTable while working at
Google and MongoDB at Compass Labs), Postgres supports relational tables, free text search, and
structured data like JSON.
Haskell Program to Play the Blackjack
Card Game
For much of my work using Haskell I deal mostly with pure code with smaller bits of impure code
for network and file IO, etc. Realizing that my use case for using Haskell (mostly pure code) may
not be typical, I wanted the last example “cookbook recipe” in this book to be an example dealing
with changing state, a program to play the Blackjack card game.
The game state is maintained in the type Table that holds information on a randomized deck of cards,
the number of players in addition to the game user and the card dealer, the cards in the current hand,
and the number of betting chips that all players own. Table data is immutable so all of the major
game playing functions take a table and any other required inputs, and generate a new table as the
function result.
This example starts by asking how many players, besides the card dealer and the game user, should
play a simulated Blackjack game. The game user controls when they want another card while the
dealer and any other simulated players play automatically (they always hit when their card score is
less than 17).
I define the types for playing cards and an entire card deck in the file Card.hs:
module Card (Card, Rank, Suit, orderedCardDeck, cardValue) where
import Data.Maybe (fromMaybe)
import Data.List (elemIndex)
import Data.Map (fromList, lookup, keys)
data Card = Card { rank :: Rank
, suit :: Suit }
deriving (Eq, Show)
data Suit = Hearts | Diamonds | Clubs | Spades
deriving (Eq, Show, Enum, Ord)
data Rank = Two | Three | Four
| Five | Six | Seven | Eight
| Nine | Ten | Jack
| Queen | King | Ace
deriving (Eq, Show, Enum, Ord)
rankMap = fromList [(Two,2), (Three,3), (Four,4), (Five,5),
Haskell Program to Play the Blackjack Card Game
(Six,6), (Seven,7), (Eight,8), (Nine,9),
(Ten,10), (Jack,10), (Queen,10),
(King,10), (Ace,11)]
orderedCardDeck :: [Card]
orderedCardDeck = [Card rank suit | rank <- keys rankMap,
suit <- [Hearts .. Clubs]]
cardValue :: Card -> Int
cardValue aCard =
case (Data.Map.lookup (rank aCard) rankMap) of
Just n -> n
Nothing -> 0 -- should never happen
As usual, the best way to understand this code is to go to the GHCi repl:
*Main Card RandomizedList Table> :l Card
[1 of 1] Compiling Card
( Card.hs, interpreted )
Ok, modules loaded: Card.
*Card> :t orderedCardDeck
orderedCardDeck :: [Card]
*Card> orderedCardDeck
[Card {rank = Two, suit = Hearts},Card {rank = Two, suit = Diamonds},Card {rank = Tw\
o, suit = Clubs},Card {rank = Three, suit = Hearts},Card {rank = Three,
*Card> head orderedCardDeck
Card {rank = Two, suit = Hearts}
*Card> cardValue $ head orderedCardDeck
So, we have a sorted deck of cards and a utility function for returning the numerical value of a card
(we always count ace cards as 11 points, deviating from standard Blackjack rules).
The next thing we need to get is randomly shuffled lists. The Haskell Wiki³⁷ has a good writeup on
randomizing list elements and we are borrowing their function randomizedList (you can see the
source code in the file RandomizedList.hs). Here is a sample use:
³⁷https://wiki.haskell.org/Random_shuffle
Haskell Program to Play the Blackjack Card Game
*Card> :l RandomizedList.hs
[1 of 1] Compiling RandomizedList
( RandomizedList.hs, interpreted )
Ok, modules loaded: RandomizedList.
*RandomizedList> import Card
*RandomizedList Card> randomizedList orderedCardDeck
[Card {rank = Queen, suit = Hearts},Card {rank = Six, suit = Diamonds},Card {rank = \
Five, suit = Clubs},Card {rank = Five, suit = Diamonds},Card {rank = Seven, suit = C\
lubs},Card {rank = Three, suit = Hearts},Card {rank = Four, suit = Diamonds},Card {r\
ank = Ace, suit = Hearts},
Much of the complexity in this example is implemented in Table.hs which defines the type Table
and several functions to deal and score hands of dealt cards:
• createNewTable :: Players -> Table. Players is the integer number of other players at the table.
• setPlayerBet :: Int -> Table -> Table. Given a new value to bet and a table, generate a new
modified table.
• showTable :: Table -> [Char]. Given a table, generate a string describing the table (in a format
useful for development)
• initialDeal :: [Card] -> Table -> Int -> Table. Given a randomized deck of cards, a table, and the
number of other players, generate a new table.
• changeChipStack :: Int -> Int -> Table -> Table. Given a player index (index order: user, dealer,
and other players), a new number of betting chips for the player, and a table, then generate a
new modified table.
• setCardDeck :: [Card] -> Table -> Table. Given a randomized card deck and a table, generate a
new table containing the new randomized card list; all other table data is unchanged.
• dealCards :: Table -> [Int] -> Table. Given a table and a list of player indices for players wanting
another card, generate a new modified table.
• resetTable :: [Card] -> Table -> Int -> Table. Given a new randomized card deck, a table, and a
new number of other players, generate a new table.
• scoreHands :: Table -> Table. Given a table, score all dealt hands and generate a new table with
these scores. There is no table type score data, rather, we “score” by changing the number of
chips all of the players (inclding the dealer) has.
• dealCardToUser :: Table -> Int -> Table. For the game user, always deal a card. For the dealer
and other players, deal another card if their hand score is less than 17.
• handOver :: Table -> Bool. Determine if the current hand is over.
• setPlayerPasses :: Table -> Table. Call this function when the payer passes. Other players and
dealer are then played out automatically.
The implementation in the file Table.hs is fairly simple, with the exception of the use of Haskell
lenses to access nested data in the table type. I will discuss the use of lenses after the program listing,
but: as you are reading the code look out for variables starting with the underscore character _ that
alerts the Lens system that it should create data accessors for these variables:
Haskell Program to Play the Blackjack Card Game
{-# LANGUAGE TemplateHaskell #-}
-- for makeLens
module Table (Table (..), createNewTable, setPlayerBet, showTable, initialDeal,
changeChipStack, setCardDeck, dealCards, resetTable, scoreHands,
dealCardToUser, handOver, setPlayerPasses) where
-- note: export dealCardToUser only required for ghci development
import Control.Lens
import Card
import Data.Bool
import Data.Maybe (fromMaybe)
data Table = Table { _numPlayers :: Int
, _chipStacks :: [Int] -- number of chips,
-- indexed by player index
, _dealtCards :: [[Card]] -- dealt cards for user,
-- dealer, and other players
, _currentPlayerBet :: Int
, _userPasses
:: Bool
, _cardDeck
:: [Card]
deriving (Show)
type Players = Int
createNewTable :: Players -> Table
createNewTable n =
Table n
[500 | _ <- [1 .. n]] -- give each player (incuding dealer) 10 chips
[[] | _ <- [0..n]] -- dealt cards for user and other players
-- (we don't track dealer's chips)
20 -- currentPlayerBet number of betting chips
[] -- placeholder for random shuffled card deck
resetTable :: [Card] -> Table -> Int -> Table
resetTable cardDeck aTable numberOfPlayers =
Table numberOfPlayers
(_chipStacks aTable)
-- using Lens accessor
[[] | _ <- [0..numberOfPlayers]]
(_currentPlayerBet aTable) -- using Lens accessor
Haskell Program to Play the Blackjack Card Game
cardDeck
-- Use lens extensions for type Table:
makeLenses ''Table
showDealtCards :: [[Card]] -> String
showDealtCards dc =
(show [map cardValue hand | hand <- dc])
setCardDeck :: [Card] -> Table -> Table
setCardDeck newDeck =
over cardDeck (\_ -> newDeck)
-- change value to new card deck
dealCards :: Table -> [Int] -> Table
dealCards aTable playerIndices =
last $ scanl dealCardToUser aTable playerIndices
initialDeal cardDeck aTable numberOfPlayers =
dealCards
(dealCards (resetTable cardDeck aTable numberOfPlayers)
[0 .. numberOfPlayers])
[0 .. numberOfPlayers]
showTable :: Table -> [Char]
showTable aTable =
"\nCurrent table data:\n" ++
Chipstacks: " ++
Player: " ++ (show (head (_chipStacks aTable))) ++
Other players: " ++ (show (tail (_chipStacks aTable))) ++
User cards: " ++ (show (head (_dealtCards aTable))) ++
Dealer cards: " ++ (show ((_dealtCards aTable) !! 1)) ++
Other player's cards: " ++ (show (tail (tail(_dealtCards aTable)))) ++
-- "\n
Dealt cards: " ++ (show (_dealtCards aTable)) ++
Dealt card values: " ++ (showDealtCards (_dealtCards aTable)) ++
Current player bet: " ++
(show (_currentPlayerBet aTable)) ++
Player pass: " ++
(show (_userPasses aTable)) ++ "\n"
clipScore aTable playerIndex =
let s = score aTable playerIndex in
if s < 22 then s else 0
Haskell Program to Play the Blackjack Card Game
scoreHands aTable =
let chipStacks2 = _chipStacks aTable
playerScore = clipScore aTable 0
dealerScore = clipScore aTable 1
otherScores = map (clipScore aTable) [2..]
newPlayerChipStack = if playerScore > dealerScore then
(head chipStacks2) + (_currentPlayerBet aTable)
if playerScore < dealerScore then
(head chipStacks2) - (_currentPlayerBet aTable)
else (head chipStacks2)
newOtherChipsStacks =
map (\(x,y) -> if x > dealerScore then
y + 20
if x < dealerScore then
y - 20
else y)
(zip otherScores (tail chipStacks2))
newChipStacks
= newPlayerChipStack:newOtherChipsStacks
over chipStacks (\_ -> newChipStacks) aTable
setPlayerBet :: Int -> Table -> Table
setPlayerBet newBet =
over currentPlayerBet (\_ -> newBet)
setPlayerPasses :: Table -> Table
setPlayerPasses aTable =
let numPlayers = _numPlayers aTable
playerIndices = [1..numPlayers]
t1 = over userPasses (\_ -> True) aTable
t2 = dealCards t1 playerIndices
t3 = dealCards t2 playerIndices
t4 = dealCards t3 playerIndices
changeChipStack :: Int -> Int -> Table -> Table
changeChipStack playerIndex newValue =
over chipStacks (\a -> a & element playerIndex .~ newValue)
Haskell Program to Play the Blackjack Card Game
scoreOLD aTable playerIndex =
let scores = map cardValue ((_dealtCards aTable) !! playerIndex)
totalScore = sum scores in
if totalScore < 22 then totalScore else 0
score aTable playerIndex =
let scores = map cardValue ((_dealtCards aTable) !! playerIndex)
totalScore = sum scores in
totalScore
dealCardToUser' :: Table -> Int -> Table
dealCardToUser' aTable playerIndex =
let nextCard = head $ _cardDeck aTable
playerCards = nextCard : ((_dealtCards aTable) !! playerIndex)
newTable = over cardDeck (\cd -> tail cd) aTable in
over dealtCards (\a -> a & element playerIndex .~ playerCards) newTable
dealCardToUser :: Table -> Int -> Table
dealCardToUser aTable playerIndex
| playerIndex == 0
= dealCardToUser' aTable playerIndex -- user
| otherwise
= if (score aTable playerIndex) < 17 then
dealCardToUser' aTable playerIndex
else aTable
handOver :: Table -> Bool
handOver aTable =
_userPasses aTable
In line 48 we use the function makeLenses to generate access functions for the type Table. We will
look in some detail at lines 54-56 where we use the lense over function to modify a nested value in
a table, returning a new table:
setCardDeck :: [Card] -> Table -> Table
setCardDeck newDeck =
over cardDeck (\_ -> newDeck)
The expression in line 3 evaluates to a partial function that takes another argument, a table, and
returns a new table with the card deck modified. Function over expects a function as its second
argument. In this example, the inline function ignores the argument it is called with, which would
be the old card deck value, and returns the new card deck value which is placed in the table value.
Using lenses can greatly simplify the code to manipulate complex types.
Haskell Program to Play the Blackjack Card Game
Another place where I am using lenses is in the definition of function scoreHands (lines 88-109). On
line 109 we are using the over function to replace the old player betting chip counts with the new
value we have just calculated:
over chipStacks (\_ -> newChipStacks) aTable
Similarly, we use over in line 113 to change the current player bet. In function handOver on line
157, notice how I am using the generated function _userPasses to extract the value of the user passes
boolean flag from a table.
The function main, defined in the file Main.hs, uses the code we have just seen to represent a table
and modify a table, is fairly simple. A main game loop repetitively accepts game user imput, and
calls the appropriate functions to modify the current table, producing a new table. Remember that
the table data is immutable: we always generate a new table from the old table when we need to
modify it.
module Main where
import Card
-- pure code
import Table
-- pure code
import RandomizedList
-- impure code
printTable :: Table -> IO ()
printTable aTable =
putStrLn $ showTable aTable
randomDeck =
randomizedList orderedCardDeck
gameLoop :: Table -> Int -> IO b
gameLoop aTable numberOfPlayers = do
printTable aTable
cardDeck <- randomDeck
if (handOver aTable) then
putStrLn "\nHand over. State of table at the end of the game:\n"
printTable aTable
putStrLn "\nNewly dealt hand:\n"
gameLoop (initialDeal cardDeck (scoreHands aTable)
numberOfPlayers)
numberOfPlayers
Haskell Program to Play the Blackjack Card Game
putStrLn "Enter command:"
putStrLn "
h)it or set bet to 10, 20, 30; any other key to stay:"
command <- getLine
if elem command ["10", "20", "30"] then
gameLoop (setPlayerBet (read command) aTable) numberOfPlayers
if command == "h" then
gameLoop (dealCards aTable [0 .. numberOfPlayers]) numberOfPlayers
gameLoop (setPlayerPasses (dealCards aTable [1 .. numberOfPlayers]))
numberOfPlayers
-- player stays (no new cards)
main :: IO b
main = do
putStrLn "Start a game of Blackjack. Besides yourself, how many other"
putStrLn "players do you want at the table?"
s <- getLine
let num = (read s :: Int) + 1
cardDeck <- randomDeck
let aTable = initialDeal cardDeck (createNewTable num) num
gameLoop aTable num
I encourage you to try playing the game yourself, but if you don’t here is a sample game:
*Main Card RandomizedList Table> main
Start a game of Blackjack. Besides yourself, how many other
players do you want at the table?
Current table data:
Chipstacks:
Player: 500
Other players: [500]
User cards: [Card {rank = Three, suit = Clubs},Card {rank = Two, suit = Hearts}]
Dealer cards: [Card {rank = Queen, suit = Diamonds},Card {rank = Seven, suit = Clu\
Other player's cards: [[Card {rank = King, suit = Hearts},Card {rank = Six, suit =\
Diamonds}]]
Dealt card values: [[3,2],[10,7],[10,6]]
Current player bet: 20
Player pass: False
Haskell Program to Play the Blackjack Card Game
Enter command: h)it or set bet to 10, 20, 30; any other key to stay:
Current table data:
Chipstacks:
Player: 500
Other players: [500]
User cards: [Card {rank = Six, suit = Hearts},Card {rank = Three, suit = Clubs},Ca\
rd {rank = Two, suit = Hearts}]
Dealer cards: [Card {rank = Queen, suit = Diamonds},Card {rank = Seven, suit = Clu\
Other player's cards: [[Card {rank = Eight, suit = Hearts},Card {rank = King, suit\
= Hearts},Card {rank = Six, suit = Diamonds}]]
Dealt card values: [[6,3,2],[10,7],[8,10,6]]
Current player bet: 20
Player pass: False
Enter command: h)it or set bet to 10, 20, 30; any other key to stay:
Current table data:
Chipstacks:
Player: 500
Other players: [500]
User cards: [Card {rank = King, suit = Clubs},Card {rank = Six, suit = Hearts},Car\
d {rank = Three, suit = Clubs},Card {rank = Two, suit = Hearts}]
Dealer cards: [Card {rank = Queen, suit = Diamonds},Card {rank = Seven, suit = Clu\
Other player's cards: [[Card {rank = Eight, suit = Hearts},Card {rank = King, suit\
= Hearts},Card {rank = Six, suit = Diamonds}]]
Dealt card values: [[10,6,3,2],[10,7],[8,10,6]]
Current player bet: 20
Player pass: False
Enter command: h)it or set bet to 10, 20, 30; any other key to stay:
Current table data:
Chipstacks:
Player: 500
Other players: [500]
User cards: [Card {rank = King, suit = Clubs},Card {rank = Six, suit = Hearts},Car\
d {rank = Three, suit = Clubs},Card {rank = Two, suit = Hearts}]
Dealer cards: [Card {rank = Queen, suit = Diamonds},Card {rank = Seven, suit = Clu\
Haskell Program to Play the Blackjack Card Game
Other player's cards: [[Card {rank = Eight, suit = Hearts},Card {rank = King, suit\
= Hearts},Card {rank = Six, suit = Diamonds}]]
Dealt card values: [[10,6,3,2],[10,7],[8,10,6]]
Current player bet: 20
Player pass: True
Hand over. State of table at the end of the game:
Current table data:
Chipstacks:
Player: 520
Other players: [520]
User cards: [Card {rank = King, suit = Clubs},Card {rank = Six, suit = Hearts},Car\
d {rank = Three, suit = Clubs},Card {rank = Two, suit = Hearts}]
Dealer cards: [Card {rank = Queen, suit = Diamonds},Card {rank = Seven, suit = Clu\
Other player's cards: [[Card {rank = Eight, suit = Hearts},Card {rank = King, suit\
= Hearts},Card {rank = Six, suit = Diamonds}]]
Dealt card values: [[10,6,3,2],[10,7],[8,10,6]]
Current player bet: 20
Player pass: True
Here the game user has four cards with values of [10,6,3,2] for a winning score of 21. The dealer has
[10,7] for a score of 17 and the other player has [8,10,6], a value greater than 21 so the player went
“bust.”
I hope that you enjoyed this last example that demonstrates a reasonable approach for managing
state when using immutable data.
Section 3 - Larger Projects
This section is new for the second edition of this book. So far we have covered the basics of Haskell
programming and seen many examples. In this section we look at a few new projects that I derived
from my own work and these new examples will hopefully further encourage you to think of novel
uses for Haskell in your own work.
The project knowledge_graph_creator helps to automate the process of creating Knowledge
Graphs from raw text input and generates data for both the Neo4J open source graph database
as well as RDF data for use in semantic web and linked data applications. I have also implemented
this same application in Common Lisp that is also a new example in the latest edition of my book
Loving Common Lisp, Or The Savvy Programmer’s Secret Weapon³⁸ (released September 2019).
The next two chapters in this section are similar in that they both use examples of using Python for
Natural Language Processing (NLP) tasks, wrapping the Python code as a REST service, and then
writing Haskell clients for these services.
The project HybridHaskellPythonNlp uses web services written in Python for natural language
processing. The Python web services use the SpaCy library.
The project HybridHaskellPythonCorefAnaphoraResolution uses web services written in Python
to allow Haskell applications to use deep learning models created with TensorFlow and Keras.
In these last two examples I use REST APIs to access code written in Python. A good alternative that
I don’t cover in this book is using the servant library³⁹ for generating distributed applications.
³⁸https://leanpub.com/lovinglisp
³⁹https://www.servant.dev/
Knowledge Graph Creator
The large project described here processes raw text inputs and generates data for knowledge graphs
in formats for both the Neo4J graph database and in RDF format for semantic web and linked data
applications.
This application works by identifying entities in text. Example entity types are people, companies,
country names, city names, broadcast network names, political party names, and university names.
We saw earlier code for detecting entities in the chapter on natural language processing (NLP) and
we will reuse this code. We will discuss later three strategies for reusing code from different projects.
The following figure shows part of a Neo4J Knowledge Graph created with the example code. This
graph has shortened labels in displayed nodes but Neo4J offers a web browser-based console that lets
you interactively explore Knowledge Graphs. We don’t cover setting up Neo4J here so please use the
Neo4J documentation⁴⁰. As an introduction to RDF data, the semantic web, and linked data you can
get free copies of my two books Practical Semantic Web and Linked Data Applications, Common
Lisp Edition⁴¹ and Practical Semantic Web and Linked Data Applications, Java, Scala, Clojure, and
JRuby Edition⁴².
Part of a Knowledge Graph shown in Neo4J web application console
There are two versions of this project that deal with generating duplicate data in two ways:
• As either Neo4J Cypher data or RDF triples data are created, store generated data in a SQLite
embedded database. Check this database before writing new output data.
• Ignore the problem of generating duplicate data and filter out duplicates in the outer processing
pipeline that uses the Knowledge Graph Creator as one processing step.
⁴⁰https://neo4j.com/docs/operations-manual/current/introduction/
⁴¹http://markwatson.com/opencontentdata/book_lisp.pdf
⁴²http://markwatson.com/opencontentdata/book_java.pdf
Knowledge Graph Creator
For my own work I choose the second method since filtering duplicates is as easy as a few Makefile
targets (the following listing is in the file Makefile in the directory haskell_tutorial_cookbook_-
examples/knowledge_graph_creator_pure):
all: gendata rdf cypher
gendata:
stack build --fast --exec Dev-exe
echo "Removing duplicate RDF statements"
awk '!visited[$$0]++' out.n3 > output.n3
rm -f out.n3
cypher:
echo "Removing duplicate Cypher statements"
awk '!visited[$$0]++' out.cypher > output.cypher
rm -f out.cypher
The Haskell KGCreator application we develop here writes output files out.n3 (N3 is a RDF data
format) and out.cypher (Cypher is the import output format and query language for the Neo4J open
source and commercial graph database). The awk commands remove duplicate lines and write de-
duplicated data to output.n3 and output.cypher.
We will use this second approach but the next section provides sufficient information and a link to
alternative code in case you are interested in using SQLite to prevent duplicate data generation.
Notes for Using SQLite to Avoid Duplicates (Optional Material)
We saw two methods of avoiding duplicates in generated data in the last section. If you want to
use the first method for avoiding generating duplicate data, I leave it as an exercise but here are
some notes to get you started: you can then modify the example code by using the utility function
Blackboard.h in the directory knowledge_graph_creator_pure/src/fileutils and implement the
logic seen below for checking new generated data to see if it is in the SQLite database. This first
method as it also is a good example for wrapping the embedded SQLite library in an IO Monad and
is left as an exercise, otherwise skip this section.
Before you write either an RDF statement or a Neo4J Cypher data import statement, check to see if
the statement has already been written using something like:
Knowledge Graph Creator
check <- blackboard_check_key new_data_uri
if check
and after writing a RDF statement or a Neo4J Cypher data import statement, write it to the
temportary SQLite database using something like:
blackboard_write newStatementString
For the rest of the chapter we will use the approach of not keeping track of generated data in SQLite
and instead remove duplicates during postprocessing using the standard awk command line utility.
This section is optional. In the rest of this chapter we use the example code in knowledge_graph_-
creator_pure.
Code Layout For the KGCreator Project and strategies
for sharing Haskell code between projects
We will reuse the code for finding entities that we studied in an earlier chapter. There are several
ways to reuse code from multiple local Haskell projects:
• In a project’s cabal file, use relative paths to the source code for other projects. This is
my preferred way to work but has the drawback that the stack command sdist to make a
distribution tarball will not work with relative paths. If this is a problem for you then create
relative symbolic file links to the source directories in other projects.
• In your project’s stack.yaml file, add the other project’s name and path as a extra-deps.
• In library projects, define a packages definition and install the library globally on your system.
I almost always use the first method on my projects with dependencies on other local projects I work
on and this is also the approach we use here. The relevant lines in the file KGCreator.cabal are:
library
exposed-modules:
CorefWebClient
NlpWebClient
ClassificationWebClient
DirUtils
FileUtils
BlackBoard
GenTriples
GenNeo4jCypher
Knowledge Graph Creator
Categorize
NlpUtils
Summarize
Entities
other-modules:
Paths_KGCreator
BroadcastNetworkNamesDbPedia
Category1Gram
Category2Gram
CityNamesDbpedia
CompanyNamesDbpedia
CountryNamesDbpedia
PeopleDbPedia
PoliticalPartyNamesDbPedia
Sentence
Stemmer
TradeUnionNamesDbPedia
UniversityNamesDbPedia
hs-source-dirs:
src/webclients
src/fileutils
src/sw
src/toplevel
../NlpTool/src/nlp
../NlpTool/src/nlp/data
This is a standard looking cabal file except for lines 37 and 38 where the source paths reference
the example code for the NlpTool application developed in a previous chapter. The exposed module
BlackBoard (line 8) is not used but I leave it in the cabal file in case you want to experiment with
recording generated data in SQLite to avoid data duplication. You are likely to also want to use
BlackBoard if you modify this example to continuously process incoming data in a production
system. This is left as an exercise.
Before going into too much detail on the implementation let’s look at the layout of the project code:
Knowledge Graph Creator
src/fileutils:
BlackBoard.hs
DirUtils.hs
FileUtils.hs
../NlpTool/src/nlp:
Categorize.hs
Entities.hs
NlpUtils.hs
Sentence.hs
Stemmer.hs
../NlpTool/src/nlp/data:
BroadcastNetworkNamesDbPedia.hs
CompanyNamesDbpedia.hs
TradeUnionNamesDbPedia
Category1Gram.hs
CountryNamesDbpedia.hs
UniversityNamesDbPedia.hs
Category2Gram.hs
PeopleDbPedia.hs
CityNamesDbpedia.hs
PoliticalPartyNamesDbPedia.hs
src/sw:
GenNeo4jCypher.hs
GenTriples.hs
src/toplevel:
Apis.hs
As mentioned before, we are using the Haskell source fies in a relative path ../NlpTool/src/… and
the local src directory. We discuss this code in the next few sections.
The Main Event: Detecting Entities in Text
A primary task in KGCreator is to identify entities (people, places, etc.) in text and then we will
create RDF and Neo4J Cypher data statements using these entities, knowledge of the origin of text
data and general relationships between entities.
We will use the top level code that we developed earlier that is located in the directory ../NlpTool/s-
rc/nlp (please see the chapter Natural Language Processing Tools for more detail):
• Categorize.hs - categorizes text into categories like news, religion, business, politics, science,
• Entities.hs - identifies entities like people, companies, places, new broadcast networks, labor
unions, etc. in text
• Summarize.hs - creates an extractive summary of text
The KGCreator Haskell application looks in a specified directory for text files to process. For each
file with a .txt extension there should be a matching file with the extension .meta that contains a
single line: the URI of the web location where the corresponding text was found. The reason we need
this is that we want to create graph knowledge data from information found in text sources and the
original location of the data is important to preserve. In other words, we want to know where the
data elements in our knowledge graph came from.
Knowledge Graph Creator
We have not looked at an example of using command line arguments yet so let’s go into some detail
on how we do this. Previously when we have defined an output target executable in our .cabal file,
in this case KGCreator-exe, we could use stack to build the executable and run it with:
stack build --fast --exec KGCreator-exe"
Now, we have an executable that requires two arguments: a source input directory and the file
root for generated RDF and Cypher output files. We can pass command line arguments using this
notation:
stack build --fast --exec "KGCreator-exe test_data outtest"
The two command line arguments are:
• test_data which is the file path of a local directory containing the input files
• outtest which is the root file name for generated Neo4J Cypher and RDF output files
If you are using KGCreator in production, then you will want to copy the compiled and linked
executable file KGCreator-exe to somewhere on your PATH like /usr/local/bin.
The following listing shows the file app/Main.hs, the main program for this example that handles
command line arguments and calls two top level functions in src/toplevel/Apis.hs:
module Main where
import System.Environment (getArgs)
import Apis (processFilesToRdf, processFilesToNeo4j)
main :: IO ()
main = do
args <- getArgs
case args of
[] -> error "must supply an input directory containing text and meta files"
[_] -> error "in addition to an input directory, also specify a root file name f\
or the generated RDF and Cypher files"
[inputDir, outputFileRoot] -> do
processFilesToRdf
inputDir $ outputFileRoot ++ ".n3"
processFilesToNeo4j inputDir $ outputFileRoot ++ ".cypher"
_ -> error "too many arguments"
Here we use getArgs in line8 to fetch a list of command line arguments and verify that at
least two arguments have been provided. Then we call the functions processFilesToRdf and
processFilesToNeo4j and the functions they call in the next three sections.
Knowledge Graph Creator
Utility Code for Generating RDF
The code for generating RDF and for generating Neo4J Cypher data is similar. We start with the code
to generate RDF triples. Before we look at the code, let’s start with a few lines of generated RDF:
<http://dbpedia.org/resource/The_Wall_Street_Journal>
<http://knowledgebooks.com/schema/aboutCompanyName>
"Wall Street Journal" .
<https://newsshop.com/june/z902.html>
<http://knowledgebooks.com/schema/containsCountryDbPediaLink>
<http://dbpedia.org/resource/Canada> .
The next listing shows the file src/sw/GenTriples.hs that finds entities like broadcast network
names, city names, company names, people’s names, political party names, and university names in
text and generates RDF triple data. If you need to add more entity types for your own applications,
then use the following steps:
• Look at the format of entity data for the NlpTool example and add names for the new entity
type you are adding.
• Add a utility function to find instances of the new entity type to NlpTools. For example, if
you are adding a new entity type “park names”, then copy the code for companyNames to
parkNames, modify as necessary, and export parkNames.
• In the following code, add new code for the new entity helper function after lines 10, 97, 151,
and 261. Use the code for companyNames as an example.
The map *category_to_uri_map** created in lines 36 to 84 maps a topic name to a linked Data URI
that describes the topic. For example, we would not refer to an information source as being about
the topic “economics”, but would instead refer to a linked data URI like http://knowledgebooks.
com/schema/topic/economics. The utility function uri_from_categor takes a text description of a
topic like “economy” and converts it to an appropriate URI using the map *category_to_uri_map**.
The utility function textToTriple takes a file path to a text input file and a path to meta file path,
calculates the text string representing the generated triples for the input text file, and returns the
result wrapped in an IO monad.
Knowledge Graph Creator
module GenTriples
( textToTriples
, category_to_uri_map
) where
import Categorize (bestCategories)
import Entities
( broadcastNetworkNames
, cityNames
, companyNames
, countryNames
, peopleNames
, politicalPartyNames
, tradeUnionNames
, universityNames
import FileUtils
( MyMeta
, filePathToString
, filePathToWordTokens
, readMetaFile
import Summarize (summarize, summarizeS)
import qualified Data.Map as M
import Data.Maybe (fromMaybe)
generate_triple :: [Char] -> [Char] -> [Char] -> [Char]
generate_triple s p o = s ++ "
" ++ p ++ "
" ++ o ++ " .\n"
make_literal :: [Char] -> [Char]
make_literal s = "\"" ++ s ++ "\""
category_to_uri_map :: M.Map [Char] [Char]
category_to_uri_map =
M.fromList
[ ("news_weather", "<http://knowledgebooks.com/schema/topic/weather>")
, ("news_war", "<http://knowledgebooks.com/schema/topic/war>")
, ("economics", "<http://knowledgebooks.com/schema/topic/economics>")
, ("news_economy", "<http://knowledgebooks.com/schema/topic/economics>")
, ("news_politics", "<http://knowledgebooks.com/schema/topic/politics>")
, ("religion", "<http://knowledgebooks.com/schema/topic/religion>")
Knowledge Graph Creator
, ( "religion_buddhism"
, "<http://knowledgebooks.com/schema/topic/religion/buddhism>")
, ( "religion_islam"
, "<http://knowledgebooks.com/schema/topic/religion/islam>")
, ( "religion_christianity"
, "<http://knowledgebooks.com/schema/topic/religion/christianity>")
, ( "religion_hinduism"
, "<http://knowledgebooks.com/schema/topic/religion/hinduism>")
, ( "religion_judaism"
, "<http://knowledgebooks.com/schema/topic/religion/judaism>")
, ("chemistry", "<http://knowledgebooks.com/schema/topic/chemistry>")
, ("computers", "<http://knowledgebooks.com/schema/topic/computers>")
, ("computers_ai", "<http://knowledgebooks.com/schema/topic/computers/ai>")
, ( "computers_ai_datamining"
, "<http://knowledgebooks.com/schema/topic/computers/ai/datamining>")
, ( "computers_ai_learning"
, "<http://knowledgebooks.com/schema/topic/computers/ai/learning>")
, ( "computers_ai_nlp"
, "<http://knowledgebooks.com/schema/topic/computers/ai/nlp>")
, ( "computers_ai_search"
, "<http://knowledgebooks.com/schema/topic/computers/ai/search>")
, ( "computers_ai_textmining"
, "<http://knowledgebooks.com/schema/topic/computers/ai/textmining>")
, ( "computers/programming"
, "<http://knowledgebooks.com/schema/topic/computers/programming>")
, ( "computers_microsoft"
, "<http://knowledgebooks.com/schema/topic/computers/microsoft>")
, ( "computers/programming/ruby"
, "<http://knowledgebooks.com/schema/topic/computers/programming/ruby>")
, ( "computers/programming/lisp"
, "<http://knowledgebooks.com/schema/topic/computers/programming/lisp>")
, ("health", "<http://knowledgebooks.com/schema/topic/health>")
, ( "health_exercise"
, "<http://knowledgebooks.com/schema/topic/health/exercise>")
, ( "health_nutrition"
, "<http://knowledgebooks.com/schema/topic/health/nutrition>")
, ("mathematics", "<http://knowledgebooks.com/schema/topic/mathematics>")
, ("news_music", "<http://knowledgebooks.com/schema/topic/music>")
, ("news_physics", "<http://knowledgebooks.com/schema/topic/physics>")
, ("news_sports", "<http://knowledgebooks.com/schema/topic/sports>")
uri_from_category :: [Char] -> [Char]
Knowledge Graph Creator
uri_from_category key =
fromMaybe ("\"" ++ key ++ "\"") $ M.lookup key category_to_uri_map
textToTriples :: FilePath -> [Char] -> IO [Char]
textToTriples file_path meta_file_path = do
word_tokens <- filePathToWordTokens file_path
contents <- filePathToString file_path
putStrLn $ "** contents:\n" ++ contents ++ "\n"
meta_data <- readMetaFile meta_file_path
let people = peopleNames word_tokens
let companies = companyNames word_tokens
let countries = countryNames word_tokens
let cities = cityNames word_tokens
let broadcast_networks = broadcastNetworkNames word_tokens
let political_parties = politicalPartyNames word_tokens
let trade_unions = tradeUnionNames word_tokens
let universities = universityNames word_tokens
let a_summary = summarizeS contents
let the_categories = bestCategories word_tokens
let filtered_categories =
map (uri_from_category . fst) $
filter (\(name, value) -> value > 0.3) the_categories
putStrLn "\nfiltered_categories:"
print filtered_categories
--putStrLn "a_summary:"
--print a_summary
--print $ summarize contents
let summary_triples =
generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/summaryOf>" $
"\"" ++ a_summary ++ "\""
let category_triples =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/news/category/>"
| cat <- filtered_categories
let people_triples1 =
concat
Knowledge Graph Creator
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsPersonDbPediaLink>"
(snd pair)
| pair <- people
let people_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutPersonName>"
(make_literal (fst pair))
| pair <- people
let company_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsCompanyDbPediaLink>"
(snd pair)
| pair <- companies
let company_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutCompanyName>"
(make_literal (fst pair))
| pair <- companies
let country_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsCountryDbPediaLink>"
(snd pair)
| pair <- countries
let country_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutCountryName>"
Knowledge Graph Creator
(make_literal (fst pair))
| pair <- countries
let city_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsCityDbPediaLink>"
(snd pair)
| pair <- cities
let city_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutCityName>"
(make_literal (fst pair))
| pair <- cities
let bnetworks_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsBroadCastDbPediaLink>"
(snd pair)
| pair <- broadcast_networks
let bnetworks_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutBroadCastName>"
(make_literal (fst pair))
| pair <- broadcast_networks
let pparties_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsPoliticalPartyDbPediaLink>"
(snd pair)
| pair <- political_parties
Knowledge Graph Creator
let pparties_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutPoliticalPartyName>"
(make_literal (fst pair))
| pair <- political_parties
let unions_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsTradeUnionDbPediaLink>"
(snd pair)
| pair <- trade_unions
let unions_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutTradeUnionName>"
(make_literal (fst pair))
| pair <- trade_unions
let universities_triples1 =
concat
[ generate_triple
(uri meta_data)
"<http://knowledgebooks.com/schema/containsUniversityDbPediaLink>"
(snd pair)
| pair <- universities
let universities_triples2 =
concat
[ generate_triple
(snd pair)
"<http://knowledgebooks.com/schema/aboutTradeUnionName>"
(make_literal (fst pair))
| pair <- universities
return $
concat
[ people_triples1
Knowledge Graph Creator
, people_triples2
, company_triples1
, company_triples2
, country_triples1
, country_triples2
, city_triples1
, city_triples2
, bnetworks_triples1
, bnetworks_triples2
, pparties_triples1
, pparties_triples2
, unions_triples1
, unions_triples2
, universities_triples1
, universities_triples2
, category_triples
, summary_triples
The code in this file could be shortened but having repetitive code for each entity type hopefully
makes it easier for you to understand how it works.
Utility Code for Generating Cypher Input Data for
Now we will generate Neo4J Cypher data. In order to keep the implementation simple, both the RDF
and Cypher generation code starts with raw text and performs the NLP analysis to find entities. This
example could be refactored to perform the NLP analysis just one time but in practice you will likely
be working with either RDF or NEO4J and so you will probably extract just the code you need from
this example (i.e., either the RDF or Cypher generation code).
Before we look at the code, let’s start with a few lines of generated Neo4J Cypher import data:
Knowledge Graph Creator
CREATE (newsshop_com_june_z902_html_news)-[:ContainsCompanyDbPediaLink]->(Wall_Stree\
t_Journal)
CREATE (Canada:Entity {name:"Canada", uri:"<http://dbpedia.org/resource/Canada>"})
CREATE (newsshop_com_june_z902_html_news)-[:ContainsCountryDbPediaLink]->(Canada)
CREATE (summary_of_abcnews_go_com_US_violent_long_lasting_tornadoes_threaten_oklahom\
a_texas_storyid63146361:Summary {name:"summary_of_abcnews_go_com_US_violent_long_las\
ting_tornadoes_threaten_oklahoma_texas_storyid63146361", uri:"<https://abcnews.go.co\
m/US/violent-long-lasting-tornadoes-threaten-oklahoma-texas/story?id=63146361>", sum\
mary:"Part of the system that delivered severe weather to the central U.S. over the \
weekend is moving into the Northeast today, producing strong to severe storms -- dam\
aging winds, hail or isolated tornadoes can't be ruled out. Severe weather is foreca\
st to continue on Tuesday, with the western storm moving east into the Midwest and p\
arts of the mid-Mississippi Valley."})
The following listing shows the file src/sw/GenNeo4jCypher.hs. This code is very similar to the
code for generating RDF in the last section. The same notes for adding your own new entity notes
in the last section are also relevant here.
Notice that we import in line 29 the map category_to_uri_map that was defined in the last section.
The function neo4j_category_node_defs defined in lines 35 to 43 creates category graph nodes
for each category in the map category_to_uri_map. These nodes will be referenced by graph
nodes created in the functions create_neo4j_node, create_neo4j_lin, create_summary_node,
and create_entity_node. The top level function is textToCypher that is similar to the function
textToTriples in the last section.
{-# LANGUAGE OverloadedStrings #-}
module GenNeo4jCypher
( textToCypher
, neo4j_category_node_defs
) where
import Categorize (bestCategories)
import Data.List (isInfixOf)
import Data.Char (toLower)
import Data.String.Utils (replace)
import Entities
( broadcastNetworkNames
, cityNames
, companyNames
, countryNames
, peopleNames
, politicalPartyNames
Knowledge Graph Creator
, tradeUnionNames
, universityNames
import FileUtils
( MyMeta
, filePathToString
, filePathToWordTokens
, readMetaFile
import GenTriples (category_to_uri_map)
import Summarize (summarize, summarizeS)
import qualified Data.Map as M
import Data.Maybe (fromMaybe)
import Database.SQLite.Simple
-- for debug:
import Data.Typeable (typeOf)
neo4j_category_node_defs :: [Char]
neo4j_category_node_defs =
replace
$ concat
[ "CREATE (" ++ c ++ ":CategoryType {name:\"" ++ c ++ "\"})\n"
| c <- M.keys category_to_uri_map
uri_from_category :: p -> p
uri_from_category s = s -- might want the full version from GenTriples
repl :: Char -> Char
repl '-' = '_'
repl '/' = '_'
repl '.' = '_'
repl c = c
filterChars :: [Char] -> [Char]
filterChars = filter (\c -> c /= '?' && c /= '=' && c /= '<' && c /= '>')
create_neo4j_node :: [Char] -> ([Char], [Char])
Knowledge Graph Creator
create_neo4j_node uri =
let name =
(map repl (filterChars
(replace "https://" "" (replace "http://" "" uri)))) ++
"_" ++
(map toLower node_type)
node_type =
if isInfixOf "dbpedia" uri
then "DbPedia"
else "News"
new_node =
"CREATE (" ++
name ++ ":" ++
node_type ++ " {name:\"" ++ (replace " " "_" name) ++
"\", uri:\"" ++ uri ++ "\"})\n"
in (name, new_node)
create_neo4j_link :: [Char] -> [Char] -> [Char] -> [Char]
create_neo4j_link node1 linkName node2 =
"CREATE (" ++ node1 ++ ")-[:" ++ linkName ++ "]->(" ++ node2 ++ ")\n"
create_summary_node :: [Char] -> [Char] -> [Char]
create_summary_node uri summary =
let name =
"summary_of_" ++
(map repl $
filterChars (replace "https://" "" (replace "http://" "" uri)))
s1 = "CREATE (" ++ name ++ ":Summary {name:\"" ++ name ++ "\", uri:\""
s2 = uri ++ "\", summary:\"" ++ summary ++ "\"})\n"
in s1 ++ s2
create_entity_node :: ([Char], [Char]) -> [Char]
create_entity_node entity_pair =
"CREATE (" ++ (replace " " "_" (fst entity_pair)) ++
":Entity {name:\"" ++ (fst entity_pair) ++ "\", uri:\"" ++
(snd entity_pair) ++ "\"})\n"
create_contains_entity :: [Char] -> [Char] -> ([Char], [Char]) -> [Char]
create_contains_entity relation_name source_uri entity_pair =
let new_person_node = create_entity_node entity_pair
new_link = create_neo4j_link source_uri
relation_name
(replace " " "_" (fst entity_pair))
Knowledge Graph Creator
(new_person_node ++ new_link)
entity_node_helper :: [Char] -> [Char] -> [([Char], [Char])] -> [Char]
entity_node_helper relation_name node_name entity_list =
concat [create_contains_entity
relation_name node_name entity | entity <- entity_list]
textToCypher :: FilePath -> [Char] -> IO [Char]
textToCypher file_path meta_file_path = do
let prelude_nodes = neo4j_category_node_defs
putStrLn "+++++++++++++++++ prelude node defs:"
print prelude_nodes
word_tokens <- filePathToWordTokens file_path
contents <- filePathToString file_path
putStrLn $ "** contents:\n" ++ contents ++ "\n"
meta_data <- readMetaFile meta_file_path
putStrLn "++ meta_data:"
print meta_data
let people = peopleNames word_tokens
let companies = companyNames word_tokens
putStrLn "^^^^ companies:"
print companies
let countries = countryNames word_tokens
let cities = cityNames word_tokens
let broadcast_networks = broadcastNetworkNames word_tokens
let political_parties = politicalPartyNames word_tokens
let trade_unions = tradeUnionNames word_tokens
let universities = universityNames word_tokens
let a_summary = summarizeS contents
let the_categories = bestCategories word_tokens
let filtered_categories =
map (uri_from_category . fst) $
filter (\(name, value) -> value > 0.3) the_categories
putStrLn "\nfiltered_categories:"
print filtered_categories
let (node1_name, node1) = create_neo4j_node (uri meta_data)
let summary1 = create_summary_node (uri meta_data) a_summary
let category1 =
concat
[ create_neo4j_link node1_name "Category" cat
| cat <- filtered_categories
Knowledge Graph Creator
let pp = entity_node_helper "ContainsPersonDbPediaLink" node1_name people
let cmpny = entity_node_helper "ContainsCompanyDbPediaLink" node1_name companies
let cntry = entity_node_helper "ContainsCountryDbPediaLink" node1_name countries
let citys = entity_node_helper "ContainsCityDbPediaLink" node1_name cities
let bnet = entity_node_helper "ContainsBroadcastNetworkDbPediaLink"
node1_name broadcast_networks
let ppart = entity_node_helper "ContainsPoliticalPartyDbPediaLink"
node1_name political_parties
let tunion = entity_node_helper "ContainsTradeUnionDbPediaLink"
node1_name trade_unions
let uni = entity_node_helper "ContainsUniversityDbPediaLink"
node1_name universities
return $ concat [node1, summary1, category1, pp, cmpny, cntry, citys, bnet,
ppart, tunion, uni]
Because the top level function is textToCypher returns a string wrapped in a monad, it is possible to
add “debug”” print statements in textToCypher. I left many such debug statements in the example
code to help you understand the data that is being operated on. I leave it as an exercise to remove
these print statements if you use this code in your own projects and no longer need to see the debug
output.
Top Level API Code for Handling Knowledge Graph
Data Generation
So far we have looked at processing command line arguments and processing individual input
files. Now we look at higher level utility APIs for processing an entire directory of input files. The
following listing shows the file API.hs that contains the two top level helper functions we saw in
app/Main.hs.
The functions processFilesToRdf and processFilesToNeo4j both have the function type signature
FilePath->FilePath->IO() and are very similar except for calling different helper functions to
generate RDF triples or Cypher input graph data:
Knowledge Graph Creator
module Apis
( processFilesToRdf
, processFilesToNeo4j
) where
import FileUtils
import GenNeo4jCypher
import GenTriples (textToTriples)
import qualified Database.SQLite.Simple as SQL
import Control.Monad (mapM)
import Data.String.Utils (replace)
import System.Directory (getDirectoryContents)
import Data.Typeable (typeOf)
processFilesToRdf :: FilePath -> FilePath -> IO ()
processFilesToRdf dirPath outputRdfFilePath = do
files <- getDirectoryContents dirPath :: IO [FilePath]
let filtered_files = filter isTextFile files
let full_paths = [dirPath ++ "/" ++ fn | fn <- filtered_files]
putStrLn "full_paths:"
print full_paths
let r =
[textToTriples fp1 (replace ".txt" ".meta" fp1)
fp1 <- full_paths] :: [IO [Char]]
tripleL <-
mapM (\fp -> textToTriples fp (replace ".txt" ".meta" fp)) full_paths
let tripleS = concat tripleL
putStrLn tripleS
writeFile outputRdfFilePath tripleS
processFilesToNeo4j :: FilePath -> FilePath -> IO ()
processFilesToNeo4j dirPath outputRdfFilePath = do
files <- getDirectoryContents dirPath :: IO [FilePath]
let filtered_files = filter isTextFile files
let full_paths = [dirPath ++ "/" ++ fn | fn <- filtered_files]
putStrLn "full_paths:"
print full_paths
let prelude_node_defs = neo4j_category_node_defs
putStrLn
Knowledge Graph Creator
("+++++
type of prelude_node_defs is: " ++
(show (typeOf prelude_node_defs)))
print prelude_node_defs
cypher_dataL <-
mapM (\fp -> textToCypher fp (replace ".txt" ".meta" fp)) full_paths
let cypher_dataS = concat cypher_dataL
putStrLn cypher_dataS
writeFile outputRdfFilePath $ prelude_node_defs ++ cypher_dataS
Since both of these functions return IO monads, I could add “debug” print statements that should be
helpful in understanding the data being operated on.
Wrapup for Automating the Creation of Knowledge
Graphs
The code in this chapter will provide you with a good start for creating both test knowledge graphs
and for generating data for production. In practice, generated data should be reviewed before use
and additional data manually generated as needed. It is good practice to document required manual
changes because this documentation can be used in the requirements for updating the code in this
chapter to more closely match your knowledge graph requirements.
Hybrid Haskell and Python Natural
Language Processing
Here we will write a Haskell client for using a Natural Language Processing (NLP) server written
in Python. There is some common material in this chapter and the next chapter Hybrid Haskell and
Python For Coreference Resolution because I wanted both chapters to be self contained.
Example Use of the Haskell NLP Client
Before learning how to use the Python NLP server code and understand the code for the Haskell
client code, let’s look at an example of running the client code so you understand the type of
processing that we are performing:
$ stack build --fast --exec HybridHaskellPythonNlp-exe
Enter text (all on one line)
John Smith went to Mexico to see the Pepsi plant
response from NLP server:
NlpResponse {entities = ["John Smith/PERSON","Mexico/GPE","Pepsi/ORG"],
tokens = ["John","Smith","went","to","Mexico","to","see","the","Pepsi",\
"plant"]}
Enter text (all on one line)
Notice on line 5 that each of the three entities is tagged with the entity type. GPE is the tag for a
country and the tag ORG can refer to an entity that is a company or a non-profit organization.
There is some overlap in functionality between the Python SpaCy NLP library and my pure Haskell
code in the NLP Tools chapter. SpaCy has the advantage of using state of the art deep learning
models.
Setting up the Python NLP Server
I assume that you have some familiarity with using Python. If not, you will still be able to follow these
directions assuming that you have the utilities pip, and python installed. I recommend installing
Python and Pip using Anaconda⁴³.
⁴³https://anaconda.org/anaconda/conda
Hybrid Haskell and Python Natural Language Processing
The server code is in the subdirectory HybridHaskellPythonNlp/python_spacy_nlp_server where
you will work when performing a one time initialization. After the server is installed you can then
run it from the command line from any directory on your laptop.
I recommend that you use virtual Python environments when using Python applications to separate
the dependencies required for each application or development project. Here I assume that you are
running in a Python version 3.6 (or higher) version environment. First install the dependencies:
pip install -U spacy
python -m spacy download en
pip install falcon
Then change directory to the subdirectory HybridHaskellPythonNlp/python_spacy_nlp_server
and install the NLP server:
cd HybridHaskellPythonNlp/python_spacy_nlp_server
python setup.py install
Once you install the server, you can run it from any directory on your laptop or server using:
spacynlpserver
I use deep learning models written in Python using TensorFlow or PyTorch in applications I write
in Haskell or Common Lisp. While it is possible to directly embed models in Haskell and Common
Lisp, I find it much easier and developer friendly to wrap deep learning models I use a REST services
as I have done here. Often deep learning models only require about a gigabyte of memory and
using pre-trained models has lightweight CPU resource needs so while I am developing on my
laptop I might have two or three models running and available as wrapped REST services. For
production, I configure both the Python services and my Haskell and Common Lisp applications
to start automatically on system startup.
This is not a Python programming book and I will not discuss the simple Python wrapping code but
if you are also a Python developer you can easily read and understand the code.
Understanding the Haskell NLP Client Code
The Python server returns JSON file. We saw earlier the use of the Haskell aeson library for parsing
JSON data stored as a string into Haskell native data. We also used the wreq library to access remote
web services. We use both of these libraries here:
Hybrid Haskell and Python Natural Language Processing
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE DeriveDataTypeable #-}
-- reference: http://www.serpentine.com/wreq/tutorial.html
module NlpWebClient
( nlpClient, NlpResponse
) where
import Control.Lens
import Data.ByteString.Lazy.Char8 (unpack)
import Data.Maybe (fromJust)
import Network.URI.Encode as E -- encode is also in Data.Aeson
import Network.Wreq
import Text.JSON.Generic
data NlpResponse = NlpResponse {entities::[String], tokens::[String]} deriving (Show\
, Data, Typeable)
base_url = "http://127.0.0.1:8008?text="
nlpClient :: [Char] -> IO NlpResponse
nlpClient query = do
putStrLn $ "\n\n***
Processing " ++ query
r <- get $ base_url ++ (E.encode query) ++ "&no_detail=1"
let ret = (decodeJSON (unpack (fromJust (r ^? responseBody)))) :: NlpResponse
return ret
The main command line program for using the client library:
module Main where
import NlpWebClient
main :: IO ()
main = do
putStrLn "Enter text (all on one line)"
s <- getLine
response <- (nlpClient s) :: IO NlpResponse
putStr "response from NLP server:\n"
putStrLn $ show response
Hybrid Haskell and Python Natural Language Processing
Wrapup for Using the Python SpaCy NLP Service
The example in this chapter shows a technique that I often use for using libraries and frameworks
that are not written in Haskell: wrap the service implemented in another programming language
is a REST web service. While it is possible to use a foreign function interface (FFI) to call out to
code written in other languages I find for my own work that I prefer calling out to a separate
service especially when I run other services on remote servers so I do not need to run them on
my development laptop. For production it is also useful to be able to easily scale horizontally across
servers.
Hybrid Haskell and Python For
Coreference Resolution
Here we will write a Haskell client for using a server written in Python that performs coreference
resolution (more on this later). There is some common material in this chapter and the last chapter
Hybrid Haskell and Python Natural Language Processing because I wanted both chapters to be self
contained. The code for this chapter can be found in the subdirectory HybridHaskellPythonCore-
fAnaphoraResolution.
Coreference resolution is also called anaphora resolution and is the process for replacing pronouns
in text with the original nouns, proper nouns, or noun phrases that the pronouns refer to.
Before discussing setting up the Python library for performing coreference analysis and the Haskell
client, let’s run the client so you can see and understand anaphora resolution:
$ stack build --fast --exec HybridHaskellPythonCorefAnaphoraResolution-exe
Enter text (all on one line)
John Smith drove a car. He liked it.
Processing John%20Smith%20drove%20a%20car.%20He%20liked%20it.
status code: 200
content type: Just "application/text"
response body: John Smith drove a car. John Smith liked a car.
response from coreference server:
"John Smith drove a car. John Smith liked a car."
Enter text (all on one line)
In this example notice that the words “He” and “it” in the second sentence are replaced by “John
Smith” and “a car” which makes it easier to write information extraction applications.
Installing the Python Coreference Server
I recommend that you use virtual Python environments when using Python applications to separate
the dependencies required for each application or development project. Here I assume that you
are running in a Python version 3.6 (or higher) version environment. If you want to install
the neuralcoref library using pip you must use and older version of spaCy. First install the
dependencies:
Hybrid Haskell and Python For Coreference Resolution
pip install spacy==2.1.0
pip install neuralcoref
pip install falcon
As I write this chapter the neuralcoref model and library require a slightly older version of spaCy
(the current latest version is 2.3.0).
If you want to instead use the latest version of spaCy then install neuralcoref from source:
pip install spacy
git clone https://github.com/huggingface/neuralcoref.git
cd neuralcoref
python setup.py install
pip install falcon
After installing all dependencies, then change directory to the subdirectory python_coreference_-
anaphora_resolution_server and install the coref server:
cd python_coreference_anaphora_resolution_server
python setup.py install
Once you install the server, you can run it from any directory on your laptop or server using:
corefserver
I use deep learning models written in Python using TensorFlow or PyTorch in applications I write
in Haskell or Common Lisp. While it is possible to directly embed models in Haskell and Common
Lisp, I find it much easier and developer friendly to wrap deep learning models I use a REST services
as I have done here. Often deep learning models only require about a gigabyte of memory and
using pre-trained models has lightweight CPU resource needs so while I am developing on my
laptop I might have two or three models running and available as wrapped REST services. For
production, I configure both the Python services and my Haskell and Common Lisp applications
to start automatically on system startup.
This is not a Python programming book and I will not discuss the simple Python wrapping code but
if you are also a Python developer you can easily read and understand the code.
Understanding the Haskell Coreference Client Code
The code for the library for fetching data from the Python service is in the subdirectory src in the
file CorefWebClient.hs.
We will use techniques for accessing remote web services using the wreq library and using the
lens library for accessing the response from the Python server. Here the response is plain text with
pronouns replaced by the nouns that they represent. We don’t use the aeson library to parse JSON
data as we did in the previous chapter.
Hybrid Haskell and Python For Coreference Resolution
{-# LANGUAGE OverloadedStrings #-}
-- reference: http://www.serpentine.com/wreq/tutorial.html
module CorefWebClient
( corefClient
) where
import Control.Lens
import Data.ByteString.Lazy.Char8 (unpack)
import Data.Maybe (fromJust)
import Network.URI.Encode (encode)
import Network.Wreq
base_url = "http://127.0.0.1:8000?text="
corefClient :: [Char] -> IO [Char]
corefClient query = do
putStrLn $ "\n\n***
Processing " ++ (encode query)
r <- get $ base_url ++ (encode query) ++ "&no_detail=1"
putStrLn $ "status code: " ++ (show (r ^. responseStatus . statusCode))
putStrLn $ "content type: " ++ (show (r ^? responseHeader "Content-Type"))
putStrLn $ "response body: " ++ (unpack (fromJust (r ^? responseBody)))
return $ unpack (fromJust (r ^? responseBody))
The code for the main application is in the subdirectory app in the file Main.hs.
module Main where
import CorefWebClient
main :: IO ()
main = do
putStrLn "Enter text (all on one line)"
s <- getLine
response <- corefClient s
putStr "response from coreference server:\t"
putStrLn $ show response
Hybrid Haskell and Python For Coreference Resolution
Wrapup for Using the Python Coreference NLP Service
The example in this chapter is fairly simple but shows a technique that I often use for using
libraries and frameworks that are not written in Haskell: wrap the service implemented in another
programming language is a REST web service. While it is possible to use a foreign function interface
(FFI) to call out to code written in other languages I find for my own work that I prefer calling out
to a separate service, especially when I run other services on remote servers so I do not need to run
them on my development laptop. For production it is also useful to be able to easily scale horizontally
across servers.
Book Wrap Up
As I mentioned in the Preface, I had a slow start learning Haskell because I tried to learn too much
at one time. In this book I have attempted to show you a subset of Haskell that is sufficient to write
interesting programs - a gentle introduction.
Haskell beginners often dislike the large error listings from the compiler. The correct attitude is to
recognize that these error messages are there to help you. That is easier said than done, but try to be
happy when the compiler points out an error - in the long run I find using Haskell’s fussy compiler
saves me time and lets me refactor code knowing that if I miss something in my refactoring the
compiler will immediately let me know what needs to be fixed.
The other thing that I hope you learned working through this book is how effective repl based
programming is. Most code I write, unless it is very trivial, starts its life in a GHCi repl. When you
are working with somene else’s Haskell code it is similarly useful to have their code loaded in a repl
as you read.
I have been programming professionally for forty years and I use many programming languages.
Once I worked my way through early difficulties using Haskell it has become a favorite program-
ming language. I hope that you enjoy Haskell development as much as I do.
Appendix A - Haskell Tools Setup
I recommend that if you are new to Haskell that you at least do a minimal installation of stack and
work through the first chapter using an interactive REPL. After experimenting with the REPL then
do please come back to Appendix A and install support for the editor of your choice (or an IDE) and
hlint.
I assume that you have the Haskell package manager stack installed. If you have not installed stack
yet please follow these directions⁴⁴.
After installing stack and running it you will have a directory “.stack” in your home directory
where stack will keep compiled libraries and configuration data. You will want to create a file
“∼/.stack/config.yaml” with contents similar to my stack configuration file:
templates:
params:
author-email: markw@markwatson.com
author-name: Mark Watson
category: dev
copyright: Copyright 2016 Mark Watson. All rights reserved
github-username: mark-watson
Replace my name and email address with yours. You might also want to install the package manager
Cabal and the “lint” program hlint:
$ stack install cabal-install
$ stack install hlint
These installs might take a while so go outside for ten minutes and get some fresh air.
You should get in the habit of running hlint on your code and consider trying to remove all or at
least most warnings. You can customize the types of warnings hlint shows: read the documentation
for hlint⁴⁵.
⁴⁴http://docs.haskellstack.org/en/stable/README.html
⁴⁵https://github.com/ndmitchell/hlint#readme
Appendix A - Haskell Tools Setup
Creating a New Stack Project
I have already created stack projects for the examples in this book. When you have worked through
them, then please refer to the stack documentation for creating projects⁴⁶.
Emacs Setup
There are several good alternatives to using the Emacs editor:
• GEdit on Linux
• TextMate on OS X
• IntelliJ with the Haskell plugin (all platforms)
I use all three of these alternatives on occasion, but Emacs with haskell-mode is my favorite
environment. There are instructions for adding haskell-mode to Emacs on the project home page on
github⁴⁷. If you follow these instructions you will have syntax hiliting and Emacs will understand
Haskell indentation rules.
Do you want more of an IDE-like Development
Environment?
I recommend and use the Intero Emacs package⁴⁸ to get auto completions and real time syntax error
warnings. Intero is designed to work with stack.
I add the following to the bottom of my .emacs file:
(add-hook ‘haskell-mode-hook ‘intero-mode)
and if Intero is too “heavy weight” for my current project, then I comment out the add-hook
expression. Intero can increase the startup time for Emacs for editing Haskell files. That said, I
almost always keep Intero enabled in my Emacs environment.
hlint is a wonderful tool for refining your knowledge and use of the Haskell language. After writing
new code and checking that it works, then run hlint for suggestions on how to improve your code.
Install hlint using:
⁴⁶https://docs.haskellstack.org/en/stable/README/#start-your-new-project
⁴⁷https://github.com/haskell/haskell-mode
⁴⁸https://commercialhaskell.github.io/intero/
Appendix A - Haskell Tools Setup
stack install hlint
Practical Artificial Intelligence Programming
With Java
Fifth Edition (July 2020)
Mark Watson
This book is for sale at http://leanpub.com/javaai
This version was published on 2023-02-04
This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing
process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and
many iterations to get reader feedback, pivot until you have the right book and build traction once
you do.
© 2013 - 2023 Mark Watson
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Personal Artificial Intelligence Journey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Maven Setup for Combining Examples in this Book . . . . . . . . . . . . . . . . . . . . . . . .
Software Licenses for Example Programs in this Book . . . . . . . . . . . . . . . . . . . . . .
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Representation of Search State Space and Search Operators . . . . . . . . . . . . . . . . . . .
Finding Paths in Mazes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Finding Paths in Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Adding Heuristics to Breadth-first Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Heuristic Search and Game Playing: Tic-Tac-Toe and Chess . . . . . . . . . . . . . . . . . . .
Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
PowerLoom Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Running PowerLoom Interactively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using the PowerLoom APIs in Java Programs . . . . . . . . . . . . . . . . . . . . . . . . . . .
Suggestions for Further Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Anomaly Detection Machine Learning Example . . . . . . . . . . . . . . . . . . . . . . . . . . .
Motivation for Anomaly Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Math Primer for Anomaly Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
AnomalyDetection Utility Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example Using the University of Wisconsin Cancer Data . . . . . . . . . . . . . . . . . . . .
Genetic Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Java Library for Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Finding the Maximum Value of a Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Road Map for the Neural Network Example Code . . . . . . . . . . . . . . . . . . . . . . . . .
Backpropagation Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Java Class Library for Back Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CONTENTS
Adding Momentum to Speed Up Back-Prop Training . . . . . . . . . . . . . . . . . . . . . . .
Wrap-up for Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Deep Learning Using Deeplearning4j . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Feed Forward Classification Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Feed Forward Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Configuring the Example Using Maven . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Documentation for Other Types of Deep Learning Layers . . . . . . . . . . . . . . . . . . . . 104
Running the DL4J Example Programs and Modifying Them For Your Use . . . . . . . . . . 106
Modifying the Character Generating LSTM Example to Model and Generate CSV
Spreadsheet Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
Roadmap for the DL4J Model Zoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
Deep Learning Wrapup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Overview of the NLP Library and Running the Examples . . . . . . . . . . . . . . . . . . . . 113
Tokenizing, Stemming, and Part of Speech Tagging Text . . . . . . . . . . . . . . . . . . . . . 115
Named Entity Extraction From Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
Automatically Assigning Categories to Text
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
Text Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Wrapup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Natural Language Processing Using OpenNLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
Using OpenNLP Pre-Trained Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
Training a New Categorization Model for OpenNLP . . . . . . . . . . . . . . . . . . . . . . . 134
Using Our New Trained Classification Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
Using the OpenNLP Parsing Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
Combining the WordNet Linguistic Database With OpenNLP . . . . . . . . . . . . . . . . . . 145
Using the WordNet Linguistic Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
Installing the Libraries and Linguistic Data for this Example . . . . . . . . . . . . . . . . . . 148
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Other Type Relationships Supported by WordNet . . . . . . . . . . . . . . . . . . . . . . . . . 154
Wrap-up and Ideas for Using WordNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
Information Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
Web Scraping Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
DBPedia Entity Lookup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
Client for GeoNames Service . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
Wrap-up for Information Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
Resolve Entity Names to DBPedia References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
DBPedia Entities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
CONTENTS
Wrap-up for Resolving Entity Names to DBPedia References . . . . . . . . . . . . . . . . . . 180
Semantic Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
Available Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
Relational Database Model Has Problems Dealing with Rapidly Changing Data Require-
ments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
RDF: The Universal Data Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
Extending RDF with RDF Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
The SPARQL Query Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
Using Jena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
OWL: The Web Ontology Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
Semantic Web Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
Automatically Generating Data for Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . 210
Implementation Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
Generating RDF Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
KGCreator Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
Knowledge Graph Navigator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
Entity Types Handled by KGN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
General Design of KGN with Example Output . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
UML Class Diagram for Example Application . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
Preface
The latest edition of this book is always available at https://leanpub.com/javaai¹. You can also
download a free copy from my website². Currently the latest edition was released in the summer of
2020. It had been seven years since the previous edition and this is largely a rewrite, dropping some
material like Drools based expert systems, Weka for machine learning, and the implementation of
an RDF server with geolocation support. I am now placing a heavier emphasis on neural networks
and deep learning, a greatly expanded discussion of the semantic web and linked data including
examples to generate knowledge graphs automatically from text documents and also a system to
help navigate public Knowledge Graphs like DBPedia and WikiData.
The code and PDF for the 4th edition from 2013 can be found here³.
I decided which material to keep from old editions and which new material to add based on what
my estimation is of which AI technologies are most useful and interesting to Java developers.
I have been developing commercial Artificial Intelligence (AI) tools and applications since the 1980s.
Mark Watson
I wrote this book for both professional programmers and home hobbyists who already know how
to program in Java and who want to learn practical AI programming and information processing
techniques. I have tried to make this an enjoyable book to work through. In the style of a “cook
book,” the chapters can be studied in any order. When an example depends on a library developed
in a previous chapter this is stated clearly. Most chapters follow the same pattern: a motivation
¹https://leanpub.com/javaai
²https://markwatson.com/books
³https://github.com/mark-watson/Java-AI-Book-Code_4th_edition
Preface
for learning a technique, some theory for the technique, and a Java example program that you can
experiment with.
The code for the example programs is available on github:
https://github.com/mark-watson/Java-AI-Book-Code⁴
My Java code in this book can be used under either or both the LGPL3 and Apache 2 licenses - choose
whichever of these two licenses that works best for you. Git pull requests with code improvements
will be appreciated by me and the readers of this book.
My goal is to introduce you to common AI techniques and to provide you with Java source code
to save you some time and effort. Even though I have worked almost exclusively in the field of
deep learning in the last six years, I urge you, dear reader, to look at the field of AI as being far
broader than machine learning and deep learning in particular. Just as it is wrong to consider the
higher level fields of Category Theory or Group Theory to “be” mathematics, there is far more to AI
than machine learning. Here we will take a more balanced view of AI, and indeed, my own current
research is in hybrid AI, that is, the fusion of deep learning with good old fashioned symbolic AI,
probabilistic reasoning, and explainability.
This book is released with a Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-
NC-ND 4.0)⁵ license. Feel free to share copies of this book with friends and colleagues at work. This
book is also available to read free online or to purchase⁶ if you want to support my writing activities.
Personal Artificial Intelligence Journey
I have been interested in AI since reading Bertram Raphael’s excellent book Thinking Computer:
Mind Inside Matter in the early 1980s. I have also had the good fortune to work on many interesting
AI projects including the development of commercial expert system tools for the Xerox LISP
machines and the Apple Macintosh, development of commercial neural network tools, application
of natural language and expert systems technology, medical information systems, application of AI
technologies to Nintendo and PC video games, and the application of AI technologies to the financial
markets. I have also applied statistical natural language processing techniques to analyzing social
media data from Twitter and Facebook. I worked at Google on their Knowledge Graph and I managed
a deep learning team at Capital One.
I enjoy AI programming, and hopefully this enthusiasm will also infect you, the reader.
Maven Setup for Combining Examples in this Book
The chapter on WordNet uses the examples from the previous chapter on OpenNLP. Both chapters
discuss the use of maven to support this code and data sharing.
⁴https://github.com/mark-watson/Java-AI-Book-Code
⁵https://creativecommons.org/licenses/by-nc-nd/4.0/
⁶https://leanpub.com/javaai
Preface
Additionally, the chapter Statistical Natural Language Processing is configured so the code and
linguistic data can be combined with other examples.
Code sharing is achieved by installing the code in your local maven repository, for example:
cd Java-AI-Book-Code/opennlp
mvn install
Now, the code in the OpenNLP example is installed on your system.
Software Licenses for Example Programs in this Book
My example programs (i.e., the code I wrote) are licensed under the LGPL version 3 and the Apache
2. Use whichever of these two licenses that works better for you. I also use several open source
libraries in the book examples and their licenses are:
• PowerLoom Reasoning: LGPL
• Jena Semantic Web: Apache 2
• OpenNlp: Apache 2
• WordNet: MIT style license (link to license⁷)
• Deep Learning for Java (DL4J): Apache 2
My desire is for you to be able to use my code examples and data in your projects with no hassles.
Acknowledgements
I process the manuscript for this book using the leanpub.com⁸ publishing system and I recommend
leanpub.com to other authors. Write one manuscript and use leanpub.com to generate assets for PDF,
iPad/iPhone, and Kindle versions. It is also simple to push new book updates to readers.
I would like to thank Kevin Knight for writing a flexible framework for game search algorithms in
Common LISP (Rich, Knight 1991) and for giving me permission to reuse his framework, rewritten
in Java for some of the examples in the Chapter on Search. I would like to thank my friend Tom
Munnecke for my photo in this Preface. I have a library full of books on AI and I would like to thank
the authors of all of these books for their influence on my professional life. I frequently reference
books in the text that have been especially useful to me and that I recommend to my readers.
In particular, I would like to thank the authors of the following two books that have probably had
the most influence on me:
⁷https://wordnet.princeton.edu/license-and-commercial-use
⁸http://leanpub.com
Preface
• Stuart Russell and Peter Norvig’s Artificial Intelligence: A Modern Approach which I
consider to be the best single reference book for AI theory
• John Sowa’s book Knowledge Representation is a resource that I turn to for a holistic
treatment of logic, philosophy, and knowledge representation in general
Book Editor: Carol Watson
Thanks to the following people who found typos in this and earlier book editions: Carol Watson,
James Fysh, Joshua Cranmer, Jack Marsh, Jeremy Burt, Jean-Marc Vanel
Search
Unless you write the AI for game programs and entertainment systems (which I have done for Angel
Studios, Nintendo, and Disney), the material in the chapter may not be relevant to your work. That
said I recommend that you develop some knowledge of defining search spaces for problems and
techniques to search these spaces. I hope that you have fun with the material in this chapter.
Early AI research emphasized the optimization of search algorithms. At this time in the 1950s and
1960s this approach made sense because many AI tasks can be solved effectively by defining state
spaces and using search algorithms to define and explore search trees in this state space. This
approach for AI research encountered some early success in game playing systems like checkers
and chess which reinforced confidence in viewing many AI problems as search problems.
I now consider this form of classic search to be a well understood problem but that does not mean
that we will not see exciting improvements in search algorithms in the future. This book does not
cover Monte Carlo Search or game search using Reinforcement Learning with Monte Carlo Search
that Alpha Go uses.
We will cover depth-first and breadth-first search. The basic implementation for depth-first and
breadth-first search is the same with one key difference. When searching from any location in state
space we start by calculating nearby locations that can be moved to in one search cycle. For depth-
first search we store new locations to be searched in a stack data structure and for breadth-first
search we store new locations to search in a queue data structure. As we will shortly see this simple
change has a large impact on search quality (usually breadth-first search will produce better results)
and computational resources (depth-first search requires less storage).
It is customary to cover search in AI books but to be honest I have only used search techniques in one
interactive planning system in the 1980s and much later while doing the “game AI” in two Nintendo
games, a PC hovercraft racing game and a VR system for Disney. Still, you should understand how
to optimize search.
What are the limitations of search? Early on, success in applying search to problems like checkers and
chess misled early researchers into underestimating the extreme difficulty of writing software that
performs tasks in domains that require general world knowledge or deal with complex and changing
environments. These types of problems usually require the understanding and the implementation
of domain specific knowledge.
In this chapter, we will use three search problem domains for studying search algorithms: path
finding in a maze, path finding in a graph, and alpha-beta search in the games tic-tac-toe and chess.
If you want to try the examples before we proceed to the implementation then you can do that right
now using the Makefile in the search directory:
Search
chess:
mvn install
mvn exec:java -Dexec.mainClass="search.game.Chess"
graph:
mvn install
mvn exec:java -Dexec.mainClass="search.graph.GraphDepthFirstSearch"
mvn install
mvn exec:java -Dexec.mainClass="search.maze.MazeBreadthFirstSearch"
You can run the examples using:
make maze
make graph
make chess
Representation of Search State Space and Search
Operators
We will use a single search tree representation in graph search and maze search examples in this
chapter. Search trees consist of nodes that define locations in state space and links to other nodes.
For some small problems, the search tree can be pre-computed and cover all of the search space.
For most problems however it is impossible to completely enumerate a search tree for a state space
so we must define successor node search operators that for a given node produce all nodes that
can be reached from the current node in one step. For example, in the game of chess we can not
possibly enumerate the search tree for all possible games of chess, so we define a successor node
search operator that given a board position (represented by a node in the search tree) calculates all
possible moves for either the white or black pieces. The possible Chess moves are calculated by a
successor node search operator and are represented by newly calculated nodes that are linked to the
previous node. Note that even when it is simple to fully enumerate a search tree, as in the small
maze example, we still want to use the general implementation strategy of generating the search
tree dynamically as we will do in this chapter.
For calculating a search tree we use a graph. We will represent graphs as nodes with links between
some of the nodes. For solving puzzles and for game related search, we will represent positions in
the search space with Java objects called nodes. Nodes contain arrays of references to child nodes
and for some applications we also might store links back to parent nodes. A search space using this
node representation can be viewed as a directed graph or a tree. The node that has no parent nodes
is the root node and all nodes that have no child nodes a called leaf nodes.
Search
Search operators are used to move from one point in the search space to another. We deal with
quantized search spaces in this chapter, but search spaces can also be continuous in some applications
(e.g., a robot’s position while moving in the real world). In general search spaces are either very large
or are infinite. We implicitly define a search space using some algorithm for extending the space from
our reference position in the space. The figure Search Space Representations shows representations
of search space as both connected nodes in a graph and as a two-dimensional grid with arrows
indicating possible movement from a reference point denoted by R.
Search Space Representations
When we specify a search space as a two-dimensional array, search operators will move the point
of reference in the search space from a specific grid location to an adjoining grid location. For some
applications, search operators are limited to moving up/down/left/right and in other applications
operators can additionally move the reference location diagonally.
When we specify a search space using node representation, search operators can move the reference
point down to any child node or up to the parent node. For search spaces that are represented
implicitly, search operators are also responsible for determining legal child nodes, if any, from the
reference point.
Note that I created different libraries for the maze and graph search examples.
Finding Paths in Mazes
The example program used in this section is MazeSearch.java in the directory search/src/main/-
java/search/maze and I assume that you have cloned the GitHub repository for this book⁹. The
figure UML Diagram for Search Classes shows an overview of the maze search strategies: depth-first
and breadth-first search. The abstract base class AbstractSearchEngine contains common code and
data that is required by both the classes DepthFirstSearch and BreadthFirstSearch. The class Maze
is used to record the data for a two-dimensional maze, including which grid locations contain walls
⁹https://github.com/mark-watson/Java-AI-Book-Code
Search
or obstacles. The class Maze defines three static short integer values used to indicate obstacles, the
starting location, and the ending location.
UML Diagram for Search Classes
The Java class Maze defines the search space. This class allocates a two-dimensional array of short
integers to represent the state of any grid location in the maze. Whenever we need to store a pair
of integers, we will use an instance of the standard Java class java.awt.Dimension, which has two
integer data components: width and height. Whenever we need to store an x-y grid location, we
create a new Dimension object (if required), and store the x coordinate in Dimension.width and
the y coordinate in Dimension.height. As in the right-hand side of figure Search Space, the operator
for moving through the search space from given x-y coordinates allows a transition to any adjacent
grid location that is empty. The Maze class also contains the x-y location for the starting location
(startLoc) and goal location (goalLoc). Note that for these examples, the class Maze sets the starting
location to grid coordinates 0-0 (upper left corner of the maze in the figures to follow) and the goal
node in (width - 1) - (height - 1) (lower right corner in the following figures).
The abstract class AbstractSearchEngine is the base class for both the depth-first (uses a stack
to store moves) search class DepthFirstSearchEngine and the breadth-first (uses a queue to store
Search
moves) search class BreadthFirstSearchEngine. We will start by looking at the common data and
behavior defined in AbstractSearchEngine. The class constructor has two required arguments: the
width and height of the maze, measured in grid cells. The constructor defines an instance of the Maze
class of the desired size and then calls the utility method initSearch to allocate an array searchPath of
Dimension objects, which will be used to record the path traversed through the maze. The abstract
base class also defines other utility methods:
• equals(Dimension d1, Dimension d2) – checks to see if two arguments of type Dimension
are the same.
• getPossibleMoves(Dimension location) – returns an array of Dimension objects that can be
moved to from the specified location. This implements the movement operator.
Now, we will look at the depth-first search procedure. The constructor for the derived class
DepthFirstSearchEngine calls the base class constructor and then solves the search problem by
calling the method iterateSearch. We will look at this method in some detail. The arguments to
iterateSearch specify the current location and the current search depth:
private void iterateSearch(Dimension loc, int depth) {
The class variable isSearching is used to halt search, avoiding more solutions, once one path to the
goal is found.
if (isSearching == false) return;
We set the maze value to the depth for display purposes only:
maze.setValue(loc.width, loc.height, (short)depth);
Here, we use the super class getPossibleMoves method to get an array of possible neighboring
squares that we could move to; we then loop over the four possible moves (a null value in the array
indicates an illegal move):
Dimension [] moves = getPossibleMoves(loc);
for (int i=0; i<4; i++) {
if (moves[i] == null) break; // out of possible moves
// from this location
Record the next move in the search path array and check to see if we are done:
Search
searchPath[depth] = moves[i];
if (equals(moves[i], goalLoc)) {
System.out.println("Found the goal at " +
moves[i].width +
``, " + moves[i].height);
isSearching = false;
maxDepth = depth;
return;
} else {
If the next possible move is not the goal move, we recursively call the iterateSearch method again,
but starting from this new location and increasing the depth counter by one:
iterateSearch(moves[i], depth + 1);
if (isSearching == false) return;
The figure showing the depth-first search in a maze shows how poor a path a depth-first search
can find between the start and goal locations in the maze. The maze is a 10-by-10 grid. The letter S
marks the starting location in the upper left corner and the goal position is marked with a G in the
lower right corner of the grid. Blocked grid cells are painted light gray. The basic problem with the
depth-first search is that the search engine will often start searching in a bad direction, but still find
a path eventually, even given a poor start. The advantage of a depth-first search over a breadth-first
search is that the depth-first search requires much less memory. We will see that possible moves
for depth-first search are stored on a stack (last in, first out data structure) and possible moves for a
breadth-first search are stored in a queue (first in, first out data structure).
Depth-first search of a maze
The derived class BreadthFirstSearch is similar to the DepthFirstSearch procedure with one major
difference: from a specified search location we calculate all possible moves, and make one possible
Search
trial move at a time. We use a queue data structure for storing possible moves, placing possible
moves on the back of the queue as they are calculated, and pulling test moves from the front of the
queue. The effect of a breadth-first search is that it “fans out” uniformly from the starting node until
the goal node is found.
The class constructor for BreadthFirstSearch calls the super class constructor to initialize the maze,
and then uses the auxiliary method doSearchOn2Dgrid for performing a breadth-first search for the
goal. We will look at the class BreadthFirstSearch in some detail. Breadth first search uses a queue
instead of a stack (depth-first search) to store possible moves. The utility class DimensionQueue
implements a standard queue data structure that handles instances of the class Dimension.
The method doSearchOn2Dgrid is not recursive, it uses a loop to add new search positions to the
end of an instance of class DimensionQueue and to remove and test new locations from the front of
the queue. The two-dimensional array allReadyVisited keeps us from searching the same location
twice. To calculate the shortest path after the goal is found, we use the predecessor array:
private void doSearchOn2DGrid() {
int width = maze.getWidth();
int height = maze.getHeight();
boolean alReadyVisitedFlag[][] =
new boolean[width][height];
Dimension predecessor[][] =
new Dimension[width][height];
DimensionQueue queue =
new DimensionQueue();
for (int i=0; i<width; i++) {
for (int j=0; j<height; j++) {
alReadyVisitedFlag[i][j] = false;
predecessor[i][j] = null;
We start the search by setting the already visited flag for the starting location to true value and
adding the starting location to the back of the queue:
alReadyVisitedFlag[startLoc.width][startLoc.height]
= true;
queue.addToBackOfQueue(startLoc);
boolean success = false;
This outer loop runs until either the queue is empty or the goal is found:
Search
outer:
while (queue.isEmpty() == false) {
We peek at the Dimension object at the front of the queue (but do not remove it) and get the adjacent
locations to the current position in the maze:
Dimension head = queue.peekAtFrontOfQueue();
Dimension [] connected =
getPossibleMoves(head);
We loop over each possible move; if the possible move is valid (i.e., not null) and if we have not
already visited the possible move location, then we add the possible move to the back of the queue
and set the predecessor array for the new location to the last square visited (head is the value from
the front of the queue). If we find the goal, break out of the loop:
for (int i=0; i<4; i++) {
if (connected[i] == null) break;
int w = connected[i].width;
int h = connected[i].height;
if (alReadyVisitedFlag[w][h] == false) {
alReadyVisitedFlag[w][h] = true;
predecessor[w][h] = head;
queue.addToBackOfQueue(connected[i]);
if (equals(connected[i], goalLoc)) {
success = true;
break outer; // we are done
We have processed the location at the front of the queue (in the variable head), so remove it:
queue.removeFromFrontOfQueue();
Now that we are out of the main loop, we need to use the predecessor array to get the shortest path.
Note that we fill in the searchPath array in reverse order, starting with the goal location:
Search
maxDepth = 0;
if (success) {
searchPath[maxDepth++] = goalLoc;
for (int i=0; i<100; i++) {
searchPath[maxDepth] =
predecessor[searchPath[maxDepth - 1].
width][searchPath[maxDepth - 1].
height];
maxDepth++;
if (equals(searchPath[maxDepth - 1],
startLoc))
break;
// back to starting node
The figure of breadth search of a maze shows a good path solution between starting and goal nodes.
Starting from the initial position, the breadth-first search engine adds all possible moves to the back
of a queue data structure. For each possible move added to this queue in one search cycle, all possible
moves are added to the queue for each new move recorded. Visually, think of possible moves added
to the queue as “fanning out” like a wave from the starting location. The breadth-first search engine
stops when this “wave” reaches the goal location. In general, I prefer breadth-first search techniques
to depth-first search techniques when memory storage for the queue used in the search process is
not an issue. In general, the memory requirements for performing depth-first search is much less
than breadth-first search.
Beadth-first Search of a Maze
Note that the classes MazeDepthFirstSearch and MazeBreadthFirstSearch are simple Java JFC
applications that produced the figure showing the depth-first search in a maze and the figure of
breadth search of a maze. The interested reader can read through the source code for the GUI test
Search
programs, but we will only cover the core AI code in this book. If you are interested in the GUI
test programs and you are not familiar with the Java JFC (or Swing) classes, there are several good
tutorials on JFC programming on the web.
Finding Paths in Graphs
In the last section, we used both depth-first and breadth-first search techniques to find a path
between a starting location and a goal location in a maze. Another common type of search space is
represented by a graph. A graph is a set of nodes and links. We characterize nodes as containing the
following data:
• A name and/or other data
• Zero or more links to other nodes
• A position in space (this is optional, usually for display or visualization purposes)
Links between nodes are often called edges. The algorithms used for finding paths in graphs are
very similar to finding paths in a two-dimensional maze. The primary difference is the operators
that allow us to move from one node to another. In the last section we saw that in a maze, an agent
can move from one grid space to another if the target space is empty. For graph search, a movement
operator allows movement to another node if there is a link to the target node.
The figure showing UML Diagram for Search Classes shows the UML class diagram for the graph
search Java classes that we will use in this section. The abstract class AbstractGraphSearch class is
the base class for both DepthFirstSearch and BreadthFirstSearch. The classes GraphDepthFirst-
Search and GraphBreadthFirstSearch and test programs also provide a Java Foundation Class (JFC)
or Swing based user interface. These two test programs produced figures Search Depth-First and
Search Breadth-First.
Search
UML Diagram for Graphics Search Demo
As seen in the previous figure, most of the data for the search operations (i.e., nodes, links, etc.)
is defined in the abstract class AbstractGraphSearch. This abstract class is customized through
inheritance to use a stack for storing possible moves (i.e., the array path) for depth-first search and
a queue for breadth-first search.
The abstract class AbstractGraphSearch allocates data required by both derived classes:
Search
final public static int MAX = 50;
protected int [] path =
new int[AbstractGraphSearch.MAX];
protected int num_path = 0;
// for nodes:
protected String [] nodeNames =
new String[MAX];
protected int [] node_x = new int[MAX];
protected int [] node_y = new int[MAX];
// for links between nodes:
protected int [] link_1 = new int[MAX];
protected int [] link_2 = new int[MAX];
protected int [] lengths = new int[MAX];
protected int numNodes = 0;
protected int numLinks = 0;
protected int goalNodeIndex = -1,
startNodeIndex = -1;
The abstract base class also provides several common utility methods:
• addNode(String name, int x, int y) – adds a new node
• addLink(int n1, int n2) – adds a bidirectional link between nodes indexed by n1 and n2. Node
indexes start at zero and are in the order of calling addNode.
• addLink(String n1, String n2) – adds a bidirectional link between nodes specified by their names
• getNumNodes() – returns the number of nodes
• getNumLinks() – returns the number of links
• getNodeName(int index) – returns a node’s name
• getNodeX(), getNodeY() – return the coordinates of a node
• getNodeIndex(String name) – gets the index of a node, given its name
The abstract base class defines an abstract method findPath that must be overridden. We will start
with the derived class DepthFirstSearch, looking at its implementation of findPath. The findPath
method returns an array of node indices indicating the calculated path:
public int [] findPath(int start_node,
int goal_node) {
The class variable path is an array that is used for temporary storage; we set the first element to the
starting node index, and call the utility method findPathHelper:
Search
path[0] = start_node; // the starting node
return findPathHelper(path, 1, goal_node);
The method findPathHelper is the interesting method in this class that actually performs the depth-
first search; we will look at it in some detail:
The path array is used as a stack to keep track of which nodes are being visited during the search.
The argument num_path is the number of locations in the path, which is also the search depth:
public int [] findPathHelper(int [] path,
int num_path,
int goal_node) {
First, re-check to see if we have reached the goal node; if we have, make a new array of the current
size and copy the path into it. This new array is returned as the value of the method:
if (goal_node == path[num_path - 1]) {
int [] ret = new int[num_path];
for (int i=0; i<num_path; i++) {
ret[i] = path[i];
return ret;
// we are done!
We have not found the goal node, so call the method connected_nodes to find all nodes connected to
the current node that are not already on the search path (see the source code for the implementation
of connected_nodes):
int [] new_nodes = connected_nodes(path,
num_path);
If there are still connected nodes to search, add the next possible “node to visit” to the top of the
stack (variable path in the program) and recursively call the method findPathHelper again:
Search
if (new_nodes != null) {
for (int j=0; j<new_nodes.length; j++) {
path[num_path] = new_nodes[j];
int [] test = findPathHelper(new_path,
num_path + 1,
goal_node);
if (test != null) {
if (test[test.length-1] == goal_node) {
return test;
If we have not found the goal node, return null, instead of an array of node indices:
return null;
Derived class BreadthFirstSearch also must define abstract method findPath. This method is very
similar to the breadth-first search method used for finding a path in a maze: a queue is used to store
possible moves. For a maze, we used a queue class that stored instances of the class Dimension, so
for this problem, the queue only needs to store integer node indices. The return value of findPath
is an array of node indices that make up the path from the starting node to the goal.
public int [] findPath(int start_node,
int goal_node) {
We start by setting up a flag array alreadyVisited to prevent visiting the same node twice, and
allocating a predecessors array that we will use to find the shortest path once the goal is reached:
// data structures for depth-first search:
boolean [] alreadyVisitedFlag =
new boolean[numNodes];
int [] predecessor = new int[numNodes];
The class IntQueue is a private class defined in the file BreadthFirstSearch.java; it implements a
standard queue:
IntQueue queue = new IntQueue(numNodes + 2);
Before the main loop, we need to initialize the already visited predecessor arrays, set the visited flag
for the starting node to true, and add the starting node index to the back of the queue:
Search
for (int i=0; i<numNodes; i++) {
alreadyVisitedFlag[i] = false;
predecessor[i] = -1;
alreadyVisitedFlag[start_node] = true;
queue.addToBackOfQueue(start_node);
The main loop runs until we find the goal node or the search queue is empty:
outer:
while (queue.isEmpty() == false) {
We will read (without removing) the node index at the front of the queue and calculate the nodes that
are connected to the current node (but not already on the visited list) using the connected_nodes
method (the interested reader can see the implementation in the source code for this class):
int head = queue.peekAtFrontOfQueue();
int [] connected = connected_nodes(head);
if (connected != null) {
If each node connected by a link to the current node has not already been visited, set the predecessor
array and add the new node index to the back of the search queue; we stop if the goal is found:
for (int i=0; i<connected.length; i++) {
if (alreadyVisitedFlag[connected[i]] == false) {
predecessor[connected[i]] = head;
queue.addToBackOfQueue(connected[i]);
if (connected[i] == goal_node) break outer;
alreadyVisitedFlag[head] = true;
queue.removeFromQueue(); // ignore return value
Now that the goal node has been found, we can build a new array of returned node indices for the
calculated path using the predecessor array:
Search
int [] ret = new int[numNodes + 1];
int count = 0;
ret[count++] = goal_node;
for (int i=0; i<numNodes; i++) {
ret[count] = predecessor[ret[count - 1]];
count++;
if (ret[count - 1] == start_node)
break;
int [] ret2 = new int[count];
for (int i=0; i<count; i++) {
ret2[i] = ret[count - 1 - i];
return ret2;
In order to run both the depth-first and breadth-first graph search examples, change directory to
src-search-maze and type the following commands:
javac *.java
java GraphDepthFirstSearch
java GraphBeadthFirstSearch
The following figure shows the results of finding a route from node 1 to node 9 in the small test
graph. Like the depth-first results seen in the maze search, this path is not optimal.
Depth-first Search in a Graph
The next figure shows an optimal path found using a breadth-first search. As we saw in the maze
search example, we find optimal solutions using breadth-first search at the cost of extra memory
required for the breadth-first search.
Search
Breadth-first Search in a Graph
Adding Heuristics to Breadth-first Search
We can usually make breadth-first search more efficient by ordering the search order for all branches
from a given position in the search space. For example, when adding new nodes from a specified
reference point in the search space, we might want to add nodes to the search queue first that are “in
the direction” of the goal location: in a two-dimensional search like our maze search, we might want
to search connected grid cells first that were closest to the goal grid space. In this case, pre-sorting
nodes (in order of closest distance to the goal) added to the breadth-first search queue could have a
dramatic effect on search efficiency. The alpha-beta additions to breadth-first search are seen in in
the next section.
Heuristic Search and Game Playing: Tic-Tac-Toe and
Now that a computer program has won a match against the human world champion, perhaps
people’s expectations of AI systems will be prematurely optimistic. Game search techniques are
not real AI, but rather, standard programming techniques. A better platform for doing AI research
is the game of Go. There are so many possible moves in the game of Go that brute force look ahead
(as is used in Chess playing programs) simply does not work. In 2016 the Alpha Go program became
stronger than human players by using Reinforcement Learning and Monte Carlo search.
Min-max type search algorithms with alpha-beta cutoff optimizations are an important program-
ming technique and will be covered in some detail in the remainder of this chapter. We will design
an abstract Java class library for implementing alpha-beta enhanced min-max search, and then use
this framework to write programs to play tic-tac-toe and chess.
Alpha-Beta Search
The first game that we will implement will be tic-tac-toe, so we will use this simple game to explain
how the min-max search (with alpha-beta cutoffs) works.
The figure showing possible moves for tic-tac-toe shows some of the possible moves generated from
a tic-tac-toe position where X has made three moves and O has made two moves; it is O’s turn to
Search
move. This is “level 0” in this figure. At level 0, O has four possible moves. How do we assign a
fitness value to each of O’s possible moves at level 0? The basic min-max search algorithm provides
a simple solution to this problem: for each possible move by O in level 1, make the move and store
the resulting 4 board positions. Now, at level 1, it is X’s turn to move. How do we assign values to
each of X’s possible three moves in the figure showing possible moves for tic-tac-toe? Simple, we
continue to search by making each of X’s possible moves and storing each possible board position
for level 2. We keep recursively applying this algorithm until we either reach a maximum search
depth, or there is a win, loss, or draw detected in a generated move. We assume that there is a fitness
function available that rates a given board position relative to either side. Note that the value of any
board position for X is the negative of the value for O.
Alpha Beta Search for Tic-Tac-Toe
To make the search more efficient, we maintain values for alpha and beta for each search level. Alpha
and beta determine the best possible/worst possible move available at a given level. If we reach a
situation like the second position in level 2 where X has won, then we can immediately determine
that O’s last move in level 1 that produced this position (of allowing X an instant win) is a low valued
move for O (but a high valued move for X). This allows us to immediately “prune” the search tree by
ignoring all other possible positions arising from the first O move in level 1. This alpha-beta cutoff
(or tree pruning) procedure can save a large percentage of search time, especially if we can set the
search order at each level with “probably best” moves considered first.
While tree diagrams as seen in the figure showing possible moves for tic-tac-toe quickly get
complicated, it is easy for a computer program to generate possible moves, calculate new possible
board positions and temporarily store them, and recursively apply the same procedure to the next
search level (but switching min-max “sides” in the board evaluation). We will see in the next section
that it only requires about 100 lines of Java code to implement an abstract class framework for
handling the details of performing an alpha-beta enhanced search. The additional game specific
classes for tic-tac-toe require about an additional 150 lines of code to implement; chess requires an
additional 450 lines of code.
Search
A Java Framework for Search and Game Playing
The general interface for the Java classes that we will develop in this section was inspired by
the Common LISP game-playing framework written by Kevin Knight and described in (Rich,
Knight 1991). The abstract class GameSearch contains the code for running a two-player game and
performing an alpha-beta search. This class needs to be sub-classed to provide the eight methods:
public abstract boolean drawnPosition(Position p)
public abstract boolean wonPosition(Position p,
boolean player)
positionEvaluation(Position p,
boolean player)
public abstract void printPosition(Position p)
public abstract Position []
possibleMoves(Position p,
boolean player)
public abstract Position makeMove(Position p,
boolean player,
Move move)
public abstract boolean reachedMaxDepth(Position p,
int depth)
public abstract Move getMove()
The method drawnPosition should return a Boolean true value if the given position evaluates to
a draw situation. The method wonPosition should return a true value if the input position is won
for the indicated player. By convention, I use a Boolean true value to represent the computer and
a Boolean false value to represent the human opponent. The method positionEvaluation returns a
position evaluation for a specified board position and player. Note that if we call positionEvaluation
switching the player for the same board position, then the value returned is the negative of the
value calculated for the opposing player. The method possibleMoves returns an array of objects
belonging to the class Position. In an actual game like chess, the position objects will actually
belong to a chess-specific refinement of the Position class (e.g., for the chess program developed
later in this chapter, the method possibleMoves will return an array of ChessPosition objects).
The method makeMove will return a new position object for a specified board position, side to
move, and move. The method reachedMaxDepth returns a Boolean true value if the search process
has reached a satisfactory depth. For the tic-tac-toe program, the method reachedMaxDepth does
not return true unless either side has won the game or the board is full; for the chess program, the
method reachedMaxDepth returns true if the search has reached a depth of 4 half moves deep (this
is not the best strategy, but it has the advantage of making the example program short and easy to
understand). The method getMove returns an object of a class derived from the class Move (e.g.,
TicTacToeMove or ChessMove).
The GameSearch class implements the following methods to perform game search:
Search
protected Vector alphaBeta(int depth, Position p,
boolean player)
protected Vector alphaBetaHelper(int depth,
Position p,
boolean player,
float alpha,
float beta)
public void playGame(Position startingPosition,
boolean humanPlayFirst)
The method alphaBeta is simple; it calls the helper method alphaBetaHelper with initial search
conditions; the method alphaBetaHelper then calls itself recursively. The code for alphaBeta is:
protected Vector alphaBeta(int depth,
Position p,
boolean player)
Vector v = alphaBetaHelper(depth, p, player,
1000000.0f,
-1000000.0f);
return v;
It is important to understand what is in the vector returned by the methods alphaBeta and
alphaBetaHelper. The first element is a floating point position evaluation for the point of view
of the player whose turn it is to move; the remaining values are the “best move” for each side to the
last search depth. As an example, if I let the tic-tac-toe program play first, it places a marker at square
index 0, then I place my marker in the center of the board an index 4. At this point, to calculate the
next computer move, alphaBeta is called and returns the following elements in a vector:
next element: 0.0
next element: [-1,0,0,0,1,0,0,0,0,]
next element: [-1,1,0,0,1,0,0,0,0,]
next element: [-1,1,0,0,1,0,0,-1,0,]
next element: [-1,1,0,1,1,0,0,-1,0,]
next element: [-1,1,0,1,1,-1,0,-1,0,]
next element: [-1,1,1,1,1,-1,0,-1,0,]
next element: [-1,1,1,1,1,-1,-1,-1,0,]
next element: [-1,1,1,1,1,-1,-1,-1,1,]
Here, the alpha-beta enhanced min-max search looked all the way to the end of the game and these
board positions represent what the search procedure calculated as the best moves for each side. Note
that the class TicTacToePosition (derived from the abstract class Position) has a toString method
to print the board values to a string.
The same printout of the returned vector from alphaBeta for the chess program is:
Search
next element: 5.4
next element:
[4,2,3,5,9,3,2,4,7,7,1,1,1,0,1,1,1,1,7,7,
0,0,0,0,0,0,0,0,7,7,0,0,0,1,0,0,0,0,7,7,
0,0,0,0,0,0,0,0,7,7,0,0,0,0,-1,0,0,0,7,7,
-1,-1,-1,-1,0,-1,-1,-1,7,7,-4,-2,-3,-5,-9,
-3,-2,-4,]
next element:
[4,2,3,0,9,3,2,4,7,7,1,1,1,5,1,1,1,1,7,7,
0,0,0,0,0,0,0,0,7,7,0,0,0,1,0,0,0,0,7,7,
0,0,0,0,0,0,0,0,7,7,0,0,0,0,-1,0,0,0,7,7,
-1,-1,-1,-1,0,-1,-1,-1,7,7,-4,-2,-3,-5,-9,
-3,-2,-4,]
next element:
[4,2,3,0,9,3,2,4,7,7,1,1,1,5,1,1,1,1,7,7,
0,0,0,0,0,0,0,0,7,7,0,0,0,1,0,0,0,0,7,7,
0,0,0,0,0,0,0,0,7,7,0,0,0,0,-1,-5,0,0,7,7,
-1,-1,-1,-1,0,-1,-1,-1,7,7,-4,-2,-3,0,-9,
-3,-2,-4,]
next element:
[4,2,3,0,9,3,0,4,7,7,1,1,1,5,1,1,1,1,7,7,
0,0,0,0,0,2,0,0,7,7,0,0,0,1,0,0,0,0,7,7,
0,0,0,,0,0,0,0,0,7,7,0,0,0,0,-1,-5,0,0,7,7,
-1,-1,-1,-1,0,-1,-1,-1,7,7,-4,-2,-3,0,-9,
-3,-2,-4,]
next element:
[4,2,3,0,9,3,0,4,7,7,1,1,1,5,1,1,1,1,7,7,
0,0,0,0,0,2,0,0,7,7,0,0,0,1,0,0,0,0,7,7,
-1,0,0,0,0,0,0,0,7,7,0,0,0,0,-1,-5,0,0,7,7,
0,-1,-1,-1,0,-1,-1,-1,7,7,-4,-2,-3,0,-9,
-3,-2,-4,]
Here, the search procedure assigned the side to move (the computer) a position evaluation score of
5.4; this is an artifact of searching to a fixed depth. Notice that the board representation is different
for chess, but because the GameSearch class manipulates objects derived from the classes Position
and Move, the GameSearch class does not need to have any knowledge of the rules for a specific
game. We will discuss the format of the chess position class ChessPosition in more detail when we
develop the chess program.
The classes Move and Position contain no data and methods at all. The classes Move and Position
are used as placeholders for derived classes for specific games. The search methods in the abstract
GameSearch class manipulate objects derived from the classes Move and Position.
Now that we have seen the debug printout of the contents of the vector returned from the methods
Search
alphaBeta and alphaBetaHelper, it will be easier to understand how the method alphaBetaHelper
works. The following text shows code fragments from the alphaBetaHelper method interspersed
with book text:
protected Vector alphaBetaHelper(int depth,
Position p,
boolean player,
float alpha,
float beta) {
Here, we notice that the method signature is the same as for alphaBeta, except that we pass floating
point alpha and beta values. The important point in understanding min-max search is that most of
the evaluation work is done while “backing up” the search tree; that is, the search proceeds to a leaf
node (a node is a leaf if the method reachedMaxDepth return a Boolean true value), and then a
return vector for the leaf node is created by making a new vector and setting its first element to
the position evaluation of the position at the leaf node and setting the second element of the return
vector to the board position at the leaf node:
if (reachedMaxDepth(p, depth)) {
Vector v = new Vector(2);
float value = positionEvaluation(p, player);
v.addElement(new Float(value));
v.addElement(p);
return v;
If we have not reached the maximum search depth (i.e., we are not yet at a leaf node in the
search tree), then we enumerate all possible moves from the current position using the method
possibleMoves and recursively call alphaBetaHelper for each new generated board position. In
terms of the figure showing possible moves for tic-tac-toe, at this point we are moving down to
another search level (e.g., from level 1 to level 2; the level in the figure showing possible moves for
tic-tac-toe corresponds to depth argument in alphaBetaHelper):
Vector best = new Vector();
Position [] moves = possibleMoves(p, player);
for (int i=0; i<moves.length; i++) {
Vector v2 = alphaBetaHelper(depth + 1, moves[i],
!player,
-beta, -alpha);
float value = -((Float)v2.elementAt(0)).floatValue();
if (value > beta) {
if(GameSearch.DEBUG)
Search
System.out.println(" ! ! ! value="+
value+
",beta="+beta);
beta = value;
best = new Vector();
best.addElement(moves[i]);
Enumeration enum = v2.elements();
enum.nextElement(); // skip previous value
while (enum.hasMoreElements()) {
Object o = enum.nextElement();
if (o != null) best.addElement(o);
* Use the alpha-beta cutoff test to abort
* search if we found a move that proves that
* the previous move in the move chain was dubious
if (beta >= alpha) {
break;
Notice that when we recursively call alphaBetaHelper, we are “flipping” the player argument to
the opposite Boolean value. After calculating the best move at this depth (or level), we add it to the
end of the return vector:
Vector v3 = new Vector();
v3.addElement(new Float(beta));
Enumeration enum = best.elements();
while (enum.hasMoreElements()) {
v3.addElement(enum.nextElement());
return v3;
When the recursive calls back up and the first call to alphaBetaHelper returns a vector to the
method alphaBeta, all of the “best” moves for each side are stored in the return vector, along with
the evaluation of the board position for the side to move.
The class GameSearch method playGame is fairly simple; the following code fragment is a partial
listing of playGame showing how to call alphaBeta, getMove, and makeMove:
Search
public void playGame(Position startingPosition,
boolean humanPlayFirst) {
System.out.println("Your move:");
Move move = getMove();
startingPosition = makeMove(startingPosition,
HUMAN, move);
printPosition(startingPosition);
Vector v = alphaBeta(0, startingPosition, PROGRAM);
startingPosition = (Position)v.elementAt(1);
The debug printout of the vector returned from the method alphaBeta seen earlier in this section
was printed using the following code immediately after the call to the method alphaBeta:
Enumeration enum = v.elements();
while (enum.hasMoreElements()) {
System.out.println(" next element: " +
enum.nextElement());
In the next few sections, we will implement a tic-tac-toe program and a chess-playing program using
this Java class framework.
Tic-Tac-Toe Using the Alpha-Beta Search Algorithm
Using the Java class framework of GameSearch, Position, and Move, it is simple to write a basic
tic-tac-toe program by writing three new derived classes (as seen in the next figure showing a UML
Class Diagram) TicTacToe (derived from GameSearch), TicTacToeMove (derived from Move), and
TicTacToePosition (derived from Position).
Search
UML Diagram for Tic-Tac-Toe Classes
I assume that the reader has the book example code installed and available for viewing. In this section,
I will only discuss the most interesting details of the tic-tac-toe class refinements; I assume that the
reader can look at the source code. We will start by looking at the refinements for the position and
move classes. The TicTacToeMove class is trivial, adding a single integer value to record the square
index for the new move:
public class TicTacToeMove extends Move {
public int moveIndex;
The board position indices are in the range of [0..8] and can be considered to be in the following
order:
The class TicTacToePosition is also simple:
Search
public class TicTacToePosition extends Position {
final static public int BLANK = 0;
final static public int HUMAN = 1;
final static public int PROGRAM = -1;
int [] board = new int[9];
public String toString() {
StringBuffer sb = new StringBuffer("[");
for (int i=0; i<9; i++)
sb.append(""+board[i]+",");
sb.append("]");
return sb.toString();
This class allocates an array of nine integers to represent the board, defines constant values for blank,
human, and computer squares, and defines a toString method to print out the board representation
to a string.
The TicTacToe class must define the following abstract methods from the base class GameSearch:
public abstract boolean drawnPosition(Position p)
public abstract boolean wonPosition(Position p,
boolean player)
public abstract float positionEvaluation(Position p,
boolean player)
public abstract void printPosition(Position p)
public abstract Position [] possibleMoves(Position p,
boolean player)
public abstract Position makeMove(Position p,
boolean player,
Move move)
public abstract boolean reachedMaxDepth(Position p,
int depth)
public abstract Move getMove()
The implementation of these methods uses the refined classes TicTacToeMove and TicTacToePo-
sition. For example, consider the method drawnPosition that is responsible for selecting a drawn
(or tied) position:
Search
public boolean drawnPosition(Position p) {
boolean ret = true;
TicTacToePosition pos = (TicTacToePosition)p;
for (int i=0; i<9; i++) {
if (pos.board[i] == TicTacToePosition.BLANK){
ret = false;
break;
return ret;
The overridden methods from the GameSearch base class must always cast arguments of type
Position and Move to TicTacToePosition and TicTacToeMove. Note that in the method drawnPo-
sition, the argument of class Position is cast to the class TicTacToePosition. A position is considered
to be a draw if all of the squares are full. We will see that checks for a won position are always
made before checks for a drawn position, so that the method drawnPosition does not need to
make a redundant check for a won position. The method wonPosition is also simple; it uses a
private helper method winCheck to test for all possible winning patterns in tic-tac-toe. The method
positionEvaluation uses the following board features to assign a fitness value from the point of
view of either player:
• The number of blank squares on the board
• If the position is won by either side
• If the center square is taken
The method positionEvaluation is simple, and is a good place for the interested reader to start
modifying both the tic-tac-toe and chess programs:
public float positionEvaluation(Position p,
boolean player) {
int count = 0;
TicTacToePosition pos = (TicTacToePosition)p;
for (int i=0; i<9; i++) {
if (pos.board[i] == 0) count++;
count = 10 - count;
// prefer the center square:
float base = 1.0f;
if (pos.board[4] == TicTacToePosition.HUMAN &&
player) {
base += 0.4f;
Search
if (pos.board[4] == TicTacToePosition.PROGRAM &&
!player) {
base -= 0.4f;
float ret = (base - 1.0f);
if (wonPosition(p, player))
return base + (1.0f / count);
if (wonPosition(p, !player))
return -(base + (1.0f / count));
return ret;
The only other method that we will look at here is possibleMoves; the interested reader can
look at the implementation of the other (very simple) methods in the source code. The method
possibleMoves is called with a current position, and the side to move (i.e., program or human):
public Position [] possibleMoves(Position p,
boolean player) {
TicTacToePosition pos = (TicTacToePosition)p;
int count = 0;
for (int i=0; i<9; i++) {
if (pos.board[i] == 0) count++;
if (count == 0) return null;
Position [] ret = new Position[count];
count = 0;
for (int i=0; i<9; i++) {
if (pos.board[i] == 0) {
TicTacToePosition pos2 =
TicTacToePosition();
for (int j=0; j<9; j++)
pos2.board[j] = pos.board[j];
if (player) pos2.board[i] = 1;
else pos2.board[i] = -1;
ret[count++] = pos2;
return ret;
Search
It is very simple to generate possible moves: every blank square is a legal move. (This method will
not be as straightforward in the example chess program!)
It is simple to compile and run the example tic-tac-toe program: change directory to src-search-game
and type:
mvn install
mvn exec:java -Dexec.mainClass="search.game.TicTacToe"
When asked to enter moves, enter an integer between 0 and 8 for a square that is currently blank
(i.e., has a zero value). The following shows this labeling of squares on the tic-tac-toe board:
You might need to enter two return (enter) keys after entering your move square when using a
macOS terminal.
Chess Using the Alpha-Beta Search Algorithm
Using the Java class framework of GameSearch, Position, and Move, it is reasonably easy to write
a simple chess program by writing three new derived classes (see the figure showing a UML diagram
for the chess game casses) Chess (derived from GameSearch), ChessMove (derived from Move),
and ChessPosition (derived from Position). The chess program developed in this section is intended
to be an easy to understand example of using alpha-beta min-max search; as such, it ignores several
details that a fully implemented chess program would implement:
• Allow the computer to play either side (computer always plays black in this example).
• Allow en-passant pawn captures.
• Allow the player to take back a move after making a mistake.
The reader is assumed to have read the last section on implementing the tic-tac-toe game; details of
refining the GameSearch, Move, and Position classes are not repeated in this section.
The following figure showing a UML diagram for the chess game casses shows the UML class
diagram for both the general purpose GameSearch framework and the classes derived to implement
chess specific data and behavior.
Search
UML Diagram for Chess Game Classes
The class ChessMove contains data for recording from and to square indices:
public class ChessMove extends Move {
public int from;
public int to;
The board is represented as an integer array with 120 elements. A chessboard only has 64 squares;
the remaining board values are set to a special value of 7, which indicates an “off board” square. The
initial board setup is defined statically in the Chess class and the off-board squares have a value of
Search
private static int [] initialBoard = {
7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
4, 2, 3, 5, 9, 3, 2, 4, 7, 7,
// white pieces
1, 1, 1, 1, 1, 1, 1, 1, 7, 7,
// white pawns
0, 0, 0, 0, 0, 0, 0, 0, 7, 7,
// 8 blank squares
0, 0, 0, 0, 0, 0, 0, 0, 7, 7,
// 8 blank squares
0, 0, 0, 0, 0, 0, 0, 0, 7, 7,
// 8 blank squares
0, 0, 0, 0, 0, 0, 0, 0, 7, 7,
// 8 blank squares
-1,-1,-1,-1,-1,-1,-1,-1, 7, 7,
// black pawns
-4,-2,-3,-5,-9,-3,-2,-4, 7, 7,
// black pieces
7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7
First example chess board
It is difficult to see from this listing of the board square values but in effect a regular chess board if
padded on all sides with two rows and columns of “7” values.
We see the start of a sample chess game in the previous figure and the continuation of this same
game in the next figure.The lookahead is limited to 2 moves (4 ply).
Search
Second example chess board
The class ChessPosition contains data for this representation and defines constant values for playing
sides and piece types:
public class ChessPosition extends Position {
final static public int BLANK = 0;
final static public int HUMAN = 1;
final static public int PROGRAM = -1;
final static public int PAWN = 1;
final static public int KNIGHT = 2;
final static public int BISHOP = 3;
final static public int ROOK = 4;
final static public int QUEEN = 5;
final static public int KING = 6;
int [] board = new int[120];
public String toString() {
StringBuffer sb = new StringBuffer("[");
for (int i=22; i<100; i++) {
sb.append(""+board[i]+",");
sb.append("]");
return sb.toString();
The class Chess also defines other static data. The following array is used to encode the values
Search
assigned to each piece type (e.g., pawns are worth one point, knights and bishops are worth 3 points,
etc.):
private static int [] value = {
0, 1, 3, 3, 5, 9, 0, 0, 0, 12
The following array is used to codify the possible incremental moves for pieces:
private static int [] pieceMovementTable = {
0, -1, 1, 10, -10, 0, -1, 1, 10, -10, -9, -11, 9,
11, 0, 8, -8, 12, -12, 19, -19, 21, -21, 0, 10, 20,
0, 0, 0, 0, 0, 0, 0, 0
The starting index into the pieceMovementTable array is calculated by indexing the following array
with the piece type index (e.g., pawns are piece type 1, knights are piece type 2, bishops are piece
type 3, rooks are piece type 4, etc.:
private static int [] index = {
0, 12, 15, 10, 1, 6, 0, 0, 0, 6
When we implement the method possibleMoves for the class Chess, we will see that except for
pawn moves, all other possible piece type moves are very easy to calculate using this static data.
The method possibleMoves is simple because it uses a private helper method calcPieceMoves to
do the real work. The method possibleMoves calculates all possible moves for a given board position
and side to move by calling calcPieceMove for each square index that references a piece for the side
to move.
We need to perform similar actions for calculating possible moves and squares that are controlled by
each side. In the first version of the class Chess that I wrote, I used a single method for calculating
both possible move squares and controlled squares. However, the code was difficult to read, so I split
this initial move generating method out into three methods:
• possibleMoves – required because this was an abstract method in GameSearch. This method
calls calcPieceMoves for all squares containing pieces for the side to move, and collects all
possible moves.
• calcPieceMoves – responsible to calculating pawn moves and other piece type moves for a
specified square index.
• setControlData – sets the global array computerControl and humanControl. This method is
similar to a combination of possibleMoves and calcPieceMoves, but takes into effect “moves”
onto squares that belong to the same side for calculating the effect of one piece guarding
another. This control data is used in the board position evaluation method positionEvaluation.
Search
We will discuss calcPieceMoves here, and leave it as an exercise to carefully read the similar method
setControlData in the source code. This method places the calculated piece movement data in static
storage (the array piece_moves) to avoid creating a new Java object whenever this method is called;
method calcPieceMoves returns an integer count of the number of items placed in the static array
piece_moves. The method calcPieceMoves is called with a position and a square index; first, the
piece type and side are determined for the square index:
private int calcPieceMoves(ChessPosition pos,
int square_index) {
int [] b = pos.board;
int piece = b[square_index];
int piece_type = piece;
if (piece_type < 0) piece_type = -piece_type;
int piece_index = index[piece_type];
int move_index = pieceMovementTable[piece_index];
if (piece < 0) side_index = -1;
side_index = 1;
Then, a switch statement controls move generation for each type of chess piece (movement
generation code is not shown – see the file Chess.java):
switch (piece_type) {
case ChessPosition.PAWN:
break;
case ChessPosition.KNIGHT:
case ChessPosition.BISHOP:
case ChessPosition.ROOK:
case ChessPosition.KING:
case ChessPosition.QUEEN:
break;
The logic for pawn moves is a little complex but the implementation is simple. We start by checking
for pawn captures of pieces of the opposite color. Then check for initial pawn moves of two squares
forward, and finally, normal pawn moves of one square forward. Generated possible moves are
placed in the static array piece_moves and a possible move count is incremented. The move logic
for knights, bishops, rooks, queens, and kings is very simple since it is all table driven. First, we use
the piece type as an index into the static array index; this value is then used as an index into the static
array pieceMovementTable. There are two loops: an outer loop fetches the next piece movement
delta from the pieceMovementTable array and the inner loop applies the piece movement delta set
in the outer loop until the new square index is off the board or “runs into” a piece on the same side.
Note that for kings and knights, the inner loop is only executed one time per iteration through the
outer loop:
Search
move_index = piece;
if (move_index < 0) move_index = -move_index;
move_index = index[move_index];
//System.out.println("move_index="+move_index);
next_square =
square_index + pieceMovementTable[move_index];
outer:
while (true) {
inner:
while (true) {
if (next_square > 99) break inner;
if (next_square < 22) break inner;
if (b[next_square] == 7) break inner;
// check for piece on the same side:
if (side_index < 0 && b[next_square] < 0)
break inner;
if (side_index >0 && b[next_square]
break inner;
piece_moves[count++] = next_square;
if (b[next_square] != 0) break inner;
if (piece_type == ChessPosition.KNIGHT)
break inner;
if (piece_type == ChessPosition.KING)
break inner;
next_square += pieceMovementTable[move_index];
move_index += 1;
if (pieceMovementTable[move_index] == 0)
break outer;
next_square = square_index +
pieceMovementTable[move_index];
The figures show the start of a second example game. The computer was making too many trivial
mistakes in the first game so here I increased the lookahead to 2 1/2 moves. Now the computer takes
one to two seconds per move and plays a better game. Increasing the lookahead to 3 full moves
yields a better game but then the program can take up to about ten seconds per move.
Search
After 1 d4 e6 2 e4 Qh4 Black (the computer) increases the mobility of its pieces by bringing out the queen early but
we will see that this soon gets black in trouble.
After 3 Nc3 Nf6 4 Bd3 Bb4 5 Nf3 Qh5 Black continues to develop pieces and puts pressure on the pawn on E4 but the
vulnerable queen makes this a weak position for black.
The method setControlData is very similar to this method; I leave it as an exercise to the reader to
read through the source code. Method setControlData differs in also considering moves that protect
pieces of the same color; calculated square control data is stored in the static arrays computerCon-
trol and humanControl. This square control data is used in the method positionEvaluation that
assigns a numerical rating to a specified chessboard position on either the computer or human side.
The following aspects of a chessboard position are used for the evaluation:
• material count (pawns count 1 point, knights and bishops 3 points, etc.)
• count of which squares are controlled by each side
Search
• extra credit for control of the center of the board
• credit for attacked enemy pieces
Notice that the evaluation is calculated initially assuming the computer’s side to move. If the position
if evaluated from the human player’s perspective, the evaluation value is multiplied by minus one.
The implementation of positionEvaluation is:
public float positionEvaluation(Position p,
boolean player) {
ChessPosition pos = (ChessPosition)p;
int [] b = pos.board;
float ret = 0.0f;
// adjust for material:
for (int i=22; i<100; i++) {
if (b[i] != 0 && b[i] != 7)
ret += b[i];
// adjust for positional advantages:
setControlData(pos);
int control = 0;
for (int i=22; i<100; i++) {
control += humanControl[i];
control -= computerControl[i];
// Count center squares extra:
control += humanControl[55] - computerControl[55];
control += humanControl[56] - computerControl[56];
control += humanControl[65] - computerControl[65];
control += humanControl[66] - computerControl[66];
control /= 10.0f;
ret += control;
// credit for attacked pieces:
for (int i=22; i<100; i++) {
if (b[i] == 0 || b[i] == 7) continue;
if (b[i] < 0) {
if (humanControl[i] > computerControl[i]) {
ret += 0.9f * value[-b[i]];
if (b[i] > 0) {
if (humanControl[i] < computerControl[i]) {
Search
ret -= 0.9f * value[b[i]];
// adjust if computer side to move:
if (!player) ret = -ret;
return ret;
It is simple to compile and run the example chess program by typing in the search directory:
make chess
When asked to enter moves, enter string like “d2d4” to enter a move in chess algebraic notation.
Here is sample output from the program:
Board position:
BR BN BB .
BK BB BN BR
BP BP BP BP .
BP BP BP
WP WP WP .
WP WP WP WP
WR WN WB WQ WK WB .
Your move:
Note that the newest code in the GitHub repository uses Unicode characters to display graphics for
Chess pieces.
The example chess program plays in general good moves, but its play could be greatly enhanced
with an “opening book” of common chess opening move sequences. If you run the example chess
program, depending on the speed of your computer and your Java runtime system, the program
takes a while to move (about 5 seconds per move on my PC). Where is the time spent in the chess
program? The following table shows the total runtime (i.e., time for a method and recursively all
called methods) and method-only time for the most time consuming methods. Methods that show
zero percent method only time used less than 0.1 percent of the time so they print as zero values.
Search
Class.method name
% of total runtime
% in this method
---------------------------- ------------------------ ----------------
Chess.main
GameSearch.playGame
GameSearch.alphaBeta
GameSearch.alphaBetaHelper
Chess.positionEvaluate
Chess.setControlData
Chess.possibleMoves
Chess.calcPossibleMoves
Chess.calcPieceMoves
The interested reader is encouraged to choose a simple two-player game, and using the game search
class framework, implement your own game-playing program.
Reasoning
While the topic of reasoning may not be as immediately useful for your work as for example deep
learning, reasoning is a broad and sometimes useful topic. You might want to just quickly review this
chapter and revisit it when and if you need to use any reasoning system. That said, the introductory
discussion of logic and reasoning is good background information to know.
In this chapter we will concentrate on the use of the PowerLoom descriptive logic reasoning system.
PowerLoom is available with a Java runtime and Java API - this is what I will use for the examples
in this chapter. PowerLoom can also be used with other JVM languages like JRuby and Clojure.
PowerLoom is also available in Common Lisp and C++ versions.
The PowerLoom system has not been an active project since 2010. As I update this chapter in July
2020, I still consider PowerLoom to be a useful tool for learning about logic based systems and I
have attempted to package PowerLoom in a way that will be easy for you to run interactively and I
provide a few simple Java examples in the package com.markwatson.powerloom that demonstrate
how to embed PowerLoom in your own Java programs. The complete Java source for PowerLoom
is in the directory powerloom/src/main/java/edi/isi/powerloom.
Additionally, we will look at different kinds of reasoning systems (the OWL language) in the chapter
on the Semantic Web and use this reasoning in the later chapters Automatically Generating Data
for Knowledge Graphs and Knowledge Graph Navigator.
While the material in this chapter will get you started with development using a powerful reasoning
system and embedding this reasoning system in Java applications, you may want to dig deeper and
I suggest sources for further study at the end of this chapter.
PowerLoom is a newer version of the classic Loom Descriptive Logic reasoning system written at ISI
although as I mentioned earlier it has not been developed past 2010. At some point you may want
to download the entire PowerLoom distribution to get more examples and access to documentation;
the PowerLoom web site¹⁰.
While we will look at an example of embedding the PowerLoom runtime and a PowerLoom model
in a Java example program, I want to make a general comment on PowerLoom development: you
will spend most of your time interactively running PowerLoom in an interactive shell that lets you
type in concepts, relations, rules, and queries and immediately see the results. If you have ever
programmed in Lisp, then this mode of interactive programming will be familiar to you. As seen
in the next figure, after interactive development you can deploy in a Java application. This style
of development supports entering facts and trying rules and relations interactively and as you get
things working you can paste what works into a PowerLoom source file. If you have only worked
with compiled languages like Java and C++ this development style may take a while to get used
¹⁰http://www.isi.edu/isd/LOOM/PowerLoom/
Reasoning
to and appreciate. As seen in the next figure the PowerLoom runtime system, with relations and
rules, can be embedded in Java applications that typically clear PowerLoom data memory, assert
facts from other live data sources, and then use PowerLoom for inferencing.
Developing Reasoning Systems
We will look at different types of logic and reasoning systems in this section and then later we will
get into PowerLoom specific examples. Logic is the basis for both Knowledge Representation and
for reasoning about knowledge. We will encode knowledge using logic and see that we can then
infer new facts that are not explicitly asserted. In AI literature you often see discussions of implicit
vs. explicit knowledge. Implicit knowledge is inferred from explicitly stated information by using a
reasoning system.
First Order Logic was invented by the philosophers Frege and Peirce and is the most widely studied
logic system. Unfortunately, full First Order Logic is not computationally tractable for most non-
trivial problems so we use more restricted logics. We will use two reasoning systems in this book
that support more limited logics:
• We use PowerLoom in this chapter. PowerLoom supports a combination of limited first order
predicate logic and features of description logic. PowerLoom is able to classify objects, use rules
to infer facts from existing facts and to perform subsumption (determining class membership
of instances).
• We will use RDF Schema (RDFS) reasoning in the Chapter on Semantic Web. RDFS supports
more limited reasoning than descriptive logic reasoners like PowerLoom and OWL Description
Logic reasoners.
Reasoning
History of Logic
The Greek philosopher Aristotle studied forms of logic as part of his desire to improve the
representation of knowledge. He started a study of logic and the definition of both terms (e.g.,
subjects, predicates, nouns, verbs) and types of logical deduction. Much later the philosopher Frege
defined predicate logic (for example: All birds have feathers. Brady is a bird, therefore Brady has
feathers) that forms the basis for the modern Prolog programming language.
Examples of Different Logic Types
Propositional logic is limited to atomic statements that can be either true or false:
Brady-is-a-bird
Brady-has-feathers
First Order Predicate Logic allows access to the structure of logic statements dealing with predicates
that operate on atoms. To use a Prolog notation:
feathers(X) :- bird(X).
bird(brady).
In this example, “feathers” and “bird” are predicates and “brady” is an atom. The first example states
that for all X, if X is a bird, then X has feathers. In the second example we state that Brady is a bird.
Notice that in the Prolog notation that we are using, variables are capitalized and predicate names
and literal atoms are lower case.
Here is a query that asks who has feathers:
?- feathers(X).
X = brady
In this example through inference we have determined a new fact, that Brady has feathers because
we know that Brady is a bird and we have the rule (or predicate) stating that all birds have feathers.
Prolog is not strictly a pure logic programming language since the order in which rules (predicates)
are defined changes the inference results. Prolog is a great language for some types of projects (I
have used Prolog in both natural language processing and in planning systems). We will see that
PowerLoom is considerably more flexible than Prolog but does have a steep learning curve.
Description Logic deals with descriptions of concepts and how these descriptions define the domain
of concepts. In terms used in object oriented programming languages: membership in a class is
determined implicitly by the description of the object and not by explicitly stating something like
“Brady is a member of the bird class.” Description logics divide statements into relations (historically
referred to as TBox) and concepts (historically called ABox). We would say that a statement like “All
birds have feathers” is stored in the TBox while a specific assertion like “Brady is a bird” is stored
in the ABox.
Reasoning
PowerLoom Overview
PowerLoom is designed to be an expressive language for knowledge representation and reasoning.
As a result, PowerLoom is not a complete reasoning system but makes tradeoffs for completeness
of inferences and expressivity vs. computational efficiency. It is interesting to note that Loom and
PowerLoom were designed and implemented to solve real world problems and the tradeoffs to make
these problems computationally tractable have informed the design and implementation of these
systems. PowerLoom does not make all possible inferences from concepts that it operates on.
The PowerLoom distribution contains two very detailed examples for representing relationships
between companies and for information dealing with airplanes. These examples are more detailed
than the simpler examples in this chapter. We will look at just one of these examples (business rules
and relations) and after working through this chapter, I encourage you to interactively experiment
with the two examples that ship with PowerLoom.
We will start by defining some terms used in PowerLoom:
• concept - the Java equivalent would be an instance of a class.
• relation - specifies a link between two concepts.
• function - functional mapping of one concept to another.
• rule - allows new concepts to be deduced without explicitly asserting them.
A relation can specify the types of concepts that a relation connects. An example will make this clear
and introduce the Lisp-like syntax of PowerLoom statements:
;;; Concepts:
(defconcept person)
(defconcept parent (?p person))
;;; Relation:
(defrelation parent-of ((?p1 parent) (?p2 person)))
Here I have defined two concepts: person and parent. Note that we have a hierarchy of concept
types: the parent is a more specific concept type than the person concept. I loose metaphor is that in
object oriented programming a parent is a subclass of a person and this hierarchy is stated in line 3
of the last listing. All instances that are parents are also of type person. The relation parent-of links
a parent concept to a person concept.
We will learn more about basic PowerLoom functionality in the next two sections as we use
PowerLoom in an interactive session and when we embed PowerLoom in a Java example program.
Reasoning
Running PowerLoom Interactively
We will experiment with PowerLoom concepts, relations, and rules in this section in an interactive
command shell. I will introduce more examples of PowerLoom functionality for asserting instances
of concepts, performing queries, loading PowerLoom source files, defining relations, using separate
modules, and asking PowerLoom to explain the inference process that it used for query processing.
You can run PowerLoom using the command line interface using:
cd powerloom
mvn install
mvn exec:java -Dexec.mainClass="edu.isi.powerloom.PowerLoom"
This starts the PowerLoom standalone system and prints a prompt that includes the name of the
current module. The default module name is “PL-USER”. In the first example, when I enter the
person concept at the interactive prompt then PowerLoom prints the result of the expression that
just entered. You can enter (demo) to have access to all of the demo scripts from the PowerLoom
distribution. These demo files are in the subdirectory powerloom/sources/logic/demos in the
GitHub repository for this book.
Please note that depending on which terminal you are running in, the prompt “PL-USER” does not
occur until after entering an “extra” return or enter key.
PL-USER |= (defconcept person)
|c|PERSON
PL-USER |= (defconcept parent (?p person))
|c|PARENT
PL-USER |= (defrelation parent-of
((?p1 parent) (?p2 person)))
|r|PARENT-OF
PL-USER |= (assert (person Ken))
|P|(PERSON KEN)
PL-USER |= (assert (person Mark))
|P|(PERSON MARK)
PL-USER |= (assert (parent-of Ken Mark))
|P|(PARENT-OF KEN MARK)
Now that we have entered two concepts, a test relation, and asserted a few facts, we can look at an
example of PowerLoom’s query language:
Reasoning
PL-USER |= (retrieve all ?p (person ?p))
There are 2 solutions:
#1: ?P=MARK
#2: ?P=KEN
PL-USER |= (retrieve all ?p (parent ?p))
There is 1 solution:
#1: ?P=KEN
PL-USER |=
The obvious point to note from this example is that we never specified that Ken was a parent; rather,
PowerLoom deduced this from the parent-of relation. The asserted data is explicit data while the
inferred data is implicit and only exists when required to reason over a query.
PowerLoom’s command line system prompts you with the string “PL-USER |=“ and you can
type any definition or query. Like Lisp, PowerLoom uses a prefix notation and expressions are
contained in parenthesis. PowerLoom supports a module system for partitioning concepts, relations,
functions, and rules into different sets and as previously mentioned “PL-USER” is the default module.
PowerLoom modules can form a hierarchy, inheriting concepts, relations, and rules from parent
modules.
The subdirectory test_data contains the demo file business.plm written by Robert MacGregor that
is supplied with the full PowerLoom distribution. You can load his complete example using:
PL-USER |= (load "test_data/business.plm")
This is a good example because it demonstrates most of the available functionality of PowerLoom
in a short 200 lines. When you are finished reading this chapter, please take a few minutes to read
through this example file since I do not list all of it it here. There are a few things to notice in this
example. Here we see a rule used to make the relation “contains” transitive:
(defrelation contains (
(?l1 geographic-location)
(?l2 geographic-location)))
(defrule transitive-contains
(=> (and (contains ?l1 ?l2)
(contains ?l2 ?l3))
(contains ?l1 ?l3)))
The operator => means that if the first clause is true then so is the second. In English, this rule
could be stated “if an instance i1 contains i2 and if instance i2 contains i3 then we can infer that
i1 also contains i3.” To see how this rule works in practice, we can switch to the example module
“BUSINESS” and find all locations contained inside another location:
Reasoning
PL-USER |= (in-module "BUSINESS")
BUSINESS |= (retrieve all
(?location1 ?location2)
(contains ?location1 ?location2))
There are 15 solutions:
#1: ?LOCATION1=SOUTHERN-US, ?LOCATION2=TEXAS
#2: ?LOCATION1=TEXAS, ?LOCATION2=AUSTIN
#3: ?LOCATION1=TEXAS, ?LOCATION2=DALLAS
#4: ?LOCATION1=UNITED-STATES, ?LOCATION2=SOUTHERN-US
#5: ?LOCATION1=GEORGIA, ?LOCATION2=ATLANTA
#6: ?LOCATION1=EASTERN-US, ?LOCATION2=GEORGIA
#7: ?LOCATION1=UNITED-STATES, ?LOCATION2=EASTERN-US
#8: ?LOCATION1=SOUTHERN-US, ?LOCATION2=DALLAS
#9: ?LOCATION1=SOUTHERN-US, ?LOCATION2=AUSTIN
#10: ?LOCATION1=UNITED-STATES, ?LOCATION2=DALLAS
#11: ?LOCATION1=UNITED-STATES, ?LOCATION2=TEXAS
#12: ?LOCATION1=UNITED-STATES, ?LOCATION2=AUSTIN
#13: ?LOCATION1=EASTERN-US, ?LOCATION2=ATLANTA
#14: ?LOCATION1=UNITED-STATES, ?LOCATION2=GEORGIA
#15: ?LOCATION1=UNITED-STATES, ?LOCATION2=ATLANTA
BUSINESS |=
Here we have fifteen solutions even though there are only seven contains relations asserted in the
business.plm file, the other eight solutions were inferred. In addition to the retrieve function that
finds solutions matching a query you can also use the ask function to determine if a specified relation
is true; for example:
BUSINESS |= (ask (contains UNITED-STATES DALLAS))
BUSINESS |=
For complex queries you can use the “why” function to see how PowerLoom solved the last query:
BUSINESS |= (ask (contains southern-us dallas))
BUSINESS |= (why)
1 (CONTAINS ?location1 ?location2)
follows by Modus Ponens
with substitution {?l1/SOUTHERN-US, ?l3/DALLAS,
?l2/TEXAS}
since 1.1 ! (FORALL (?l1 ?l3)
(<= (CONTAINS ?l1 ?l3)
Reasoning
(EXISTS (?l2)
(AND (CONTAINS ?l1 ?l2)
(CONTAINS ?l2 ?l3)))))
1.2 ! (CONTAINS SOUTHERN-US TEXAS)
1.3 ! (CONTAINS TEXAS DALLAS)
BUSINESS |=
By default the explanation facility is turned off because it causes PowerLoom to run more slowly; it
was turned on in the file business.plm using the statement:
(set-feature justifications)
Using the PowerLoom APIs in Java Programs
Once you interactively develop concepts, rules and relations then it is likely that you may want to
use them with PowerLoom in an embedded mode, making PowerLoom a part of your application. I
will get you started with a few Java example programs. The source code for this chapter divide into
two packages:
• edu.isi.powerloom - source code from the PowerLoom web site.
• com.markwatson.powerloom - book example code for utilities and two embedded examples.
These packages can be seen in this screen shot:
Reasoning
Powerloom example in IntelliJ IDE
If you download the PowerLoom manual (a PDF file) from the PowerLoom web site, you will have
the complete Java API documentation for the Java version of PowerLoom (there are also C++ and
Common Lisp versions with separate documentation). I have found that I generally use just a small
subset of the Java PowerLoom APIs and I have “wrapped” this subset in a wrapper class in the file
PowerLoomUtils.java. We will use my wrapper class for the examples in the rest of this chapter.
The following UML class diagram will give you an overview before we dive into the code:
Reasoning
UML class diagram for the Powerloom example
My wrapper class has the follow public methods:
• PowerLoomUtils() - constructor initializes the Java PowerLoom runtime system.
• load(String fpath) - load a source *.plm file.
• changeModule(String workingModule) - set the current PowerLoom working module (“PL-
USER” is the default module).
• assertProposition(String proposition) - asserts a new proposition; for example: “(and (company
c3) (company-name c3 \“Moms Grocery\”))”. Note that quotation marks are escaped with
a backslash character. You can also use single quote characters like: “(and (company c3)
(company-name c3 ’Moms Grocery’))” because I convert single quotes in my wrapper code.
• createRelation(String relation, int arity) - create a new relation with a specified arity (number
of “arguments”). For example you could create a relation “owns” with arity 2 and then assert
“(owns Elaine ’Moms Grocery’)” - I usually do not use this API since I prefer to place relations
(with rules) in a source code file ending in the extension *.plm.
• doQuery(String query) - returns a list of results from a query. Each result in the list is itself a
You will always want to work in an interactive PowerLoom console for writing and debugging
PowerLoom models. I copied the model in business.plm from the PowerLoom distribution to the
subdirectory test_data. Here we use it in an embedded Java example in the file PowerLoomExam-
ple_1.java:
Reasoning
PowerLoomUtils plu = new PowerLoomUtils();
plu.load("test_data/business.plm");
plu.changeModule("BUSINESS");
plu.assertProposition(
"(and (company c1)" +
(company-name c1 \"Moms Grocery\"))");
plu.assertProposition(
"(and (company c2)" +
(company-name c2 \"IBM\"))");
plu.assertProposition(
"(and (company c3)" +
(company-name c3 \"Apple\"))");
List answers = plu.doQuery("all ?x (company ?x)");
System.out.println(answers);
// answers: [[C3], [C2], [C1]]
answers = plu.doQuery(
"all (?x ?name)" +
(and" +
(company ?x)" +
(company-name ?x ?name))");
System.out.println(answers);
// answers:
[[C3, "Apple"],
[C2, "IBM"],
[C1, "Moms Grocery"]]
plu.createRelation("CEO", 2);
plu.assertProposition(
"(CEO \"Apple\" \"SteveJobs\")");
answers = plu.doQuery(
"all (?x ?name ?ceo)" +
(and" +
(company-name ?x ?name)" +
(CEO ?name ?ceo))");
System.out.println(answers);
// answers: [[C3, "Apple", "SteveJobs"]]
I have added the program output produced by printing the value of the list variable “answers” as
comments after each System.out.println call. In the wrapper API calls that take a string argument,
I broke long strings over several lines for formatting to the width of a page; you would not do this
in your own programs because of the cost of the extra string concatenation.
We will not look at the implementation of the PowerLoomUtils class, you can read the code if you
are interested. That said, I will make a few comments on the Java PowerLoom APIs. The class PLI
Reasoning
contains static methods for initializing the system, loading PowerLoom source files. Here are a few
examples:
PLI.initialize();
PLI.load("business.plm", null);
PLI.changeModule("BUSINESS", null);
Suggestions for Further Study
This chapter has provided a brief introduction to PowerLoom. I also showed you how to go about
embedding PowerLoom in your Java programs to add capabilities for knowledge representation
and reasoning. Assuming that you see benefit to further study I recommend reading through the
PowerLoom manual and the presentations (PDF files) on the PowerLoom web site. As you read
through this material it is best to have an interactive PowerLoom session open to try the examples
as you read them.
Knowledge Representation and Logic are huge subjects and I will close out this chapter by
recommending a few books that have been the most helpful to me:
• Knowledge Representation by John Sowa. This has always been my favorite reference for
knowledge representation, logic, and ontologies.
• Artificial Intelligence, A Modern Approach by Stuart Russell and Peter Norvig. A very good
theoretical treatment of logic and knowledge representation.
• The Art of Prolog by Leon Sterling and Ehud Shapiro. Prolog implements a form of predicate
logic that is less expressive than the descriptive logics supported by PowerLoom and OWL
(Chapter on Semantic Web). That said, Prolog is very efficient and fairly easy to learn and so
is sometimes a better choice. This book is one of my favorite general Prolog references.
The Prolog language is a powerful AI development tool. Both open source, the SWI-Prolog and Amzi
Prolog systems have good Java interfaces. I don’t cover Prolog in this book but there are several very
good tutorials on the web if you decide to experiment with Prolog.
We will continue in the chapter on the Semantic Web with our study of logic-based reasoning
systems in the context of the Semantic Web.
Anomaly Detection Machine Learning
Example
Anomaly detection models are used in one very specific class of use cases: when you have many
negative (non-anomaly) examples and relatively few positive (anomaly) examples. We can refer to
this as an unbalanced training set. For training we will ignore positive examples, create a model
of “how things should be,” and hopefully be able to detect anomalies different from the original
negative examples.
If you have a large training set of both negative and positive examples then do not use anomaly
detection models. If your training examples are balanced then use a classification model as we will
see later in the chapter Deep Learning Using Deeplearning4j.
Motivation for Anomaly Detection
There are two other examples in this book using the University of Wisconsin cancer data. These
other examples are supervised learning. Anomaly detection as we do it in this chapter is, more or
less, unsupervised learning.
When should we use anomaly detection? You should use supervised learning algorithms like neural
networks and logistic classification when there are roughly equal number of available negative and
positive examples in the training data. The University of Wisconsin cancer data set is fairly evenly
split between negative and positive examples.
Anomaly detection should be used when you have many negative (“normal”) examples and relatively
few positive (“anomaly”) examples. For the example in this chapter we will simulate scarcity of
positive (“anomaly”) results by preparing the data using the Wisconsin cancer data as follows:
• We will split the data into training (60%), cross validation (20%) and testing (20%).
• For the training data, we will discard all but two positive (“anomaly”) examples. We do this
to simulate the real world test case where some positive examples are likely to end up in the
training data in spite of the fact that we would prefer the training data to only contain negative
(“normal”) examples.
• We will use the cross validation data to find a good value for the epsilon meta parameter.
• After we find a good epsilon value, we will calculate the F1 measurement for the model.
Anomaly Detection Machine Learning Example
Math Primer for Anomaly Detection
We are trying to model “normal” behavior and we do this by taking each feature and fitting a
Gaussian (bell curve) distribution to each feature. The learned parameters for a Gaussian distribution
are the mean of the data (where the bell shaped curve is centered) and the variance. You might be
more familiar with the term standard deviation, σ. Variance is defined as σ2.
We will need to calculate the probability of a value x given the mean and variance of a probability
distribution: P(x : µ, σ2) where µ is the mean and σ2 is the squared variance:
P(x : µ, σ2) =
2π e−(x−µ)2/2σ2
where xi are the samples and we can calculate the squared variance as:
(xi − µ)2
We calculate the parameters of µ and σ2 for each feature. A bell shaped distribution in two
dimensions is easy to visualize as is an inverted bowl shape in three dimentions. What if we have
many features? Well, the math works and don’t worry about not being able to picture it in your
AnomalyDetection Utility Class
The class AnomalyDetection developed in this section is fairly general purpose. It processes a set of
training examples and for each feature calculates µ and σ2. We are also training for a third parameter:
an epsilon “cutoff” value: if for a given input vector if P(x : µ, σ2) evaluates to a value greater
than epsilon then the input vector is “normal”, less than epsilon implies that the input vector is an
“anomaly.” The math for calulating these three features from training data is fairly easy but the code
is not: we need to organize the training data and search for a value of epsilon that minimizes the
error for a cross validaton data set.
To be clear: we separate the input examples into three separate sets of training, cross validation, and
testing data. We use the training data to set the model parameters, use the cross validation data to
learn an epsilon value, and finally use the testing data to get precision, recall, and F1 scores that
indicate how well the model detects anomalies in data not used for training and cross validation.
I present the example program as one long listing, with more code explanation after the listing.
Please note the long loop over each input training example starting at line 28 and ending on line 74.
The code in lines 25 through 44 processes the input training data sample into three disjoint sets of
training, cross validation, and testing data. Then the code in lines 45 through 63 copies these three
sets of data to Java arrays.
Anomaly Detection Machine Learning Example
The code in lines 65 through 73 calculates, for a training example, the value of µ (the varible mu in
the code).
Please note in the code example that I prepend class variables used in methods with “this.” even
when it is not required. I do this for legibility and is a personal style.
The following UML class diagram will give you an overview before we dive into the code:
UML class diagram for the anomaly detection example
package com.markwatson.anomaly_detection;
import java.util.ArrayList;
import java.util.List;
* Created by markw on 10/7/15.
public class AnomalyDetection {
public AnomalyDetection() { }
* AnomalyDetection is a general purpose class for building anomaly detection
* models. You should use this type of mdel when you have mostly negative
* training examples with relatively few positive examples and you need a model
* that detects postive (anomaly) inputs.
* @param num_features
Anomaly Detection Machine Learning Example
* @param num_training_examples
* @param training_examples [num_training_examples][num_features]
public AnomalyDetection(int num_features, int num_training_examples,
double [][] training_examples) {
List<double[]> training = new ArrayList<>();
List<double []> cross_validation = new ArrayList<>();
List<double []> testing = new ArrayList<>();
int outcome_index = num_features - 1; // index of target outcome
for (int i=0; i<num_training_examples; i++) {
if (Math.random() < 0.6) { // 60% training
// Only keep normal (negative) examples in training, except
// remember the reason for using this algorithm is that it
// works with many more negative examples than positive
// examples, and that the algorithm works even with some positive
// examples mixed into the training set. The random test is to
// allow about 10% positive examples to get into the training set:
if (training_examples[i][outcome_index] < 0.5 || Math.random() < 0.1)
training.add(training_examples[i]);
} else if (Math.random() < 0.7) {
cross_validation.add(training_examples[i]);
} else {
testing.add(training_examples[i]);
this.num_training_examples = training.size();
this.num_cross_validation_examples = cross_validation.size();
this.num_testing_examples = testing.size();
this.training_examples = new double[this.num_training_examples][];
for (int k=0; k<this.num_training_examples; k++) {
this.training_examples[k] = training.get(k);
this.cross_validation_examples = new double[num_cross_validation_examples][];
for (int k=0; k<num_cross_validation_examples; k++) {
this.cross_validation_examples[k] = cross_validation.get(k);
this.testing_examples = new double[num_testing_examples][];
for (int k=0; k<num_testing_examples; k++) {
// dimensions of [num_training_examples][num_features]:
this.testing_examples[k] = testing.get(k);
Anomaly Detection Machine Learning Example
this.mu = new double[num_features];
this.sigma_squared = new double[num_features];
this.num_features = num_features;
for (int nf = 0; nf < this.num_features; nf++) { //
double sum = 0;
for (int nt = 0; nt < this.num_training_examples; nt++)
sum += training_examples[nt][nf];
this.mu[nf] = sum / this.num_training_examples;
public double [] muValues()
{ return mu; }
public double [] sigmaSquared() { return sigma_squared; }
public double bestEpsilon()
{ return best_epsilon; }
* Train the model using a range of epsilon values. Epsilon is a
* hyper parameter that we want to find a value that
* minimizes the error count.
public void train() {
double best_error_count = 1e10;
for (int epsilon_loop=0; epsilon_loop<40; epsilon_loop++) {
double epsilon = 0.05 + 0.01 * epsilon_loop;
double error_count = train(epsilon);
if (error_count < best_error_count) {
best_error_count = error_count;
best_epsilon = epsilon;
System.out.println("\n**** Best epsilon value = " + best_epsilon );
// retrain for the best epsilon setting the maximum likelyhood parameters
// which are now in epsilon, mu[] and sigma_squared[]:
train_helper(best_epsilon);
// finally, we are ready to see how the model performs with test data:
test(best_epsilon);
Anomaly Detection Machine Learning Example
* calculate probability p(x; mu, sigma_squared)
* @param x - example vector
* @return
private double p(double [] x) {
double sum = 0;
// use (num_features - 1) to skip target output:
for (int nf=0; nf<num_features - 1; nf++) {
sum += (1.0 / (SQRT_2_PI * sigma_squared[nf])) *
Math.exp(- (x[nf] - mu[nf]) * (x[nf] - mu[nf]));
return sum / num_features;
* returns 1 if input vector is an anoomaly
* @param x - example vector
* @return
public boolean isAnamoly(double [] x) {
double sum = 0;
// use (num_features - 1) to skip target output:
for (int nf=0; nf<num_features - 1; nf++) {
sum += (1.0 / (SQRT_2_PI * sigma_squared[nf])) *
Math.exp(- (x[nf] - mu[nf]) * (x[nf] - mu[nf]));
return (sum / num_features) < best_epsilon;
private double train_helper_(double epsilon) {
// use (num_features - 1) to skip target output:
for (int nf = 0; nf < this.num_features - 1; nf++) {
double sum = 0;
for (int nt=0; nt<this.num_training_examples; nt++) {
sum += (this.training_examples[nt][nf] - mu[nf]) *
(this.training_examples[nt][nf] - mu[nf]);
sigma_squared[nf] = (1.0 / num_features) * sum;
double cross_validation_error_count = 0;
Anomaly Detection Machine Learning Example
for (int nt=0; nt<this.num_cross_validation_examples; nt++) {
double[] x = this.cross_validation_examples[nt];
double calculated_value = p(x);
if (x[9] > 0.5) { // target training output is ANOMALY
// if the calculated value is greater than epsilon
// then this x vector is not an anomaly (e.g., it
// is a normal sample):
if (calculated_value > epsilon) cross_validation_error_count += 1;
} else { // target training output is NORMAL
if (calculated_value < epsilon) cross_validation_error_count += 1;
System.out.println("
cross_validation_error_count = " +
cross_validation_error_count +
" for epsilon = " + epsilon);
return cross_validation_error_count;
private double test(double epsilon) {
double num_false_positives = 0, num_false_negatives = 0;
double num_true_positives = 0, num_true_negatives = 0;
for (int nt=0; nt<this.num_testing_examples; nt++) {
double[] x = this.testing_examples[nt];
double calculated_value = p(x);
if (x[9] > 0.5) { // target training output is ANOMALY
if (calculated_value > epsilon) num_false_negatives++;
num_true_positives++;
} else { // target training output is NORMAL
if (calculated_value < epsilon) num_false_positives++;
num_true_negatives++;
double precision = num_true_positives /
(num_true_positives + num_false_positives);
double recall = num_true_positives /
(num_true_positives + num_false_negatives);
double F1 = 2 * precision * recall / (precision + recall);
System.out.println("\n\n -- best epsilon = " + this.best_epsilon);
System.out.println(" -- number of test examples = " +
this.num_testing_examples);
System.out.println(" -- number of false positives = " + num_false_positives);
System.out.println(" -- number of true positives = " + num_true_positives);
Anomaly Detection Machine Learning Example
System.out.println(" -- number of false negatives = " + num_false_negatives);
System.out.println(" -- number of true negatives = " + num_true_negatives);
System.out.println(" -- precision = " + precision);
System.out.println(" -- recall = " + recall);
System.out.println(" -- F1 = " + F1);
return F1;
double best_epsilon = 0.02;
private double [] mu;
// [num_features]
private double [] sigma_squared; // [num_features]
private int num_features;
private static double SQRT_2_PI = 2.50662827463;
private double[][] training_examples; // [num_features][num_training_examples]
// [num_features][num_training_examples]:
private double[][] cross_validation_examples;
private double[][] testing_examples; // [num_features][num_training_examples]
private int num_training_examples;
private int num_cross_validation_examples;
private int num_testing_examples;
Once the training data and the values of µ (the varible mu in the code) are defined for each feature
we can define the method train in lines 86 through 104 that calculated the best epsilon “cutoff”
value for the training data set using the method train_helper defined in lines 138 through 165. We
use the “best” epsilon value by testing with the separate cross validation data set; we do this by
calling the method test that is defined in lines 167 through 198.
Example Using the University of Wisconsin Cancer
The example in this section loads the University of Wisconsin data and uses the class AnomalyDe-
tection developed in the last section to find anomalies, which for this example will be input vectors
that represented malignancy in the original data.
Anomaly Detection Machine Learning Example
package com.markwatson.anomaly_detection;
import java.io.*;
import org.apache.commons.io.FileUtils;
* Train a deep belief network on the University of Wisconsin Cancer Data Set.
public class WisconsinAnomalyDetection {
private static boolean PRINT_HISTOGRAMS = true;
public static void main(String[] args) throws Exception {
String [] lines =
FileUtils.readFileToString(
new File("data/cleaned_wisconsin_cancer_data.csv")).split("\n");
double [][] training_data = new double[lines.length][];
int line_count = 0;
for (String line : lines) {
String [] sarr = line.split(",");
double [] xs = new double[10];
for (int i=0; i<10; i++) xs[i] = Double.parseDouble(sarr[i]);
for (int i=0; i<9; i++) xs[i] *= 0.1;
xs[9] = (xs[9] - 2) * 0.5; // make target output be [0,1] instead of [2,4]
training_data[line_count++] = xs;
if (PRINT_HISTOGRAMS) {
PrintHistogram.historam("Clump Thickness", training_data, 0, 0.0, 1.0, 10);
PrintHistogram.historam("Uniformity of Cell Size", training_data,
1, 0.0, 1.0, 10);
PrintHistogram.historam("Uniformity of Cell Shape", training_data,
2, 0.0, 1.0, 10);
PrintHistogram.historam("Marginal Adhesion", training_data,
3, 0.0, 1.0, 10);
PrintHistogram.historam("Single Epithelial Cell Size", training_data,
4, 0.0, 1.0, 10);
PrintHistogram.historam("Bare Nuclei", training_data, 5, 0.0, 1.0, 10);
PrintHistogram.historam("Bland Chromatin", training_data, 6, 0.0, 1.0, 10);
PrintHistogram.historam("Normal Nucleoli", training_data, 7, 0.0, 1.0, 10);
PrintHistogram.historam("Mitoses", training_data, 8, 0.0, 1.0, 10);
Anomaly Detection Machine Learning Example
AnomalyDetection detector = new AnomalyDetection(10, line_count - 1,
training_data);
// the train method will print training results like
// precision, recall, and F1:
detector.train();
// get best model parameters:
double best_epsilon = detector.bestEpsilon();
double [] mean_values = detector.muValues();
double [] sigma_squared = detector.sigmaSquared();
// to use this model, us the method AnomalyDetection.isAnamoly(double []):
double [] test_malignant = new double[] {0.5,1,1,0.8,0.5,0.5,0.7,1,0.1};
double [] test_benign = new double[] {0.5,0.4,0.5,0.1,0.8,0.1,0.3,0.6,0.1};
boolean malignant_result = detector.isAnamoly(test_malignant);
boolean benign_result = detector.isAnamoly(test_benign);
System.out.println("\n\nUsing the trained model:");
System.out.println("malignant result = " + malignant_result +
", benign result = " + benign_result);
Data used by an anomaly detecton model should have (roughly) a Gaussian (bell curve shape)
distribution. What form does the cancer data have? Unfortunately, each of the data features seems
to either have a greater density at the lower range of feature values or large density at the extremes
of the data feature ranges. This will cause our model to not perform as well as we would like. Here
are the inputs displayed as five-bin histograms:
Clump Thickness
Uniformity of Cell Size
Anomaly Detection Machine Learning Example
Uniformity of Cell Shape
Marginal Adhesion
Single Epithelial Cell Size
Bare Nuclei
Bland Chromatin
Normal Nucleoli
Anomaly Detection Machine Learning Example
Mitoses
I won’t do it in this example, but the feature “Bare Nuclei” should be removed because it is not even
close to being a bell-shaped distribution. Another thing that you can do (recommended by Andrew
Ng in his Coursera Machine Learning class) is to take the log of data and otherwise transform it to
something that looks more like a Gaussian distribution. In the class WisconsinAnomalyDetection,
you could for example, transform the data using something like:
// make the data look more like a Gaussian (bell curve shaped) distribution:
double min = 1.e6, max = -1.e6;
for (int i=0; i<9; i++) {
xs[i] = Math.log(xs[i] + 1.2);
if (xs[i] < min) min = xs[i];
if (xs[i] > max) max = xs[i];
for (int i=0; i<9; i++) xs[i] = (xs[i] - min) / (max - min);
The constant 1.2 in line 4 is a tuning parameter that I got by trial and error by iterating on adjusting
the factor and looking at the data histograms.
In a real application you would drop features that you can not transform to something like a Gaussian
distribution.
Here are the results of running the code as it is in the github repository for this book:
-- best epsilon = 0.28
-- number of test examples = 63
-- number of false positives = 0.0
-- number of true positives = 11.0
-- number of false negatives = 8.0
-- number of true negatives = 44.0
-- precision = 1.0
-- recall = 0.5789473684210527
-- F1 = 0.7333333333333334
Using the trained model:
malignant result = true, benign result = false
Anomaly Detection Machine Learning Example
How do we evaluate these results? The precision value of 1.0 means that there were no false positives.
False positives are predictions of a true result when it should have been false. The value 0.578 for
recall means that of all the samples that should have been classified as positive, we only predicted
about 57.8% of them. The F1 score is calculated as two times the product of precision and recall,
divided by the sum of precision plus recall.
Genetic Algorithms
When searching a large space with many dimensions, greedy search algorithms find locally good
results that are often much worse than the best possible solutions. Genetic Algorithms (GAs) are
an efficient means of finding near optimum solutions by more uniformly exploring a large many-
dimensional search space. Genetic algorithms are a type of heuristic search where the heuristic takes
the form of combining a fitness function with efficient search techniques inspired by biology.
Genetic algorithms can be said to be inspired by biology because they deal with mutation, crossover,
and selection. Each possible solution to a problem is encoded in a chromosome that represents a
point in a many-dimensional search space.
GA computer simulations evolve a population of chromosomes that may contain at least some fit
individuals. Fitness is specified by a fitness function that is used to rate each individual in the
population (of chromosomes) and makes it possible to use selection to choose the best candidate
chromosomes to mutate and/or do crossover operations, or save as-is for the next generation. We
make copies of the selected chromosomes and slightly perturb the copies with random mutations.
Furthermore, pairs of selected chromosomes are cut in the same random gene index and cut pieces
of the pair of chromosomes are swapped (a process called crossover).
Setting up a GA simulation is fairly easy: we need to represent (or encode) the state of a system in a
chromosome that is usually implemented as a set of bits. GA is basically a search operation: searching
for a good solution to a problem where the solution is a very fit chromosome. The programming
technique of using GA is useful for AI systems that must adapt to changing conditions because “re-
programming” can be as simple as defining a new fitness function and re-running the simulation.
An advantage of GA is that the search process will not often “get stuck” in local minimum because
the genetic crossover process produces radically different chromosomes in new generations while
occasional mutations (flipping a random bit in a chromosome) cause small changes.
As you can imagine, performing a crossover operation moves to a very distant point in the search
space. Alternatively mutating a single bit only moves a point (i.e., a chromosome) in one dimension
in the search space.
Another aspect of GA is supporting the evolutionary concept of “survival of the fittest”: by using
the fitness function we will preferentially “breed” chromosomes with higher fitness values.
It is interesting to compare how GAs are trained with how we train neural networks (see the next
chapter on Neural Networks). We need to manually “supervise” the training process: for GAs we
need to supply a fitness function. For of the neural network models used in the chapter Neural
Networks we supply training data with desired sample outputs for sample inputs.
Genetic Algorithms
Theory
GAs are typically used to search very large and usually very high dimensional search spaces. If we
want to find a solution as a single point in an N dimensional space where a fitness function has a
near maximum value, then we have N parameters to encode in each chromosome. In this chapter
we will be solving a simple problem in which we only need to encode a single number (a floating
point number for this example) in each chromosome. We are effectively downsampling a floating
point number to an integer and the bit representation of the integer is a chromosome. Using a GA
toolkit like the one developed in a later section, requires two problem-specific customizations:
• Characterize the search space by a set of parameters that can be encoded in a chromosome
(more on this later). GAs work with the coding of a parameter set, not the parameters them-
selves (Genetic Algorithms in Search, Optimization, and Machine Learning, David Goldberg,
1989).
• Provide a numeric fitness function that allows us to rate the fitness of each chromosome in
a population. We will use these fitness values in the selection process to determine which
chromosomes in the population are most likely to survive and reproduce using genetic
crossover and mutation operations.
The GA toolkit developed in this chapter treats genes as a single bit; while you can consider a gene
to be an arbitrary data structure, the approach of using single bit genes and specifying the number of
genes (or bits) in a chromosome is very flexible. A population is a set of chromosomes. A generation
is defined as one reproductive cycle of replacing some elements of the chromosome population with
new chromosomes produced by using a genetic crossover operation followed by optionally mutating
a few chromosomes in the population.
We will describe a simple example problem (that can be better solved using Newton’s method) in
this section, write a general purpose library in the section Java Library for Genetic Algorithms, and
finish the chapter in the section Java Genetic Algorithm Example by solving this problem.
Example Function
Genetic Algorithms
For a sample problem, let’s suppose that we want to find the maximum value of the function F with
one independent variable x in the following equation and as seen in last figure:
F(x) = sin(x) * sin(0.4 * x) * sin(3 * x)
The problem that we want to solve is finding a good value of x to find a near to possible maximum
value of F(x). To be clear: we encode a floating point number as a chromosome made up of a specific
number of bits so any chromosome with randomly set bits will represent some random number in
the interval [0, 10]. The fitness function is simply the function in the last equation.
Crosssover Operation
This figure shows an example of a crossover operation that we will implement later in the program
example. A random chromosome bit index is chosen, and two chromosomes are “cut” at this index
and swap cut parts. The two original chromosomes in generation_n are shown on the left of the
figure and after the crossover operation they produce two new chromosomes in generation n + 1
where n is the current generation number. The two new chromosomes are shown on the right of the
figure.
In addition to using crossover operations to create new chromosomes from existing chromosomes,
we will also perform genetic mutation by randomly flipping bits in chromosomes. A fitness function
that rates the fitness value of each chromosome allows us to decide which chromosomes to discard
and which to use for the next generation. We will use the most fit chromosomes in the population
for producing the next generation using crossover and mutation.
We will implement a general purpose Java GA library in the next section and then solve the example
problem posed at the end of this chapter in the GA Example Section.
Java Library for Genetic Algorithms
The full implementation of the GA library is in the Java source file Genetic.java. The following code
snippets show the method signatures defining the public API for the library. Note that there are two
constructors, the first using default values for the fraction of chromosomes on which to perform
crossover and mutation operations and the second constructor allows setting explicit values for
these parameters:
Genetic Algorithms
abstract public class Genetic {
public Genetic(int num_genes_per_chromosome,
int num_chromosomes)
public Genetic(int num_genes_per_chromosome,
int num_chromosomes,
float crossover_fraction,
float mutation_fraction)
The method sort is used to sort the population of chromosomes in most fit first order. The methods
getGene and setGene are used to fetch and change the value of any gene (bit) in any chromosome.
These methods are protected because you may need to override them in derived classes.
protected void sort()
protected boolean getGene(int chromosome,
int gene)
protected void setGene(int chromosome,
int gene, int value)
protected void setGene(int chromosome,
int gene,
boolean value)
The methods evolve, doCrossovers, doMutations, and doRemoveDuplicates are utilities for
running GA simulations. These methods are protected but you will probably not need to override
them in derived classes.
protected void evolve()
protected void doCrossovers()
protected void doMutations()
protected void doRemoveDuplicates()
When you subclass class Genetic you must implement the following abstract method calcFitness
that will determine the evolution of chromosomes during the GA simulation.
// Implement the following method in sub-classes:
abstract public void calcFitness();
The class Chromosome represents an ordered bit sequence with a specified number of bits and a
floating point fitness value.
Genetic Algorithms
class Chromosome {
private Chromosome()
public Chromosome(int num_genes)
public boolean getBit(int index)
public void setBit(int index, boolean value)
public float getFitness()
public void setFitness(float value)
public boolean equals(Chromosome c)
The class ChromosomeComparator implements a Comparator interface and is application specific.
It is used to sort a population in “best first” order:
class ChromosomeComparator
implements Comparator<Chromosome> {
public int compare(Chromosome o1,
Chromosome o2)
The last class ChromosomeComparator is used when using the Java Collection class static sort
method.
The class Genetic is an abstract class: you must subclass it and implement the method calcFitness
that uses an application specific fitness function (that you must supply) to set a fitness value for each
chromosome.
The following UML class diagram provides an overview of the Java classes and their public APIs
as well as the class MyGenetic that will implement a fitness function for our example of finding a
maximum value in an equation and the test class TestGenetic:
Genetic Algorithms
UML Class Diagram for library and test program
This GA library provides the following behavior:
• Generates an initial random population with a specified number of bits (or genes) per
chromosome and a specified number of chromosomes in the population
• Ability to evaluate each chromosome based on a numeric fitness function
• Ability to create new chromosomes from the most fit chromosomes in the population using
the genetic crossover and mutation operations
There are two class constructors for Genetic set up a new GA experiment. Both constructors
Genetic Algorithms
require the number of genes (or bits) per chromosome, and the number of chromosomes in the
population. The second constructor allows you to optionally set the fractions for mutation and
crossover operations.
The Genetic class constructors build an array of integers rouletteWheel which is used to weight
the most fit chromosomes in the population for choosing the parents of crossover and mutation
operations. When a chromosome is being chosen, a random integer is selected to be used as
an index into the rouletteWheel array; the values in the array are all integer indices into the
chromosome array. More fit chromosomes are heavily weighted in favor of being chosen as parents
for the crossover operations. The algorithm for the crossover operation is fairly simple; here is the
implementation:
public void doCrossovers() {
int num = (int)(numChromosomes * crossoverFraction);
for (int i = num - 1; i >= 0; i--) {
// Don't overwrite the "best" chromosome
// from current generation:
int c1 = 1 + (int) ((rouletteWheelSize - 1) *
Math.random() * 0.9999f);
int c2 = 1 + (int) ((rouletteWheelSize - 1) *
Math.random() * 0.9999f);
c1 = rouletteWheel[c1];
c2 = rouletteWheel[c2];
if (c1 != c2) {
int locus = 1+(int)((numGenesPerChromosome-2) *
Math.random());
for (int g = 0; g<numGenesPerChromosome; g++) {
if (g < locus) {
setGene(i, g, getGene(c1, g));
} else {
setGene(i, g, getGene(c2, g));
The method doMutations is similar to doCrossovers: we randomly choose chromosomes from the
population and for these selected chromosomes we randomly “flip” the value of one gene (a gene is
a bit in our implementation):
Genetic Algorithms
public void doMutations() {
int num = (int)(numChromosomes * mutationFraction);
for (int i = 0; i < num; i++) {
// Don't overwrite the "best" chromosome
// from current generation:
int c = 1 + (int) ((numChromosomes - 1) *
Math.random() * 0.99);
int g = (int) (numGenesPerChromosome *
Math.random() * 0.99);
setGene(c, g, !getGene(c, g));
We developed a general purpose library in this section for simulating populations of chromosomes
that can evolve to a more “fit” population given a fitness function that ranks individual chromosomes
in order of fitness. In the next section we will develop an example GA application by defining the
size of a population and the fitness function that we saw earlier.
Finding the Maximum Value of a Function
We will use the Java library in the last section to develop an example application to find the
maximum of the function seen in the figure showing the sample function which shows a plot of
our test function we are using a GA to fit, plotted in the interval [0, 10].
While we could find the maximum value of this function by using Newton’s method (or even a
simple brute force search over the range of the independent variable x), the GA method scales very
well to similar problems of higher dimensionality. The GA also helps us to find better than locally
optimum solutions. In this example we are working in one dimension so we only need to encode a
single variable in a chromosome. As an example of a 20-dimensional space, we might have a financial
model with 20 independent variables x1, x2, ..x20 and a single chromosome would still represent
a point in this 20-dimensional space. To continue this example, if we used 10 bits to represent the
value range in each of the 20 dimensions, then the chromosome would be represented as 200 bits.
To generalize, our first task is to characterize the search space as one or more parameters. In general
when we write GA applications we might need to encode several parameters in a single chromosome.
As another example, if a fitness function has three arguments we would encode three numbers in a
single chromosome.
Let’s get back to our 1-dimensional example seen in the figure showing the sample function. This
is a simple example showing you how to set up a GA simulation. In this example problem we have
only one parameter, the independent variable x. We will encode the parameter x using ten bits (so
we have ten 1-bit genes per chromosome). A good starting place is writing a utility method for
converting the 10-bit representation to a floating-point number in the range [0.0, 10.0]:
Genetic Algorithms
float geneToFloat(int chromosomeIndex) {
int base = 1;
float x = 0;
for (int j=0; j<numGenesPerChromosome; j++)
if (getGene(chromosomeIndex, j)) {
x += base;
base *= 2;
For each bit at index j with a value of 1, add 2j to the sum x. We need to normalize this sum x that
is an integer in the range of [0,1023] to a floating point number in the approximate range of [0, 10]:
x /= 102.4f;
return x;
Note that we do not need the reverse method! We use our GA library from the last section to
create a population of 10-bit chromosomes. In order to evaluate the fitness of each chromosome
in a population, we only have to convert the 10-bit representation to a floating-point number for
evaluation using the fitness function we showed earlier (figure showing the sample function):
private float fitness(float x) {
return (float)(Math.sin(x) *
Math.sin(0.4f * x) *
Math.sin(3.0f * x));
The following table shows some sample random chromosomes and the floating point numbers that
they encode. The first column shows the gene indices where the bit is “on,” the second column
shows the chromosomes as an integer number represented in binary notation, and the third column
shows the floating point number that the chromosome encodes. Note that the center column in the
following table shows the bits in order where index 0 is the left-most bit, and index 9 is the right-
most bit; this is the reverse of the normal order for encoding integers but the GA does not care, it
works with any encoding we use as long as it is consistent.
Genetic Algorithms
“On bits” in chromosome
As binary
Number encoded
-----------------------
---------
--------------
0010010111
9.1015625
1101011000
1.0449219
1001011110
4.7753906
Using methods geneToFloat and fitness we now implement the abstract method calcFitness from
our GA library class Genetic so the derived class TestGenetic is not abstract. This method has
the responsibility for calculating and setting the fitness value for every chromosome stored in an
instance of class Genetic:
public void calcFitness() {
for (int i=0; i<numChromosomes; i++) {
float x = geneToFloat(i);
chromosomes.get(i).setFitness(fitness(x));
While it was useful to make this example more clear with a separate geneToFloat method, it would
have also been reasonable to simply place the formula in the method fitness in the implementation
of the abstract (in the base class) method calcFitness.
In any case we are done with coding this example. You can compile the two example Java files
Genetic.java and TestGenetic.java, and run the TestGenetic class to verify that the example
program quickly finds a near maximum value for this function. The project Makefile has a single
target that builds the library and runs the example test program:
mvn install
mvn exec:java -Dexec.mainClass="com.markwatson.geneticalgorithm.TestGenetic"
You can try setting different numbers of chromosomes in the population and try setting non-default
crossover rates of 0.85 and a mutation rates of 0.3. We will look at a run with a small number of
chromosomes in the population created with:
Genetic Algorithms
genetic_experiment =
new MyGenetic(10, 20, 0.85f, 0.3f);
int NUM_CYCLES = 500;
for (int i=0; i<NUM_CYCLES; i++) {
genetic_experiment.evolve();
if ((i%(NUM_CYCLES/5))==0 || i==(NUM_CYCLES-1)) {
System.out.println("Generation " + i);
genetic_experiment.print();
In this experiment 85% of chromosomes will be “sliced and diced” with a crossover operation and
30% will have one of their genes changed. We specified 10 bits per chromosome and a population
size of 20 chromosomes. In this example, I have run 500 evolutionary cycles. After you determine
a fitness function to use, you will probably need to experiment with the size of the population
and the crossover and mutation rates. Since the simulation uses random numbers (and is thus non-
deterministic), you can get different results by simply rerunning the simulation. Here is example
program output (with much of the output removed for brevity):
count of slots in roulette wheel=55
Generation 0
Fitness for chromosome 0 is 0.505, occurs at x=7.960
Fitness for chromosome 1 is 0.461, occurs at x=3.945
Fitness for chromosome 2 is 0.374, occurs at x=7.211
Fitness for chromosome 3 is 0.304, occurs at x=3.929
Fitness for chromosome 4 is 0.231, occurs at x=5.375
Fitness for chromosome 18 is -0.282 occurs at x=1.265
Fitness for chromosome 19 is -0.495, occurs at x=5.281
Average fitness=0.090 and best fitness for this
generation:0.505
Generation 499
Fitness for chromosome 0 is 0.561, occurs at x=3.812
Fitness for chromosome 1 is 0.559, occurs at x=3.703
This example is simple but is intended to show you how to encode parameters for a problem where
you want to search for values to maximize a fitness function that you specify. Using the library
developed in this chapter you should be able to set up and run a GA simulation for your own
applications.
The important takeaway is that if you can encode a problem space as a chromosome and you have
Genetic Algorithms
a fitness function to rate the numerical effectiveness of a chromosome, then Genetic Algorithms are
an effective alternative to greedy search algorithms.
Neural Networks
In a sense training neural networks is a search problem. Given training data and using gradient
descent we search for a set of parameters (the weights in a neural network) that model the training
data. A dataset is defined by the features in the dataset and the data samples. If we consider our
search space to have a number of dimensions equal to the number of features then we can at
least imagine that the training samples are an extremely sparse scattering of points in this high-
dimensional space. These training data are a distribution of the possible points in this space. Since
input features are often correlated we would not expect an even distribution of samples over this
space. We refer to the training samples as being “in distribution” and new data that we later process
with our trained model to be “out of distribution.” New “out of distribution” data may sometimes
represent areas of the space with no training examples nearby and we would expect our model to
sometimes perform poorly on this data.
Neural networks can be used to efficiently solve many problems that are intractable or difficult using
other AI programming techniques. In the late 1980s I spent almost two years on a DARPA neural
network tools advisory panel, wrote the ANSim neural network product, and have used neural
networks for a wide range of application problems (radar interpretation, bomb detection, and as
controllers in computer games). Mastering the use of neural networks will allow you to solve many
types of problems that are very difficult to solve using other methods.
Gradient descent is a general term for moving in a direction in a space that is “down hill” towards a
local minimum. Here I write an implementation for a backpropagation neural network from scratch
(pure Java code, no libraries). The material in this chapter is meant to provide you with an intuition
of how backpropagation works while the next chapter covers a robust deep learning library that you
should use in for modeling large datasets. Backpropagation is a type of gradient descent that works
by changing weight values (the parameters in a neural network model) in the direction of reducing
the error between what our model predicts for a set of input values and what the target output is in
training data.
In the next chapter Deep Learning Using Deeplearning4j we will use a popular Java deep learning
library that can efficiently process neural networks with more complex architectures and larger
numbers of simulated neurons. These “Deep Learning” neural networks use computational tricks to
efficiently train more complex Backpropagation networks.
Although most of this book is intended to provide practical advice (with some theoretical back-
ground) on using AI programming techniques, I cannot imagine being interested in practical AI
programming without also thinking about the philosophy and mechanics of how the human mind
works. I hope that my readers share this interest.
The physical structure and dynamics of the human brain are inherently parallel and distributed
[Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Rumelhart, Mc-
Clelland, etc. 1986]. We are experts at doing many things at once. For example, I simultaneously can
Neural Networks
walk, talk with my wife, keep our puppy out of cactus, and enjoy the scenery behind our house in
Sedona, Arizona. AI software systems struggle to perform even narrowly defined tasks well, so how
is it that we are able to simultaneously perform several complex tasks? There is no clear or certain
answer to this question at this time, but certainly the distributed neural architecture of our brains
is a requirement for our abilities. Classical artificial neural network simulations (like the ones we
study in this chapter) do not address “multi-tasking” (other techniques that do address this issue
are multi-agent systems with some form or mediation between agents). Deep learning models can
have multiple types of inputs, different types of outputs, and complex internal structure - basically
connected models that are trained jointly. While deep learning models trained jointly on different
types of input data break through the limitations of simple backpropagation models, the simple
backpropagation models we look at now are still very practical today for a range of problems.
Also interesting is the distinction between instinctual behavior and learned behavior. Our knowledge
of GAs from the chapter on Genetic Algorithms provides a clue to how the brains of lower
order animals can be hardwired to provide efficient instinctual behavior under the pressures of
evolutionary forces (i.e., likely survival of more fit individuals). This works by using genetic
algorithms to design specific neural wiring. I have used genetic algorithms to evolve recurrent neural
networks for control applications (I wrote about this in C++ Power Paradigms, 1995, McGraw-Hill).
This work had only partial success but did convince me that biological genetic pressure is probably
adequate to “pre-wire” some forms of behavior in natural (biological) neural networks.
While we will study supervised learning techniques in this chapter, it is possible to evolve both
structure and attributes of neural networks using other types of neural network models like Adaptive
Resonance Theory (ART) to autonomously learn to classify learning examples without intervention.
We will start this chapter by discussing human neuron cells and which features of real neurons that
we will model. Unfortunately, we do not yet understand all of the biochemical processes that occur in
neurons, but there are fairly accurate models available (web search “neuron biochemical”). Neurons
are surrounded by thin hair-like structures called dendrites which serve to accept activation from
other neurons. Neurons sum up activation from their dendrites and each neuron has a threshold
value; if the activation summed over all incoming dendrites exceeds this threshold, then the neuron
fires, spreading its activation to other neurons. Dendrites are very localized around a neuron. Output
from a neuron is carried by an axon, which is thicker than dendrites and potentially much longer
than dendrites in order to affect remote neurons. The following figure shows the physical structure
of a neuron; in general, the neuron’s axon would be much longer than is seen in this figure. The
axon terminal buttons transfer activation to the dendrites of neurons that are close to the individual
button. An individual neuron is connected to up to ten thousand other neurons in this way.
Neural Networks
Neuron
The activation absorbed through dendrites is summed together, but the firing of a neuron only occurs
when a threshold is passed. In neural network simulations there are several common ways to model
neurons and connections between neurons that we will see in both this and the next chapter.
Road Map for the Neural Network Example Code
The following UML class diagram will give you an overview all of the neural network library classes
in this chapter before we dive into the code:
Neural Networks
UML class diagram for neural network code
There are three parts to the code base: main backpropagation library, GUI examples, and text-only
tests. The following screen show of the project open in an IDE is useful to see the file layout for the
project:
Neural Networks
IDE view of code showing main library, GUI examples, and text-only tests
Backpropagation Neural Networks
The neural network model that we use is called backpropagation, also known as back-prop or delta
rule learning. In this model, neurons are organized into data structures that we call layers. The figure
Backpropagation network with No Hidden Layer shows a simple neural network with two layers;
this network is shown in two different views: just the neurons organized as two one-dimensional
arrays, and as two one-dimensional arrays with the connections between the neurons. In our model,
there is a connection between two neurons that is characterized by a single floating-point number
that we will call the connection’s weight. A weight W_{i,j} connects input neuron i to output neuron
j. In the back propagation model, we always assume that a neuron is connected to every neuron in
the previous layer.
A key feature of back-prop neural networks is that they can be efficiently trained. Training is
performed by calculating sets of weights for connecting each layer. As we will see, we will train
networks by applying input values to the input layer, allowing these values to propagate through
the network using the current weight values, and calculating the errors between desired output
values and the output values from propagation of input values through the network.
The errors at the output layer are used to calculate gradients (or corrections) to the weights feeding
Neural Networks
into the output layer. Gradients are back propagated through the network allowing all weights in
the network to be updated to reduce errors visible at the output layer.
One limitation of early back propagation neural networks is that they are limited to the number
of neuron layers that can be efficiently trained. More recent advances in deep learning have
mostly solved this problem: as error gradients are back propagated through the network toward
the input layer, the gradients get smaller and smaller. If these gradients vanish because they can’t
be represented as a floating point number then weight updates will be zero and the model is not
trained. Another effect of many hidden layers is that it can take a lot of time to train back propagation
networks. This problem has also mostly been solved with modern deep learning libraries as seen in
the next chapter Deep Learning.
Initially, weights are set to small random values. You will get a general idea for how this is done
in this section and then we will look at Java implementation code in the section for a Java Class
Library for Backpropagation.
In the figure showing a Backpropagation network with No Hidden Layer, we have only two neuron
layers, one for the input neurons and one for the output neurons. Networks with no hidden layers
are not generally useful - I am using the network in the figure showing a Backpropagation network
with No Hidden Layer just to demonstrate layer to layer connections through a weights array.
Example Backpropagation network with No Hidden Layer
To calculate the activation of the first output neuron O1, we evaluate the sum of the products of
the input neurons times the appropriate weight values; this sum is input to a Sigmoid activation
function (see the figure showing the Sigmoid Function) and the result is the new activation value
for O1. Here is the formula for the simple network in the figure showing a Backpropagation network
with No Hidden Layer:
Neural Networks
O1 = Sigmoid (I1 * W[1,1] + I2 * W[2,1])
O2 = Sigmoid (I2 * W[1,2] + I2 * W[2,2])
The figure showing the Sigmoid Function shows a plot of the Sigmoid function and the derivative of
the sigmoid function (SigmoidP). We will use the derivative of the Sigmoid function when training
a neural network (with at least one hidden neuron layer) with classified data examples.
Sigmoid Function and Derivative of Sigmoid Function (SigmoidP)
A neural network like the one seen in the figure showing a Backpropagation network with No
Hidden Layer is trained by using a set of training data. For back propagation networks, training
data consists of matched sets of input with matching desired output values. We want to train a
network to not only produce similar outputs for training data inputs as appear in the training data,
but also to generalize its pattern matching ability based on the training data to be able to match test
patterns that are similar to training input patterns. A key here is to balance the size of the network
against how much information it must hold. A common mistake when using back-prop networks is
to use too large a network: a network that contains too many neurons and connections will simply
memorize the training examples, including any noise in the training data. However, if we use a
smaller number of neurons with a very large number of training data examples, then we force the
network to generalize, ignoring noise in the training data and learning to recognize important traits
in input data while ignoring statistical noise.
How do we train a back propagation neural network given that we have a good training data set?
The algorithm is quite easy; we will now walk through the simple case of a two-layer network like
the one in the figure showing a Backpropagation network with No Hidden Layer. Later in the section
for a Java Class Library for Back Propagation we will review the algorithm in more detail when we
have either one or two hidden neuron layers between the input and output layers.
In order to train the network in the figure for a Backpropagation network with No Hidden Layer,
we repeat the following learning cycle several times:
1. Zero out temporary arrays for holding the error at each neuron. The error, starting at the
output layer, is the difference between the output value for a specific output layer neuron and
Neural Networks
the calculated value from setting the input layer neuron’s activation values to the input values
in the current training example, and letting activation spread through the network.
2. Update the weight W_{i,j} (where i is the index of an input neuron, and j is the index
of an output neuron) using the formula W_{i,j} += learning_rate * output_error_j*I_i
(learning_rate is a tunable parameter) and output_error_j was calculated in step 1, and I_i is
the activation of input neuron at index i.
This process is continued to either a maximum number of learning cycles or until the calculated
output errors get very small. We will see later that the algorithm is similar but slightly more
complicated when we have hidden neuron layers; the difference is that we will “back propagate”
output errors to the hidden layers in order to estimate errors for hidden neurons. We will cover more
on this later. This type of neural network is too simple to solve very many interesting problems, and
in practical applications we almost always use either one additional hidden neuron layer or two
additional hidden neuron layers. The figure showing mappings supported by zero hidden layer, one
hidden layer, and two hidden hidden layer networks shows the types of problems that can be solved
by networks with different numbers of hidden layers.
Mappings supported by 0, 1, and 2 hidden layer neural networks
A Java Class Library for Back Propagation
The back propagation neural network library used in this chapter was written to be easily understood
and is useful for many problems. However, one thing that is not in the implementation in this
section (it is added in the section Using Momentum to speed up training) is something usually called
“momentum” to speed up the training process at a cost of doubling the storage requirements for
weights. Adding a “momentum” term not only makes learning faster but also increases the chances
of successfully learning more difficult problems.
We will concentrate in this section on implementing a back-prop learning algorithm that works for
both one and two hidden layer networks. As we saw in the Figure showing mappings supported by
zero hidden layer, one hidden layer, and two hidden layer networks, a network with two hidden
Neural Networks
layers is capable of arbitrary mappings of input to output values. It used to be a common (and
incorrect) opinion that there was no theoretical reason for using networks with three hidden layers.
With recent projects using Deep Learning as I mentioned at the beginning of this chapter, neural
networks with many hidden layers are now common practice. Here is one of the example programs
that helps visualize a 1\one hidden layer network:
Example showing 1 hidden layer
With each layer having three neurons, the weight matrices are 3 x 3 arrays and are easy to display.
When you run the examples you can see the weight matches changing in time. Here is another
example program that adds an additional hidden layer:
Neural Networks
Example showing 2 hidden layers
Please note, dear reader, that simple networks like these examples are explainable in the sense that
you can understand how they work after they are trained because they have so few parameters.
We can characterize the complexity of models as the number of layers and the total number of
connection weights. We might say that the one hidden layer model has 9 parameters and the two
hidden layer model has 18 parameters (i.e., the total number of weights that we must learn effective
values for). In general large neural models, especially deep learning models, are not explainable.
There are some applications that are required by law to be explainable for which neural networks
are not appropriate to use. Another issue is fairness of models if they are trained on biased data.
The relevant files for the back propagation examples are:
• Neural_1H.java - contains a class for simulating a neural network with one hidden neuron
• Test_1H.java - a text-based test program for the class Neural_1H
• GUITest_1H.java - a GUI-based test program for the class Neural_1H
• Neural_2H.java - contains a class for simulating a neural network with two hidden neuron
layers
• Neural_2H_momentum.java - contains a class for simulating a neural network with two
hidden neuron layers and implements momentum learning (implemented in the section Using
Momentum to speed up training
• Test_2H.java - a text-based test program for the class Neural_2H
• GUITest_2H.java - a GUI-based test program for the class Neural_2H
• GUITest_2H_momentum.java - a GUI-based test program for the class Neural_2H_momentum
that uses momentum learning (implemented in the section Using Momentum to speed up
training
Neural Networks
• Plot1DPanel - a Java JFC graphics panel for the values of a one-dimensional array of floating
point values
• Plot2DPanel - a Java JFC graphics panel for the values of a two-dimensional array of floating
point values
The GUI files are for demonstration purposes only, and we will not discuss the code for these classes;
if you are interested in the demo graphics code and do not know JFC Java programming, there are
a few good JFC tutorials at the web.
It is common to implement back-prop libraries to handle either zero, one, or two hidden layers in
the same code base. At the risk of having to repeat similar code in two different classes, I decided to
make the Neural_1H and Neural_2H classes distinct. I think that this makes the code a little easier
to understand. As a practical point, you will almost always start solving a neural network problem
using only one hidden layer and only progress to trying two hidden layers if you can’t train a one
hidden layer network to solve the problem at-hand with sufficiently small error when tested with
data that is different from the original training data. Networks with only one hidden layer require
less storage space and run faster in simulation than two hidden layer networks.
In this section we will look only at the implementation of the class Neural_2H (class Neural_-
1H is simpler and when you understand how Neural_2H works, the simpler class is also easy to
understand). This class implements the Serializable interface and contains a utility method save to
write a trained network to a disk file:
class Neural_2H implements Serializable {
There is a static factory method that reads a saved network file from disk and builds an instance of
Neural_2H and there is a class constructor that builds a new untrained network in memory, given
the number of neurons in each layer:
public static Neural_2H Factory(String serialized_file_name)
public Neural_2H(int num_in,
int num_hidden1,
int num_hidden2, int num_output)
An instance of Neural_2H contains training data as transient data that is not saved by method save.
transient protected ArrayList inputTraining = new Vector();
transient protected ArrayList outputTraining = new Vector();
I want the training examples to be native float arrays so I used generic ArrayList containers. You
will usually need to experiment with training parameters in order to solve difficult problems. The
learning rate not only controls how large the weight corrections we make each learning cycle but
this parameter also affects whether we can break out of local minimum. Other parameters that
Neural Networks
affect learning are the ranges of initial random weight values that are hardwired in the method
randomizeWeights() and the small random values that we add to weights during the training cycles;
these values are set in in slightlyRandomizeWeights(). I usually only need to adjust the learning
rate when training back-prop networks:
public float TRAINING_RATE = 0.5f;
I often decrease the learning rate during training - that is, I start with a large learning rate and
gradually reduce it during training. The calculation for output neuron values given a set of inputs
and the current weight values is simple. I placed the code for calculating a forward pass through the
network in a separate method forwardPass() because it is also used later in the method training:
public float[] recall(float[] in) {
for (int i = 0; i < numInputs; i++) inputs[i] = in[i];
forwardPass();
float[] ret = new float[numOutputs];
for (int i = 0; i < numOutputs; i++) ret[i] = outputs[i];
return ret;
public void forwardPass() {
for (int h = 0; h < numHidden1; h++) {
hidden1[h] = 0.0f;
for (int h = 0; h < numHidden2; h++) {
hidden2[h] = 0.0f;
for (int i = 0; i < numInputs; i++) {
for (int h = 0; h < numHidden1; h++) {
hidden1[h] += inputs[i] * W1[i][h];
for (int i = 0; i < numHidden1; i++) {
for (int h = 0; h < numHidden2; h++) {
hidden2[h] += hidden1[i] * W2[i][h];
for (int o = 0; o < numOutputs; o++) outputs[o] = 0.0f;
for (int h = 0; h < numHidden2; h++) {
for (int o = 0; o < numOutputs; o++) {
outputs[o] += sigmoid(hidden2[h]) * W3[h][o];
Neural Networks
While the code for recall and forwardPass is almost trivial, the training code in method train is
more complex and we will go through it in some detail. Before we get to the code, I want to mention
that there are two primary techniques for training back-prop networks. The technique that I use is
to update the weight arrays after each individual training example. The other technique is to sum
all output errors over the entire training set (or part of the training set) and then calculate weight
updates. In the following discussion, I am going to weave my comments on the code into the listing.
The private member variable current_example is used to cycle through the training examples: one
training example is processed each time that the train method is called:
private int current_example = 0;
public float train(ArrayList ins, ArrayList v_outs) {
Before starting a training cycle for one example, we zero out the arrays used to hold the output
layer errors and the errors that are back propagated to the hidden layers. We also need to copy the
training example input values and output values:
int i, h, o; float error = 0.0f;
int num_cases = ins.size();
for (int example=0; example<num_cases; example++) {
// zero out error arrays:
for (h = 0; h < numHidden1; h++) hidden1_errors[h] = 0.0f;
for (h = 0; h < numHidden2; h++) hidden2_errors[h] = 0.0f;
for (o = 0; o < numOutputs; o++) output_errors[o] = 0.0f;
// copy the input values:
for (i = 0; i < numInputs; i++) {
inputs[i] = ((float[]) ins.get(current_example))[i];
// copy the output values:
float[] outs = (float[]) v_outs.get(current_example);
We need to propagate the training example input values through the hidden layers to the output
layers. We use the current values of the weights:
forwardPass();
After propagating the input values to the output layer, we need to calculate the output error for each
output neuron. This error is the difference between the desired output and the calculated output;
this difference is multiplied by the value of the calculated output neuron value that is first modified
by the Sigmoid function that we saw in the figure showing the Sigmoid Function. The Sigmoid
function is to clamp the calculated output value to a reasonable range.
Neural Networks
for (o = 0; o < numOutputs; o++) {
output_errors[o] = (outs[o] - outputs[o]) * sigmoidP(outputs[o]);
The errors for the neuron activation values in the second hidden layer (the hidden layer connected to
the output layer) are estimated by summing for each hidden neuron its contribution to the errors of
the output layer neurons. The thing to notice is that if the connection weight value between hidden
neuron h and output neuron o is large, then hidden neuron h is contributing more to the error of
output neuron o than other neurons with smaller connecting weight values:
for (h = 0; h < numHidden2; h++) {
hidden2_errors[h] = 0.0f; for (o = 0; o < numOutputs; o++) {
hidden2_errors[h] += output_errors[o] * W3[h][o];
We estimate the errors in activation energy for the first hidden layer neurons by using the estimated
errors for the second hidden layers that we calculated in the last code snippet:
for (h = 0; h < numHidden1; h++) {
hidden1_errors[h] = 0.0f; for (o = 0; o < numHidden2; o++) {
hidden1_errors[h] += hidden2_errors[o] * W2[h][o];
After we have scaled estimates for the activation energy errors for both hidden layers we then want
to scale the error estimates using the derivative of the sigmoid function’s value of each hidden
neuron’s activation energy:
for (h = 0; h < numHidden2; h++) {
hidden2_errors[h] = hidden2_errors[h] * sigmoidP(hidden2[h]);
for (h = 0; h < numHidden1; h++) {
hidden1_errors[h] = hidden1_errors[h] * sigmoidP(hidden1[h]);
Now that we have estimates for the hidden layer neuron errors, we update the weights connecting
to the output layer and each hidden layer by adding the product of the current learning rate, the
estimated error of each weight’s target neuron, and the value of the weight’s source neuron:
Neural Networks
// update the hidden2 to output weights:
for (o = 0; o < numOutputs; o++) {
for (h = 0; h < numHidden2; h++) {
W3[h][o] += TRAINING_RATE * output_errors[o] * hidden2[h];
W3[h][o] = clampWeight(W3[h][o]);
// update the hidden1 to hidden2 weights:
for (o = 0; o < numHidden2; o++) {
for (h = 0; h < numHidden1; h++) {
W2[h][o] += TRAINING_RATE * hidden2_errors[o] * hidden1[h];
W2[h][o] = clampWeight(W2[h][o]);
// update the input to hidden1 weights:
for (h = 0; h < numHidden1; h++) {
for (i = 0; i < numInputs; i++) {
W1[i][h] += TRAINING_RATE * hidden1_errors[h] * inputs[i];
W1[i][h] = clampWeight(W1[i][h]);
for (o = 0; o < numOutputs; o++) {
error += Math.abs(outs[o] - outputs[o]);
The last step in this code snippet was to calculate an average error over all output neurons for this
training example. This is important so that we can track the training status in real time. For very
long running back-prop training experiments I like to be able to see this error graphed in real time
to help decide when to stop a training run. This allows me to experiment with the learning rate
initial value and see how fast it decays. The last thing that method train needs to do is to update
the training example counter so that the next example is used the next time that train is called:
current_example++;
if (current_example >= num_cases)
current_example = 0;
return error;
You can look at the implementation of the Swing GUI test class GUTest_2H to see how I decrease
the training rate during training. I also monitor the summed error rate over all output neurons and
occasionally randomize the weights if the network is not converging to a solution to the current
problem.
Neural Networks
Adding Momentum to Speed Up Back-Prop Training
We did not use a momentum term in the Java code in the section for a Java Class Library for Back
Propagation. For difficult to train problems, adding a momentum term can drastically reduce the
training time at a cost of doubling the weight storage requirements. To implement momentum, we
remember how much each weight was changed in the previous learning cycle and make the weight
change larger if the current change in “direction” is the same as the last learning cycle. For example,
if the change to weight W_{i,j} had a large positive value in the last learning cycle and the calculated
weight change for W_{i,j} is also a large positive value in the current learning cycle, then make the
current weight change even larger. Adding a “momentum” term not only makes learning faster but
also increases the chances of successfully learning more difficult problems.
I modified two of the classes from the section for a Java Class Library for Back Propagation to use
momentum:
• Neural_2H_momentum.java - training and recall for two hidden layer back-prop networks.
The constructor has an extra argument “alpha” that is a scaling factor for how much of the
previous cycle’s weight change to add to the new calculated delta weight values.
• GUITest_2H_momentum.java - a GUI test application that tests the new class Neural_2H_-
momentum.
The code for class Neural_2H_momentum is similar to the code for Neural_2H that we saw in
the last section so here we will just look at the differences. The class constructor now takes another
parameter alpha that determines how strong the momentum correction is when we modify weight
values:
// momentum scaling term that is applied
// to last delta weight:
private float alpha = 0.2f;
While this alpha term is used three times in the training code, it suffices to just look at one of
these uses in detail. When we allocated the three weight arrays W1, W2, and W3 we also now
allocate three additional arrays of corresponding same size: W1_last_delta, W2_last_delta, and
W3_last_delta. These three new arrays are used to store the weight changes for use in the next
training cycle. Here is the original code to update W3 from the last section:
W3[h][o] += TRAINING_RATE * output_errors[o] * hidden2[h];
The following code snippet shows the additions required to use momentum:
Neural Networks
W3[h][o] += TRAINING_RATE * output_errors[o] * hidden2[h] +
// apply the momentum term:
alpha * W3_last_delta[h][o];
W3_last_delta[h][o] = TRAINING_RATE * output_errors[o] * hidden2[h];
I mentioned in the last section that there are at least two techniques for training back-prop networks:
updating the weights after processing each training example or waiting to update weights until all
training examples are processed. I always use the first method when I don’t use momentum. In
many cases it is best to use the second method when using momentum. In the next chapter on Deep
Learning we use a third method for using training data: we choose randomly selected small batches
of training examples for each weight update cycle.
Wrap-up for Neural Networks
I hope that the material in this chapter has given you some low-level understanding of the
implementation of backpropagation neural networks. We will use a popular deep learning library in
the next chapter that in practice you should prefer to the pedantic code here. That said, I used very
similar C++ code to that developed here for several practical engineering problems in the 1980s and
early 1990s including the prediction code for a bomb detector my company made for the FAA.
Deep Learning Using Deeplearning4j
One limitation of back propagation neural networks seen in the last chapter is that they are limited
to the number of neuron layers that can be efficiently trained. If you experimented with the
sample back propagation code then you may have noticed that it took longer to train a network
with two hidden layers compared to the training time for a network with only one hidden layer.
There are also problems like vanishing gradients (the backpropagated errors that are used to update
connection weights) that occur in architectures with many layers. Deep learning uses computational
improvements to mitigate the vanishing gradient problem like using ReLu activation functions rather
than the more traditional Sigmoid function, and networks called “skip connections” networks where
some layers are initially turned off with connections skipping to the next active layer. After some
initial training the skipped layers are activated and become part of the model (as in ResNet50,
mentioned in the section Roadmap for the DL4J Model Zoo at the end of this chapter).
Digging deeper into the problem of vanishing gradients, the problem with back propagation
networks is that as error gradients are back propagated through the network toward the input
layer, the gradients get smaller and smaller. The effect is that it can take a lot of time to train back
propagation networks with many hidden layers. Even worse, the small backpropagated errors get
so small that they cause numerical underflows.
I became interested in deep learning neural networks when I took Geoffrey Hinton’s Neural Network
class (a Coursera class, taken summer of 2012) and then for the next seven years most of my
professional work involved deep learning. I have used GAN (generative adversarial networks)
models for synthesizing numeric spreadsheet data, LSTM (long short term memory) models to
synthesize highly structured text data like nested JSON, and for NLP (natural language processing).
Several of my 55 US patents use neural network and Deep Learning technology.
The Deeplearning4j.org¹¹ Java library supports many neural network algorithms including support
for Deep Learning (DL). Note that I will often refer to Deeplearning4j as DL4J.
We will first look at a simple example of a feed forward network using the same University of
Wisconsin cancer database that we used earlier. Deep learning refers to neural networks with many
layers, possibly with weights connecting neurons in non-adjacent layers which makes it possible to
model temporal and spacial patterns in data.
There is a separate repository of DL4J examples¹² that you should clone because the last half of this
chapter is a general discussion of running the DL4J examples and modifying them for your needs
with one additional example using LSTM models.
After the first simple example we then look at how to set up DL4J projects using Maven, and then
discuss other types of layer classes that you will likely use in your projects. After learning how to
¹¹http://deeplearning4j.org/
¹²https://github.com/eclipse/deeplearning4j-examples
Deep Learning Using Deeplearning4j
set up and use DL4J and having a roadmap of commonly used layer classes, then you will then be
set to work on your own projects.
Feed Forward Classification Networks
Feed forward classification networks are a type of deep neural network that can contain multiple
hidden neuron layers. In the example here the adjacent layers are fully connected (all neurons in
adjacent layers are connected), as in the examples from the last chapter. The difference here is the
use of the DL4J library that is written to scale to large problems and to use GPUs if you have them
available.
In general, simpler network architectures are better than unnecessarily complicated architectures.
You can start with simple architectures and add layers, different layer types, and parallel models as-
needed. For feed forward networks model complexity has two dimensions: the numbers of neurons
in hidden layers, and also the number of hidden layers. If you put too many neurons in hidden layers
then the training data is effectively memorized and this will hurt performance on data samples not
used in training. In practice, I “starve the network” by reducing the number of hidden neurons until
the model has reduced accuracy on independent test data. Then I slightly increase the number of
neurons in hidden layers. This technique helps avoid models simply memorizing training data.
Feed Forward Example
The following screen shot shows an IntelliJ project (you can use the free community or professional
version for the examples in this book) for the example in this chapter:
Deep Learning Using Deeplearning4j
IntelliJ project view for the examples in this chapter
The Deeplearning4j library can use user-written Java classes to import training and testing data into
a form that the Deeplearning4j library can use. Some of the examples at https://github.com/eclipse/deeplearning4j-
examples¹³ use custom data loaders but in this simple example we use built-in utilities for reading
spreadsheet data (see lines 46-56 in the following listing).
The class ClassifierWisconsinData reads the University of Wisconsin cancer training and testing
data sets, creates a model (lines 59-81), trains it (line 82) and tests it (lines 84-97). The value of the
variable numHidden set in line 3 refers to the number of neurons in each hidden layer.
You can increase the number of hidden units in line 36 (something that you might do for more
complex problems). To add a hidden layer you can repeat lines 66-71, and you would change the
layer indices (first argument) as appropriate in calls to the chained method .layer() so the layer
indices are all different and increasing in value.
¹³https://github.com/eclipse/deeplearning4j-examples
Deep Learning Using Deeplearning4j
package com.markwatson.deeplearning;
import org.datavec.api.records.reader.RecordReader;
import org.datavec.api.records.reader.impl.csv.CSVRecordReader;
import org.datavec.api.split.FileSplit;
import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.learning.config.Sgd;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.*;
* Train a feed forward classifier network on the University of Wisconsin Cancer Dat\
a Set.
public class ClassifierWisconsinData {
private static final Logger log =
LoggerFactory.getLogger(ClassifierWisconsinData.class);
public static void main(String[] args) throws Exception {
int numHidden = 3;
int numOutputs = 1;
int batchSize = 64;
int seed = 33117;
int numInputs = 9;
int labelIndex = 9;
Deep Learning Using Deeplearning4j
int numClasses = 2;
RecordReader recordReader = new CSVRecordReader();
recordReader.initialize(new FileSplit(new File("data/","training.csv")));
DataSetIterator trainIter =
new RecordReaderDataSetIterator(recordReader,batchSize,labelIndex,numClasses);
RecordReader recordReaderTest = new CSVRecordReader();
recordReaderTest.initialize(
new FileSplit(new File("data/","testing.csv")));
DataSetIterator testIter =
new RecordReaderDataSetIterator(recordReaderTest,batchSize,
labelIndex,numClasses);
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
.seed(seed) //use the same random seed
.activation(Activation.TANH)
.weightInit(WeightInit.XAVIER)
.updater(new Sgd(0.1))
.l2(1e-4)
.list()
.layer(0,
new DenseLayer.Builder()
.nIn(numInputs)
.nOut(numHidden)
.build()
.layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
.nIn(numHidden)
.nOut(numClasses)
.activation(Activation.SOFTMAX)
.build()
.build();
MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();
model.setListeners(new ScoreIterationListener(100));
model.fit( trainIter, 10 );
Evaluation eval = new Evaluation(numOutputs);
while (testIter.hasNext()) {
DataSet ds = testIter.next();
Deep Learning Using Deeplearning4j
INDArray features = ds.getFeatures();
System.out.println("Input features: " + features);
INDArray labels = ds.getLabels();
INDArray predicted = model.output(features,false);
System.out.println("Predicted output: "+ predicted);
System.out.println("Desired output: "+ labels);
eval.eval(labels, predicted);
System.out.println();
System.out.println("Evaluate model....");
System.out.println(eval.stats());
It is very important to not use training data for testing because performance on recognizing training
data should always be good assuming that you have enough memory capacity in a network (i.e.,
enough hidden units and enough neurons in each hidden layer).
The program output is (much output removed for brevity):
Input features: [[6.0000, 10.0000, 10.0000, 2.0000,
8.0000, 10.0000, 7.0000, 3.00\
00, 3.0000],
Predicted output: [
[0.1611, 0.8389],
Desired output: [
[0, 1.0000],
========================Evaluation Metrics========================
# of classes:
Accuracy:
0.8846
Precision:
0.9000
Recall:
0.8929
F1 Score:
0.8800
Precision, recall & F1: macro-averaged (equally weighted avg. of 2 classes)
=========================Confusion Matrix=========================
Deep Learning Using Deeplearning4j
-------
0 | 0 = 0
3 11 | 1 = 1
Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
The F1 score is calculated as twice precision times recall, divided by precision + recall. We would like
F1 to be as close to 1.0 as possible and it is common to spend a fair amount of time experimenting
with meta learning parameters to increase F1.
It is also fairly common to try to learn good values of meta learning parameters also. We won’t do
this here but the process involves splitting the data into three disjoint sets: training, validation, and
testing. The meta parameters are varied, training is performed, and using the validation data the
best set of meta parameters is selected. Finally, we test the network as defined by meta parameters
and learned weights for those meta parameters with the separate test data to see what the effective
F1 score is.
Configuring the Example Using Maven
There is a Maven pom.xml configuration file for this example that is configured for a recent version
of DL4J (as I write this in July 2020). DL4J is fairly good at detecting if the Open BLAS library is
available, if CUDA software support for any GPUs on your system are available, etc. If you try
running the Makefile and get any errors, then check the DL4J Quickstart and setup guide¹⁴ to see if
there are any dependencies that you need on your system. The Makefile has a single target:
deep_wisconsin:
mvn install
mvn exec:java \
-Dexec.mainClass="com.markwatson.deeplearning.ClassifierWisconsinData"
Documentation for Other Types of Deep Learning
Layers
The documentation for the built-in layer classes in DL4J¹⁵ is probably more than you need for now
so let’s review the most other types of layers that I sometimes use. In the simple example we used
in the last section we used two types of layers:
¹⁴https://deeplearning4j.konduit.ai/getting-started/quickstart
¹⁵https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/package-tree.html
Deep Learning Using Deeplearning4j
• org.deeplearning4j.nn.conf.layers.DenseLayer¹⁶ - maintains connections to all neurons in the
previous and next layer, or it is “fully connected.”
• org.deeplearning4j.nn.conf.layers.OutputLayer¹⁷ - has built-in behavior for starting the back
propagation calculations back through previous layers.
As you build more deep learning enabled applications, depending on what requirements you have,
you will likely need to use at least some of the following Dl4J layer classes:
• org.deeplearning4j.nn.conf.layers.AutoEncoder¹⁸ - often used to remove noise from data. Au-
toencoders work by making the target training output values equal to the input training values
while reducing the number of neurons in the AutoEncoding layer. The layer learns a concise
representation of data, or “generalizes” data by learning in which features are important.
• org.deeplearning4j.nn.conf.layers.CapsuleLayer¹⁹ - Capsule networks are an attempt to be
more efficient versions of convolutional models. Convolutional networks discard position
information of detected features while capsule models maintain and use this information.
• org.deeplearning4j.nn.conf.layers.Convolution1D²⁰ - one dimensional convolutional layers
learn one dimensional feature detectors. Trained layers learn to recognize features but discard
the information of where the feature is located. These are often used for data input streams
like signal data and word tokens in natural language processing.
• org.deeplearning4j.nn.conf.layers.Convolution2D²¹ - two dimensional convolutional layers
learn two dimensional feature detectors. Trained layers learn to recognize features but discard
the information of where the feature is located. These are often used for recognizing if a type
of object appears inside a picture. Note that features, for example representing a nose or a
mouth, are recognized but their location in an input picture does not matter. For example, you
could cut up an image of someone’s face, moving the ears to the picture center, the mouth to
the upper left corner, etc., and the picture would still be predicted to contain a face with some
probability because using soft max output layers produces class labels that can be interpreted
as probabilities since the values over all output classes sum to the value 1.
• org.deeplearning4j.nn.conf.layers.EmbeddingLayer²² - embedding layers are used to transform
input data into integer data. My most frequent use of embedding layers is word embedding
where each word in training data is assigned an integer value. This data can be “one hot
encoded” and in the case of processing words, if there are 5000 unique words in the training
data for a classifier, then the embedding layer would have 5001 neurons, one for each word and
one to represent all words not in the training data. If the word index (indexing is zero-based)
is, for example 117, then the activation value for neuron at index 117 is set to one and all others
in the layer are set to zero.
¹⁶https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/DenseLayer.html
¹⁷https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/OutputLayer.html
¹⁸https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/AutoEncoder.html
¹⁹https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/CapsuleLayer.html
²⁰https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Convolution1DLayer.html
²¹https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Convolution2D.html
²²https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/EmbeddingLayer.html
Deep Learning Using Deeplearning4j
• org.deeplearning4j.nn.conf.layers.FeedForwardLayer²³ - this is a super class for most special-
ized types of feed forward layers so reading through the class reference is recommended.
• org.deeplearning4j.nn.conf.layers.DropoutLayer²⁴ - dropout layers are very useful for prevent-
ing learning new input patterns from making the network forget previously learned patterns.
For each training batch, some fraction of neurons in a dropout layer are turned off and don’t
update their weights during a training batch cycle. The development of using dropout was key
historically for getting deep learning networks to work with many layers and large amounts
of training data.
• org.deeplearning4j.nn.conf.layers.LSTM²⁵ - LSTM layers are used to extend the temporal
memory of what a layer can remember. LSTM are a refinement of RNN models that use an
input window to pass through a data stream and the RNN model can only use what is inside
this temporal sampling window.
• org.deeplearning4j.nn.conf.layers.Pooling1D²⁶ - a one dimensional pooling layer transforms a
longer input to a shorter output by downsampling, i.e., there are fewer output connections than
input connections.
• org.deeplearning4j.nn.conf.layers.Pooling2D²⁷ - a two dimensional pooling layer transforms
a larger two dimensional array of data input to a smaller output two dimensional array by
downsampling.
Running the DL4J Example Programs and Modifying
Them For Your Use
To get started clone the DL4J examples repository written by the authors of DL4J (if you have not
already done so) and fetch all of the required libraries:
git clone https://github.com/eclipse/deeplearning4j-examples
cd deeplearning4j-examples/dl4j-examples
mvn install
In the next section we will modify Alex Black’s²⁸ character generating LSTM example for a different
application (modeling and generating CSV data). Before moving on to the example in the next section
you may want to run his example using:
²³https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/FeedForwardLayer.html
²⁴https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/DropoutLayer.html
²⁵https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/LSTM.html
²⁶https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Pooling1D.html
²⁷https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Pooling2D.html
²⁸https://github.com/AlexDBlack
Deep Learning Using Deeplearning4j
cd deeplearning4j-examples/dl4j-examples
mvn exec:java -Dexec.mainClass="org.deeplearning4j.examples.advanced.modelling.charm\
odelling.generatetext.GenerateTxtModel"
His example downloads the complete works of Shakespeare from the web and trains a recurrent
network LSTM model by passing an input window through the complete text. Each input character
is one-hot encoded and the target output is the same one-hot encoded text data in the input window
except the sample is shifted one character further in the input text. The model learns to predict the
next character in a sequence given a sample of input seed text.
Let’s review one-hot encoding. We need to convert each character in the training data to a one-hot
encoding which is a vector of all 0.0 values except for a single value of 1.0. If there are, for example,
256 unique characters in the training data the vector will have 256+1 elements because we add an
“unknown character” that represents characters the model may see in the future that are not in the
training data. For example if the index of character “m” is 77, then we set element at index 77 to
one, all other elements being zero. We won’t look at the implementation of one-hot encoding here
because DL4J provides APIs for this. I cover one-hot-encoding implementation in another book in
the chapter on Deep Learning for the Hy Language²⁹ that you can read online.
Here we will be using one-hot encoding for characters but it also works well for encoding words in
a vocabulary. Training data may typically have about 10,000 unique words so the one-hot encoding
vector would have 10,001 elements. We add a special word position for “unknown word.” For some
applications we also use word-embeddings using a smaller vector, 500 elements being a reasonable
size. We won’t be using word-embeddings here, or one-hot word embedding but I want you to know
what these terms mean.
By processing sufficient sample text, an LSTM network can learn to model the “language” of the
input text. If you train on samples from Shakespeare then the model generates text that looks like
Shakespeare wrote it. This also works for any author with a specific writing style. LSTM networks
are used to model programming languages and many types of structured information.
For a customer, I used an LSTM model trained on JSON log data from AWS. They wanted to have
a large amount of test data that did not contain any sensitive information so we generated “fake
data” in the correct schema. The LSTM model worked fairly well, the major restriction being that
I checked each generated JSON datum for having valid syntax and be valid to the Schema of the
original data, discarding samples that failed these tests.
The examples provided with DL4J cover most of the deep learning use cases you may require for
your work so I want you to be comfortable running all of the examples and knowing where to start
if you want to modify them for a different purposes. If you load the entire DL4J examples repository
in a Java IDE and use global search then you should be able to find appropriate CNN, Classification,
Regression, etc., examples similar to your current use case that you can modify.
As an experiment, I wanted to try using an LSTM character generation model to generate CSV style
spreadsheet data and you can see the implementation of this idea in next section where I modified
Alex Black’s character generating LSTM example.
²⁹https://leanpub.com/hy-lisp-python/read#leanpub-auto-deep-learning
Deep Learning Using Deeplearning4j
Modifying the Character Generating LSTM Example to
Model and Generate CSV Spreadsheet Data
Here are the major changes the I made to the GenerateTxtModel example written by Alex
Black. I start by preparing the text training data where I substitute the original method that
uses the complete works of William Shakespeare in the class getShakespeareIterator with a
new class getWisconsinDataIterator that processes the CSV file in data/training.csv in class
LstmCharGenerator:
static CharacterIterator getWisconsinDataIterator(int miniBatchSize,
int sequenceLength)
throws IOException {
String fileLocation = "data/training.csv";
char[] validCharacters = {'0', '1', '2', '3', '4', '5', '6', '7', '8',
'9', ',', '\n'};
System.out.println("++ valid characters in training data: ");
for (char ch : validCharacters) System.out.print(" " + ch);
System.out.println();
return new CharacterIterator(fileLocation, StandardCharsets.UTF_8,
miniBatchSize, sequenceLength, validCharacters, new Random(12345));
Changes for configuring the model:
int lstmLayerSize = 400; //Number of units in each LSTM layer
int miniBatchSize = 16; //Size of mini batch to use when
training
int exampleLength = 250;; //Length of each training example sequence to use.
int tbpttLength = 40; //Length for truncated backpropagation through time
int numEpochs = 100; //Total number of training epochs
int generateSamplesEveryNMinibatches = 5; //How frequently to generate samples
int nSamplesToGenerate = 20; //Number of samples to generate each training epoch
int nCharactersToSample = 300; //Length of each sample to generate
I made a third change to Alex Black’s example: I discarded generated samples that didn’t match the
schema of the original data in the method sampleCharactersFromNetwork:
Deep Learning Using Deeplearning4j
System.out.println("----- Samples -----");
for( int j=0; j<samples.length; j++ ) {
// discard samples that don't contain 10 numbers per line
String [] lines = samples[j].split("\n");
for (int k=0; k<lines.length; k++) {
if (StringUtils.countMatches(lines[k], ",") == 9 &&
lines[k].split(",").length == 10) {
System.out.println(lines[k]);
We will look at three samples taken about two minutes into the training process, after ten minutes,
and finally after twenty minutes. You might want to look at the training file data/training.csv to
understand what we are trying to model and then generate similar data.
Here is sample output after training the model for about two minutes (most lines are not properly
formatted or valid):
0,4,000,2,5,1,1,5,8,0
,,,,61,,,,,8
,,,6014,0,1,1,1,1,0
1,4,,3,1,2,2,5,3,0
1,,7,,,,60,1,0,7
1,2,6,1,2,1,3,1,1,6
0,5,1,2,3,2,1,,000000,8
8,1,5,1,,8,1,0,1,0
2,1,2,6,1,1,2,8,0,2
The next sample shows output after training the model for a ten minutes. There are errors in lines
1 and 4. Line 1 has a value of “16” in the row that is outside the allowed data range. Line 4 has the
value “13” that is also outside the allowed data range.
1,1,1,1,2,16,7,1,1,1
6,1,1,1,2,1,3,1,1,0
6,1,1,1,2,1,1,1,1,0
13,10,4,10,3,10,10,7,1,1
1,1,1,1,2,1,1,1,1,0
2,1,1,1,2,1,1,1,1,0
Final model after training for twenty minutes:
Deep Learning Using Deeplearning4j
1,1,1,1,1,1,1,1,1,0
5,1,1,1,2,1,2,1,1,0
8,8,6,10,2,10,7,10,1,1
5,1,1,1,2,1,1,1,1,0
2,1,4,3,2,1,1,1,1,0
1,1,1,1,1,1,1,1,1,0
1,1,1,1,1,1,3,1,1,0
4,1,1,1,3,1,1,10,2,1
This example should convince you, dear reader, that the language modeling capabilities of LSTM
models is surprising and effective.
You can modify this example to try modeling other types of test. You might try modeling a large
sample of Python or Java source code, or text in any language you might know like German, Farsi,
Hebrew, or Spanish. For languages like Farsi and Hebrew that read from right to left you would need
to either use a Bidirectional LSTM or change the program to stream input “backwards” for each line
so a standard LSTM could correctly predict characters in the order that a human reader processes.
There are interesting papers on using LSTM models to analyze design specifications that feeds into
convolutional layers that generate images for design documents. Is this process perfect? No, but still
impressive and provides some intuition into what may be possible in the future.
Roadmap for the DL4J Model Zoo
DL4J supports a Model Zoo containing the following pre-trained models your own projects (see
documentation³⁰):
• AlexNet - was a breakthrough for image recognition, AlexNet is a convolutional neural
network designed by Alex Krizhevsky (with Ilya Sutskever and Geoffrey Hinton). AlexNet
used Relu instead of arc-tangent or Sigmoid activation.
• Darknet19 - is a type of realtime YOLO model.
• FaceNetNN4Small2 - is a small version of the FaceNet embeddings for face recognition.
• InceptionResNetV1 - Inception models use many convolutional layers.
• LeNet - of historical interest, a convolutional model design by Yann LeCun and his colleagues
(1998).
• NASNet - like Inception and Xception models, with claimed superior results.
• ResNet50 - residual neural network that uses “skip connections” that connect neurons in
non-adjacent layers which helps reduce the vanishing gradient problem for models with
many layers. Skipped layers are connected later in the training process (example class
AlphaGoZeroTrainer).
• SimpleCNN - simple architecture using alternating pooling and convolutional layers.
³⁰https://deeplearning4j.konduit.ai/model-zoo/zoo-models
Deep Learning Using Deeplearning4j
• SqueezeNet - architecture for computer vision that experiments with smaller neural networks
with fewer parameters.
• TextGenerationLSTM - a general model architecture for using LSTM layers for building
language models from input text and then generating new similar text.
• TinyYOLO - a small YOLO model (example class *TinyYoloHouseNumberDetection** demon-
strates Transfer Learning).
• UNet - convolutional model designed to perform medical image segmentation.
• VGG16 - convolutional model for classification and detection using convolutional (with ReLu),
max pooling, and fully connected layers (example class FitFromFeaturized demonstrates
Transfer Learning).
• VGG19 - a larger variant of VGG16 that uses (16 convolution layers, 3 fully connected layers,
5 max pooling layers and 1 SoftMax layer.
• Xception - a newer version of Inception V3 with slightly better performance but long training
times.
• YOLO2 - “you only look once” real time object detection.
If you are interested in fine tuning existing models (Transfer Learning) then I suggest that you start
with the example class FitFromFeaturized. You can look at the code using your favorite editor and
run this example using:
$ cd deeplearning4j-examples/dl4j-examples
$ emacs src/main/java/org/deeplearning4j/examples/advanced/features/transferlearning\
/editlastlayer/presave/FitFromFeaturized.java
$ mvn exec:java -Dexec.mainClass="org.deeplearning4j.examples.advanced.features.tran\
sferlearning.editlastlayer.presave.FitFromFeaturized"
This is a good example to get started with because it is short (about 90 lines of code) and shows
clearly how to take a saved model and build a configuration to add your own output layer and then
perform additional training using your own data.
Deep Learning Wrapup
I first used complex neural network topologies in the late 1980s for phoneme (speech) recognition,
specifically using time delay neural networks and I gave a talk about it at IEEE First Annual
International Conference on Neural Networks San Diego, California June 21-24, 1987³¹. In the
following year I wrote the Backpropagation neural network code that my company used in a bomb
detector that we built for the FAA. Back then, neural networks were not widely accepted but in
the present time Google, Microsoft, and many other companies are using deep learning for a wide
³¹http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=4307059
Deep Learning Using Deeplearning4j
range of practical problems. Exciting work is also being done in the field of natural language
processing. The examples in this chapter are simple so you can experiment with them easily. I
wanted to introduce you to Deeplearning4j³² because I think it is probably the easiest way for Java
developers to get started working with many layered neural networks and I refer you to the project
documentation³³.
I managed a deep learning team at Capital One (2018-2019) and while there much of my work in-
volved GANs and LSTM deep models (and several of my 55 US patents were at least partially inspired
by this work). I refer you to a good GAN DL4J example on the web https://github.com/wmeddie/dl4j-
gans³⁴ and a tutorial on LSTM applications from the developers of DL4J https://deeplearning4j.konduit.ai/getting-
started/tutorials/clinical-time-series-lstm³⁵.
Deep Learning has become a standard tool for modeling data and making predictions or classifying
data. Most of the online classes on Deep Learning use Python. DL4J can import Keras/TensorFlow
models so one strategy is for you to build models using Python and import trained models into DL4J.
³²http://deeplearning4j.org/
³³http://deeplearning4j.org/documentation.html
³⁴https://github.com/wmeddie/dl4j-gans
³⁵https://deeplearning4j.konduit.ai/getting-started/tutorials/clinical-time-series-lstm
Natural Language Processing
I have been working in the field of Natural Language Processing (NLP) since 1982. In this chapter
we will use a few of my open source NLP project. In the next chapter I have selected the open source
NLP project OpenNLP³⁶ to provide more examples of using NLP to get you started using NLP in
your own projects. For my current work I usually use a combination of my own library discussed
in this chapter, OpenNLP (covered in the next chapter), and two deep learning NLP libraries for the
Python language (spaCy³⁷ and Hugging Face³⁸ pre-trained deep learning models). Wile I don’t cover
these Python libraries in this book, I do have examples of using spaCy in my Hy Language book³⁹.
Hy is a Lisp language that is implemented in Python. For difficult NLP problems like coreference
resolution (or anaphora resolution) I use deep learning models like BERT.
Deep learning is apparently “eating” the AI world but I firmly believe that hybrid systems stand
the best chance of getting us to real artificial general intelligence (AGI) - time will tell as more NLP,
knowledge representation, reasoning, etc., tasks are implemented in hybrid systems. Many experts in
AI believe that deep learning only takes us so far, and in order to reach general artificial intelligence
we will use some form of hybrid deep learning, symbolic AI, and probabalistic systems. That said,
there are deep learning specialists who predict their favored technology will probably be sufficient
to get to AGI.
Overview of the NLP Library and Running the
Examples
We will cover a wide variety of techniques for processing text in this chapter. The part of speech
tagger (POS), text categorization, and entity extraction examples are all derived from either my open
source projects or my commercial projects that I developed in the 1990-2010 time frame.
The following UML class diagrams will give you an overview of my NLP library code:
³⁶https://opennlp.apache.org/
³⁷https://spacy.io/
³⁸https://huggingface.co/
³⁹https://leanpub.com/hy-lisp-python/
Natural Language Processing
UML class diagram for top level NLP code
The XML parsing code is for reading the file test_data/classification_tags.xml that contains ranked
word terms for various categories we cover (e.g., politics, economy, etc.), often referred to as term
frequency–inverse document frequency (tf-idf⁴⁰).
UML class diagram for low-level utilities
Each main class in this library has a main method that provides a short demonstration of using the
class. The Makefile has targets for running the main method for each of the top level classes:
⁴⁰https://en.wikipedia.org/wiki/Tf%E2%80%93idf
Natural Language Processing
names:
mvn install -DskipTests
mvn exec:java -Dexec.mainClass="com.markwatson.nlp.ExtractNames"
autotagger:
mvn install -DskipTests
mvn exec:java -Dexec.mainClass="com.markwatson.nlp.AutoTagger"
fasttag:
mvn install -DskipTests
mvn exec:java -Dexec.mainClass="com.markwatson.nlp.FastTag"
You might find it useful to run the examples before we look at the code.
Tokenizing, Stemming, and Part of Speech Tagging
Tokenizing text is the process of splitting a string containing text into individual tokens. Stemming
is the reduction of words to abbreviated word roots that allow for easy comparison for equality
of similar words. Tagging is identifying what part of speech each word is in input text. Tagging is
complicated by many words having different parts of speech depending on context (examples: “bank
the airplane,” “the river bank,” etc.) You can find the code in this section in the GitHub repository
in the files src/com/markwatson/nlp/fasttag/FastTag.java and src/com/markwatson/nlp/util/-
Tokenizer.java. The required data files are in the directory test_data in the files lexicon.txt (for
processing English text) and lexicon_medpost.txt (for processing medical text).
We will also look at a public domain word stemmer that I frequently use in this section.
Before we can process any text we need to break text into individual tokens. Tokens can be words,
numbers and punctuation symbols. The class Tokenizer has two static methods, both take an input
string to tokenize and return a list of token strings. The second method has an extra argument to
specify the maximum number of tokens that you want returned:
static public List<String> wordsToList(String s)
static public List<String> wordsToList(String s, int maxR)
In line 2, maxR is maximum number of tokens to return and is useful when you want to sample the
first part of a very long text.
The following listing shows a fragment of example code using this class with the output:
Natural Language Processing
String text =
"The ball, rolling quickly, went down the hill.";
List<String> tokens = Tokenizer.wordsToList(text);
System.out.println(text);
for (String token : tokens)
System.out.print("\""+token+"\" ");
System.out.println();
This code fragment produces the following output:
The ball, rolling quickly, went down the hill.
"The" "ball" "," "rolling" "quickly" "," "went"
"down" "the" "hill" "."
For many applications, it is better to “stem” word tokens to simplify comparison of similar words.
For example “run,” “runs,” and “running” all stem to “run.” The stemmer that we will use, which I
believe to be in the public domain, is in the file src/public_domain/Stemmer.java. There are two
convenient APIs defined at the end of the class, one to stem a string of multiple words and one to
stem a single word token:
public List<String> stemString(String str)
public String stemOneWord(String word)
My FastTag part of speech (POS) tagger project resulted from my using in the early 1990s the
excellent tagger written by Eric Brill while he was at the University of Pennsylvania. He used
machine learning techniques to learn transition rules for tagging text using manually tagged text as
training examples. In reading through his doctoral thesis I noticed that there were a few transition
rules that covered most of the cases and I implemented a simple “fast tagger” in Common Lisp, Ruby,
Scheme and Java. The Java version is in the file src/com/markwatson/nlp/fasttag/FastTag.java.
The file src/com/markwatson/nlp/fasttag/README.txt contains information on where to obtain
Eric Brill’s original tagging system and also defines the tags for both his English language lexicon and
the Medpost lexicon. The following table shows the most commonly used tags (see the README.txt
file for a complete description).
Natural Language Processing
singular noun
plural noun
singular proper noun
California
conjunction
and, but, or
determiner
the, some
preposition
of, in, by
adjective
large, small, green
proper pronoun
I, he, you
adverb
slowly
comparative adverb
slowest
past participle verb
gerund verb
eating
wh\* pronoun
who, what
wh\* determiner
which, that
Brill’s system worked by processing manually tagged text and then creating a list of words followed
by the tags found for each word. Here are a few random lines selected from the test_data/lexicon.txt
Arco NNP
Arctic NNP JJ
fair JJ NN RB
Here “Arco” is a proper noun because it is the name of a corporation. The word “Arctic” can be either
a proper noun or an adjective; it is used most frequently as a proper noun so the tag “NNP” is listed
before “JJ.” The word “fair” can be an adjective, singular noun, or an adverb.
The class Tagger reads the file lexicon either as a resource stream (if, for example, you put
lexicon.txt **in the same JAR file as the compiled **Tagger and Tokenizer class files) or as
a local file. Each line in the lexicon.txt file is passed through the utility method parseLine that
processes an input string using the first token in the line as a hash key and places the remaining
tokens in an array that is the hash value. So, we would process the line “fair JJ NN RB” as a hash
key of “fair” and the hash value would be the array of strings (only the first value is currently used
but I keep the other values for future use).
When the tagger is processing a list of word tokens, it looks each token up in the hash table and stores
the first possible tag type for the word. In our example, the word “fair” would be assigned (possibly
temporarily) the tag “JJ.” We now have a list of word tokens and an associated list of possible tag
types. We now loop through all of the word tokens applying eight transition rules that Eric Brill’s
system learned. We will look at the first rule in some detail; i is the loop variable in the range [0,
number of word tokens - 1] and word is the current word at index i:
Natural Language Processing
rule 1: DT, {VBD | VBP} --> DT, NN
if (i > 0 && ret.get(i - 1).equals("DT")) {
if (word.equals("VBD") ||
word.equals("VBP") ||
word.equals("VB")) {
ret.set(i, "NN");
In English, this rule states that if a determiner (DT) at word token index i - 1 is followed by either a
past tense verb (VBD) or a present tense verb (VBP) then replace the tag type at index i with “NN.”
I list the remaining seven rules in a short syntax here and you can look at the Java source code to
see how they are implemented:
rule 2: convert a noun to a number (CD) if "."
appears in the word
rule 3: convert a noun to a past participle if
words.get(i) ends with "ed"
rule 4: convert any type to adverb if it ends in "ly"
rule 5: convert a common noun (NN or NNS) to an
adjective if it ends with "al"
rule 6: convert a noun to a verb if the preceding
work is "would"
rule 7: if a word has been categorized as a common
anoun nd it ends with "s", then set its type
to plural common noun (NNS)
rule 8: convert a common noun to a present participle
verb (i.e., a gerund)
My FastTag tagger is not quite as accurate as Brill’s original tagger so you might want to use his
system written in C but which can be executed from Java as an external process or with a JNI
interface.
In the next section we will use the tokenizer, stemmer, and tagger from this section to develop a
system for identifying named entities in text.
Named Entity Extraction From Text
In this section we will look at identifying names of people and places in text. This can be
useful for automatically tagging news articles with the people and place names that occur in
the articles. The “secret sauce” for identifying names and places in text is the data in the file
test_data/propername.ser – a serialized Java data file containing hash tables for human and place
Natural Language Processing
names. This data is read in the constructor for the class Names; it is worthwhile looking at the code
if you have not used the Java serialization APIs before:
ObjectInputStream p = new ObjectInputStream(ins);
Hashtable lastNameHash = (Hashtable) p.readObject();
Hashtable firstNameHash = (Hashtable) p.readObject();
Hashtable placeNameHash = (Hashtable) p.readObject();
Hashtable prefixHash = (Hashtable) p.readObject();
If you want to see these data values, use code like
Enumeration keysE = placeNameHash.keys();
while (keysE.hasMoreElements()) {
Object key = keysE.nextElement();
System.out.println(key + " : " +
placeNameHash.get(key));
The small part of the output from running this code snippet is:
Mauritius : country
Port-Vila : country_capital
Hutchinson : us_city
Mississippi : us_state
Lithuania : country
Before we look at the entity extraction code and how it works, we will first look at an example of
using the main APIs for the Names class. The following example uses the methods isPlaceName,
isHumanName, and getProperNames:
System.out.println("Los Angeles: " +
names.isPlaceName("Los Angeles"));
System.out.println("President Bush: " +
names.isHumanName("President Bush"));
System.out.println("President George Bush: " +
names.isHumanName("President George Bush"));
System.out.println("President George W. Bush: " +
names.isHumanName("President George W. Bush"));
ScoredList[] ret = names.getProperNames(
"George Bush played golf. President
George W. Bush went to London England, \
and Mexico to see Mary
Natural Language Processing
Smith in Moscow. President Bush will
return home Monday.");
System.out.println("Human names: " +
ret[0].getValuesAsString());
System.out.println("Place names: " +
ret[1].getValuesAsString());
The output from running this example is:
Los Angeles: true
President Bush: true
President George Bush: true
President George W. Bush: true
* place name: London,
placeNameHash.get(name): country_capital
* place name: Mexico,
placeNameHash.get(name): country_capital
* place name: Moscow,
placeNameHash.get(name): country_capital
Human names: George Bush:1,
President George W . Bush:1,
Mary Smith:1,
President Bush:1
Place names: London:1, Mexico:1, Moscow:1
The complete implementation that you can read through in the source file ExtractNames.java is
reasonably simple. The methods isHumanName and isPlaceName simply look up a string in either
of the human or place name hash tables. For testing a single word this is very easy; for example:
public boolean isPlaceName(String name) {
return placeNameHash.get(name) != null;
The versions of these APIs that handle names containing multiple words are just a little more
complicated; we need to construct a string from the words between the starting and ending indices
and test to see if this new string value is a valid key in the human names or place names hash tables.
Here is the code for finding multi-word place names:
Natural Language Processing
public boolean isPlaceName(List<String> words,
int startIndex,
int numWords) {
if ((startIndex + numWords) > words.size()) {
return false;
if (numWords == 1) {
return isPlaceName(words.get(startIndex));
String s = "";
for (int i=startIndex;
i<(startIndex + numWords); i++) {
if (i < (startIndex + numWords - 1)) {
s = s + words.get(startIndex) + " ";
} else {
s = s + words.get(startIndex);
return isPlaceName(s);
This same scheme is used to test for multi-word human names. The top-level utility method
getProperNames is used to find human and place names in text. The code in getProperNames
is intentionally easy to understand but not very efficient because of all of the temporary test strings
that need to be constructed.
Automatically Assigning Categories to Text
Here we will assign zero or more categories like “politics”, “economy”, etc. to text based on the words
contained in the text. While the code for doing this is simple there is usually much work to do to
build a word count database for different classifications. The approach we use here is often called
“bag of words” because the words in input text matter but not the order of words in text or proximity
to other words.
I have been working on open source products for automatic tagging and semantic extraction since
the 1990s (see my old web site www.knowledgebooks.com if you are interested). In this section I will
show you some simple techniques for automatically assigning tags or categories to text. We will use
a set of category tags for which I have collected word frequency statistics. For example, a category
of “Java” might be associated with the use of the words “Java,” “JVM,” “Sun,” etc. You can find my
pre-trained tag data in the file:
Natural Language Processing
test_data/classification_tags.xml
The Java source code for the class AutoTagger is in the file:
src-statistical-nlp/com/markwatson/nlp/AutoTagger.java
The AutoTagger class uses a few data structures to keep track of both the names of categories (tags)
and the word count statistics for words associated with each tag name. I use a temporary hash table
for processing the XML input data:
private static Hashtable<String, Hashtable<String, Float>> tagClasses;
The names of categories (tags) used are defined in the XML tag data file: change this file, and you
alter both the tags and behavior of this utility class. Please note that the data in this XML file is from
a small set of hand-labeled text found on the Web (i.e., my wife and I labelled articles as being about
“religion”, “politics”, etc.).
This approach is called “bag of words.” The following listing shows a snippet of data defined in
the XML tag data file describing some words (and their scores) associated with the tag “religion_-
buddhism”:
<tags>
<topic name="religion_buddhism">
<term name="buddhism" score="52" />
<term name="buddhist" score="50" />
<term name="mind" score="50" />
<term name="buddha" score="37" />
<term name="practic" score="31" />
<term name="teach" score="15" />
<term name="path" score="14" />
<term name="mantra" score="14" />
<term name="thought" score="14" />
<term name="school" score="13" />
<term name="zen" score="13" />
<term name="mahayana" score="13" />
<term name="suffer" score="12" />
<term name="dharma" score="12" />
<term name="tibetan" score="11" />
</topic>
</tags>
Natural Language Processing
Notice that the term names are stemmed words and all lower case. There are 28 categories (tags)
defined in the input XML file included in the GitHub repository for this book⁴¹.
For data access, I also maintain an array of tag names and an associated list of the word frequency
hash tables for each tag name:
private static String[] tagClassNames;
private static
List<Hashtable<String, Float>> hashes =
new ArrayList<Hashtable<String, Float>>();
The XML data is read and these data structures are filled during static class load time so creating
multiple instances of the class AutoTagger has no performance penalty in memory use. Except for
an empty default class constructor, there is only one public API for this class, the method getTags
gets the categories for input text:
public List<NameValue<String, Float>>
getTags(String text) {
To be clear, the tags returned are classification tags like “politics,” “economy,” etc. The utility class
NameValue is defined in the file:
src-statistical-nlp/com/markwatson/nlp/util/NameValue.java
To determine the tags for input text, we keep a running score for each defined tag type. I use the
internal class SFtriple to hold triple values of word, score, and tag index. I choose the tags with the
highest scores as the automatically assigned tags for the input text. Scores for each tag are calculated
by taking each word in the input text, stemming it, and if the stem is in the word frequency hash
table for the tag then add the score value in the hash table to the running sum for the tag. You can
refer to the AutoTagger.java source code for details. Here is an example use of class AutoTagger:
AutoTagger test = new AutoTagger();
String s = "The President went to Congress to argue
for his tax bill before leaving on a
vacation to Las Vegas to see some shows
and gamble.";
List<NameValue<String, Float>> results =
test.getTags(s);
for (NameValue<String, Float> result : results) {
System.out.println(result);
The output looks like:
⁴¹https://github.com/mark-watson/Java-AI-Book-Code
Natural Language Processing
[NameValue: news_economy : 1.0]
[NameValue: news_politics : 0.84]
Text Clustering
Clustering text documents refers to associating similar text documents with each other. The text
clustering system that I have written for my own projects is simple and effective but it inefficient for
large numbers of documents. The example code in this section is inherently inefficient for clustering
a large number of text documents because I perform significant semantic processing on each text
document and then compare all combinations of documents. The runtime performance is O(N²)
where N is the number of text documents. If you need to cluster or compare a very large number of
documents you will probably want to use a K-Mean clustering algorithm instead.
I use a few different algorithms to rate the similarity of any two text documents and I will combine
these depending on the requirements of the project that I am working on:
• Calculate the intersection of common words in the two documents.
• Calculate the intersection of common word stems in the two documents.
• Calculate the intersection of tags assigned to the two documents.
• Calculate the intersection of human and place names in the two documents.
In this section we will implement the second option: calculate the intersection of word stems in two
documents. Without showing the package and import statements, it takes just a few lines of code to
implement this algorithm when we use the Stemmer class.
The following listing shows the implementation of class ComparableDocument with comments.
We start by defining constructors for documents defined by a File object and a String object:
public class ComparableDocument {
// disable default constructor calls:
private ComparableDocument() { }
public ComparableDocument(File document)
throws FileNotFoundException {
this(new Scanner(document).
useDelimiter("\\Z").next());
public ComparableDocument(String text) {
List<String> stems =
new Stemmer().stemString(text);
for (String stem : stems) {
stem_count++;
if (stemCountMap.containsKey(stem)) {
Natural Language Processing
Integer count = stemCountMap.get(stem);
stemCountMap.put(stem, 1 + count);
} else {
stemCountMap.put(stem, 1);
In the last constructor, I simply create a count of how many times each stem occurs in the document.
The public API allows us to get the stem count hash table, the number of stems in the original
document, and a numeric comparison value for comparing this document with another (this is the
first version – we will add an improvement later):
public Map<String, Integer> getStemMap() {
return stemCountMap;
public int getStemCount() {
return stem_count;
public float
compareTo(ComparableDocument otherDocument) {
long count = 0;
Map<String,Integer> map2 = otherDocument.getStemMap();
Iterator iter = stemCountMap.keySet().iterator();
while (iter.hasNext()) {
Object key = iter.next();
Integer count1 = stemCountMap.get(key);
Integer count2 = map2.get(key);
if (count1!=null && count2!=null) {
count += count1 * count2;
return (float) Math.sqrt(
((float)(count*count) /
(double)(stem_count *
otherDocument.getStemCount())))
private Map<String, Integer> stemCountMap =
new HashMap<String, Integer>();
private int stem_count = 0;
Natural Language Processing
I normalize the return value for the method compareTo to return a value of 1.0 if compared
documents are identical (after stemming) and 0.0 if they contain no common stems. There are
four test text documents in the test_data directory and the following test code compares various
combinations. Note that I am careful to test the case of comparing identical documents:
ComparableDocument news1 =
new ComparableDocument("testdata/news_1.txt");
ComparableDocument news2 =
new ComparableDocument("testdata/news_2.txt");
ComparableDocument econ1 =
new ComparableDocument("testdata/economy_1.txt");
ComparableDocument econ2 =
new ComparableDocument("testdata/economy_2.txt");
System.out.println("news 1 - news1: " +
news1.compareTo(news1));
System.out.println("news 1 - news2: " +
news1.compareTo(news2));
System.out.println("news 2 - news2: " +
news2.compareTo(news2));
System.out.println("news 1 - econ1: " +
news1.compareTo(econ1));
System.out.println("econ 1 - econ1: " +
econ1.compareTo(econ1));
System.out.println("news 1 - econ2: " +
news1.compareTo(econ2));
System.out.println("econ 1 - econ2: " +
econ1.compareTo(econ2));
System.out.println("econ 2 - econ2: " +
econ2.compareTo(econ2));
The following listing shows output that indicates mediocre results; we will soon make an improve-
ment that makes the results better. The output for this test code is:
news 1 - news1: 1.0
news 1 - news2: 0.4457711
news 2 - news2: 1.0
news 1 - econ1: 0.3649214
econ 1 - econ1: 1.0
news 1 - econ2: 0.32748842
econ 1 - econ2: 0.42922822
econ 2 - econ2: 1.0
There is not as much differentiation in comparison scores between political news stories and
economic news stories. What is up here? The problem is that I did not remove common words (and
Natural Language Processing
therefore common word stems) when creating stem counts for each document. I wrote a utility class
NoiseWords for identifying both common words and their stems; you can see the implementation
in the file NoiseWords.java. Removing noise words improves the comparison results (I added a few
tests since the last printout):
news 1 - news1: 1.0
news 1 - news2: 0.1681978
news 1 - econ1: 0.04279895
news 1 - econ2: 0.034234844
econ 1 - econ2: 0.26178515
news 2 - econ2: 0.106673114
econ 1 - econ2: 0.26178515
Much better results! The API for com.markwatson.nlp.util.NoiseWords is a single static method:
public static boolean checkFor(String stem)
You can add additional noise words to the data section in the file NoiseWords.java, depending on
your application.
Wrapup
This chapter contains my own experiments with ad-hoc NLP that I often find useful for my work.
In the next chapter Natural Language Processing Using OpenNLP we use the Apache OpenNLP
library that I also often use in my work.
Natural Language Processing Using
OpenNLP
Here we use the Apache OpenNLP project⁴². OpenNLP has pre-trained models for tokenization,
sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and
coreference resolution. As we see later OpenNLP has tools to also build our own models.
I have worked in the field of Natural Language Processing (NLP) since the early 1980s. Many more
people are now interested in the field of NLP and the techniques have changed drastically in the last
decade.
Currently, OpenNLP has support for Danish, German, English, Spanish, Portuguese, and Swedish.
I include in the github repository some trained models for English that are used in the examples
in this chapter. You can download models for other languages at the web page for OpenNLP 1.5
models⁴³ (we are using version 1..6.0 of OpenNLP in this book which uses the version 1.5 models).
The following figure shows the project for this chapter in the Community Edition of IntelliJ:
⁴²https://opennlp.apache.org
⁴³http://opennlp.sourceforge.net/models-1.5/
Natural Language Processing Using OpenNLP
IntelliJ project view for the examples in this chapter
We will use pre-trained models for tokenizing text, recognizing the names of organizations, people,
locations, and parts of speech for words in input text. We will also train a new model (the file
opennlp/models/en-newscat.bin in the github repository) for recognizing the category of input
text. The section on training new maximum entropy classification models using your own training
data may be the material in this chapter that you will use the most in your own projects. We will train
one model to recognize the categories of COMPUTERS, ECONOMY, HEALTH, and POLITICS.
You should then have the knowledge for training your own models using your own training texts
for the categories that you need for your applications. The OpenNLP documentation⁴⁴ has additional
detail on custom models. We will also use both some pre-trained models that are included with the
OpenNLP distribution in the next chapter when we combine using OpenNLP with the WordNet
lexical database developed at Princeton University.
After building a classification model we finish up this chapter with an interesting topic: statistically
parsing sentences to discover the most probable linguistic structure of each sentence in input text.
We will not use parsing in the rest of this book so you may skip the last section of this chapter if
you are not currently interested in understanding sentence parse tree structure of components like
noun and verb phrases, proper nouns, nouns, and adjectives, etc.
The following UML class diagrams will give you an overview of my wrapper for the OpenNLP
library and for the unit test class:
⁴⁴https://opennlp.apache.org/docs/1.9.2/manual/opennlp.html#opennlp
Natural Language Processing Using OpenNLP
UML class diagram for wrapper code for OpenNLP examples
Using OpenNLP Pre-Trained Models
Assuming that you have cloned the github repository for this book, you can fetch the maven
dependencies, compile the code, install the generated library locally, and run the unit tests using
the command:
mvn install
The model files, including the categorization model you will learn to build later in this chapter, are
found in the subdirectory models. The unit tests in src/test/java/com/markwatson/opennlp/NLPTest.java
Natural Language Processing Using OpenNLP
provide examples for using the code we develop in this chapter. As for many examples in this book,
I use unit tests as examples for using a library and not as tests that check computed values.
The Java example code for tokenization (splitting text into individual words), splitting sentences,
and recognizing organizations, locations, and people in text is all in the Java class NLP. You can
look at the source code in the repository for this book. Here I will just show a few snippets of the
code to make clear how to load and use pre-trained models.
I use static class initialization to load the model files:
static public Tokenizer tokenizer = null;
static public SentenceDetectorME sentenceSplitter = null;
static POSTaggerME tagger = null;
static NameFinderME organizationFinder = null;
static NameFinderME locationFinder = null;
static NameFinderME personNameFinder = null;
static {
InputStream organizationInputStream =
new FileInputStream("models/en-ner-organization.bin");
TokenNameFinderModel model =
new TokenNameFinderModel(organizationInputStream);
organizationFinder = new NameFinderME(model);
organizationInputStream.close();
InputStream locationInputStream =
new FileInputStream("models/en-ner-location.bin");
model = new TokenNameFinderModel(locationInputStream);
locationFinder = new NameFinderME(model);
locationInputStream.close();
InputStream personNameInputStream =
new FileInputStream("models/en-ner-person.bin");
model = new TokenNameFinderModel(personNameInputStream);
personNameFinder = new NameFinderME(model);
personNameInputStream.close();
InputStream tokienizerInputStream =
new FileInputStream("models/en-token.bin");
TokenizerModel modelTokenizer = new TokenizerModel(tokienizerInputStream);
tokenizer = new TokenizerME(modelTokenizer);
tokienizerInputStream.close();
Natural Language Processing Using OpenNLP
InputStream sentenceInputStream =
new FileInputStream("models/en-sent.bin");
SentenceModel sentenceTokenizer = new SentenceModel(sentenceInputStream);
sentenceSplitter = new SentenceDetectorME(sentenceTokenizer);
tokienizerInputStream.close();
organizationInputStream = new FileInputStream("models/en-pos-maxent.bin");
POSModel posModel = new POSModel(organizationInputStream);
tagger = new POSTaggerME(posModel);
} catch (IOException e) {
e.printStackTrace();
The first operation that you will usually start with for processing natural language text is breaking
input text into individual words and sentences. Here is the code for using the tokenizing code that
separates text stored as a Java String into individual words:
public static String[] tokenize(String s) {
return tokenizer.tokenize(s);
Here is the similar code for breaking text into individual sentences:
public static String[] sentenceSplitter(String s) {
return sentenceSplitter.sentDetect(s);
Here is some sample code to use sentenceSplitter:
String sentence =
"Apple Computer, Microsoft, and Google are in the " +
" tech sector. Each is very profitable.";
String [] sentences = NLP.sentenceSplitter(sentence);
System.out.println("Sentences found:\n" + Arrays.toString(sentences));
In line 4 the static method NLP.sentenceSplitter returns an array of strings. In line 5 I use a common
Java idiom for printing arrays by using the static method Arrays.toString to convert the array of
strings into a List<String> object. The trick is that the List class has a toString method that formats
list nicely for printing.
Here is the output of this code snippet (edited for page width and clarity):
Natural Language Processing Using OpenNLP
Sentences found:
["Apple Computer, Microsoft, and Google are in the tech sector.",
"Each is very profitable."]
The code for finding organizations, locations, and people’s names is almost identical so I will only
show the code in the next listing for recognizing locations. Please look at the methods company-
Names and personNames in the class com.markwatson.opennlp.NLP to see the implementations
for finding the names of companies and people.
public static Set<String> locationNames(String text) {
return locationNames(tokenizer.tokenize(text));
public static Set<String> locationNames(String tokens[]) {
Set<String> ret = new HashSet<String>();
Span[] nameSpans = locationFinder.find(tokens);
if (nameSpans.length == 0) return ret;
for (int i = 0; i < nameSpans.length; i++) {
Span span = nameSpans[i];
StringBuilder sb = new StringBuilder();
for (int j = span.getStart(); j < span.getEnd(); j++)
sb.append(tokens[j] + " ");
ret.add(sb.toString().trim().replaceAll(" ,", ","));
return ret;
The public methods in the class com.markwatson.opennlp.NLP are overriden to take either a single
string value which gets tokenized inside of the method and also a method that takes as input text that
has already been tokenized into a String tokens[] object. In the last example the method starting
on line 1 accepts an input string and the overriden method starting on line 5 accepts an array of
strings. Often you will want to tokenize text stored in a single input string into tokens and reuse the
tokens for calling several of the public methods in com.markwatson.opennlp.NLP that can take
input that is already tokenized. In line 2 we simply tokenize the input text and call the method that
accepts tokenized input text.
In line 6 we create a HashSet<String> object that will hold the return value of a set of location names.
The NameFinderME object locationFinder returns an array of Span objects. The Span class is used
to represent a sequence of adjacent words. The Span class has a public static attribute length and
instance methods getstart and getEnd that return the indices of the beginning and ending (plus one)
index of a span in the original input text.
Here is some sample code to use locationNames along with the output (edited for page width and
clarity):
Natural Language Processing Using OpenNLP
String sentence =
"Apple Computer is located in Cupertino, California and Microsoft " +
"is located in Seattle, Washington. He went to Oregon";
Set<String> names = NLP.locationNames(sentence);
System.out.println("Location names found: " + names);
Location names found: [Oregon, Cupertino, Seattle, California, Washington]
Note that the pre-trained model does not provide information when city and state names are
associated with each other in the original sentence. The maximum entropy models used in OpenNLP
do use information on the context or a word in a sentence, for example, the preceeding and following
word. This combination of available contextual information makes maximum entropy models more
accurate than the “bag of words” technique we used in the last chapter.
Training a New Categorization Model for OpenNLP
The OpenNLP class DoccatTrainer can process specially formatted input text files and produce
categorization models using maximum entropy which is a technique that handles data with many
features. Features that are automatically extracted from text and used in a model are things like
words in a document and word adjacency. Maximum entropy models can recognize multiple classes.
In testing a model on new text data the probablilities of all possible classes add up to the value 1
(this is often refered to as “softmax”). For example we will be training a classifier on four categories
and the probablilities of these categories for some test input text add up to the value of one:
[[COMPUTERS, 0.1578880260493374], [ECONOMY, 0.2163099374638936],
[HEALTH, 0.35546925520388845], [POLITICS, 0.27033278128288035]]
The format of the input file for training a maximum entropy classifier is simple but has to be correct:
each line starts with a category name, followed by sample text for each category which must be all
on one line. Please note that I have already trained the model producing the model file models/en-
newscat.bin so you don’t need to run the example in this section unless you want to regenerate this
model file.
The file sample_category_training_text.txt contains four lines, defining four categories. Here are
two lines from this file (I edited the following to look better on the printed page, but these are just
two lines in the file):
Natural Language Processing Using OpenNLP
COMPUTERS Computers are often used to access the Internet to meet business functions.
ECONOMY The Austrian School (also known as the Vienna School or the Psychological Sc\
hool ) is a Schools of economic thought|school of economic thought that emphasizes t\
he spontaneous organizing power of the price mechanism.
Here is one training example each for the categories COMPUTERS and ECONOMY.
You must format the training file perfectly. As an example, if you have empty (or blank) lines in
your input training file then you will get an error like:
Computing event counts...
java.io.IOException: Empty lines, or lines with
only a category string are not allowed!
The OpenNLP documentation has examples for writing custom Java code to build models but I
usually just use the command line tool; for example:
bin/opennlp DoccatTrainer -model models/en-newscat.bin -lang en \
-data sample_category_training_text.txt \
-encoding UTF-8
The model is written to the relative file path models/en-newscat.bin. The training file I am using
is tiny so the model is trained in a few seconds. For serious applications, the more training text the
better! By default the DoccatTrainer tool uses the default text feature generator which uses word
frequencies in documents but ignores word ordering. As I mention in the next section, I sometimes
like to mix word frequency feature generation with 2gram (that is, frequencies of two adjacent
words). In this case you cannot simply use the DoccatTrainer command line tool. You need to write
a little Java code yourself that you can plug another feature generator into using the alternative API:
public static DoccatModel train(String languageCode,
ObjectStream<DocumentSample> samples,
int cutoff, int iterations,
FeatureGenerator... featureGenerators)
In the next section, you will note that the last argument would look like the case where we combine
two feature generators, one that uses “bag of words” and the other that uses adjacent word sequences:
public DocumentCategorizerME(DoccatModel model,
new FeatureGenerator[]{new BagOfWordsFeatureGenerator(),
new NGramFeatureGenerator()});
For some purposes the default word frequency (or bag of words) feature generator is probably OK
so using the command line tool is a good place to start because models are smaller and training time
is minimal. Adding the NGramFeatureGenerator increases both training time and model size but
should produce better results. Here is the output from running the DoccatTrainer command line
Natural Language Processing Using OpenNLP
> bin/opennlp DoccatTrainer -model models/en-newscat.bin -lang en \
-data sample_category_training_text.txt \
-encoding UTF-8
Indexing events using cutoff of 5
Computing event counts...
done. 4 events
Indexing...
Sorting and merging events... done. Reduced 4 events to 4.
Done indexing.
Incorporating indexed data for training...
Number of Event Tokens: 4
Number of Outcomes: 4
Number of Predicates: 153
...done.
Computing model parameters ...
Performing 100 iterations.
... loglikelihood=-5.545177444479562
... loglikelihood=-4.730232204542369
... loglikelihood=-4.232192673282495
... loglikelihood=-0.23411803835475453
... loglikelihood=-0.23150121909902377
... loglikelihood=-0.22894028845170055
Writing document categorizer model ... done (0.041s)
Wrote document categorizer model to
path: /Users/mark/power-java/opennlp/models/en-newscat.bin
We will use our new trained model file en-newscat.bin in the next section.
Please note that in this simple example I used very little data, just a few hundred words for each
training category. I have used the OpenNLP maximum entropy library on various projects, mostly
to good effect, but I used many thousands of words for each category. The more data, the better.
Using Our New Trained Classification Model
The code that uses the model we trained in the last section is short enough to list in its entirety:
Natural Language Processing Using OpenNLP
package com.markwatson.opennlp;
import opennlp.tools.doccat.DoccatModel;
import opennlp.tools.doccat.DocumentCategorizerME;
import java.io.*;
import java.util.*;
* This program uses the maximum entropy model we trained
* using the instructions in the chapter on OpenNLP in the book.
public class NewsClassifier {
public static List<Pair<String,Double>> allRankedCategories(String text) {
DocumentCategorizerME aCategorizer = new DocumentCategorizerME(docCatModel);
double[] outcomes = aCategorizer.categorize(text);
List<Pair<String,Double>> results = new ArrayList<Pair<String, Double>>();
for (int i=0; i<outcomes.length; i++) {
results.add(new Pair<String, Double>(aCategorizer.getCategory(i), outcomes[i])\
return results;
public static String bestCategory(String text) {
DocumentCategorizerME aCategorizer = new DocumentCategorizerME(docCatModel);
double[] outcomes = aCategorizer.categorize(text);
return aCategorizer.getBestCategory(outcomes);
public static Pair<String,Float> classifyWithScore(String text) {
DocumentCategorizerME classifier = new DocumentCategorizerME(docCatModel);
double [] scores = classifier.categorize(text);
int num_categories = classifier.getNumberOfCategories();
if (num_categories > 0) {
String bestString =
classifier.getBestCategory(scores);
for (int i=0; i<num_categories; i++) {
if (classifier.getCategory(i).equals(bestString)) {
return new Pair<String,Float>(bestString, (float)scores[i]);
Natural Language Processing Using OpenNLP
return new Pair<String,Float>("<no category>", 0f);
static DoccatModel docCatModel = null;
static {
InputStream modelIn = new FileInputStream("models/en-newscat.bin");
docCatModel = new DoccatModel(modelIn);
} catch (IOException e) {
e.printStackTrace();
In lines 48-57 we initialize the static data for an instance of the class DoccatModel that loads the
model file created in the last section.
A new instance of the class DocumentCategorizerME is created in line 28 each time we want to
classify input text. I called the one argument constructor for this class that uses the default feature
detector. An alternative constructor is:
public DocumentCategorizerME(DoccatModel model,
FeatureGenerator... featureGenerators)
The default feature generator is BagOfWordsFeatureGenerator which just uses word frequencies
for classification. This is reasonable for smaller training sets as we used in the last section but when I
have a large amount of training data available I prefer to combine BagOfWordsFeatureGenerator
with NGramFeatureGenerator. You would use the constructor call:
public DocumentCategorizerME(DoccatModel model,
new FeatureGenerator[]{new BagOfWordsFeatureGenerator(),
new NGramFeatureGenerator()});
The following listings show interspersed both example code snippets for using the NewsClassifier
class followed by the output printed by each code snippet:
Natural Language Processing Using OpenNLP
String sentence = "Apple Computer, Microsoft, and Google are in the tech sector.\
Each is very profitable.";
System.out.println("\nNew test sentence:\n\n" + sentence + "\n");
String [] sentences = NLP.sentenceSplitter(sentence);
System.out.println("Sentences found: " + Arrays.toString(sentences));
New test sentence:
Apple Computer, Microsoft, and Google are in the tech sector.
Each is very profitable.
Sentences found: [Apple Computer, Microsoft, and Google are in the tech sector.,
Each is very profitable.]
sentence = "Apple Computer, Microsoft, and Google are in the tech sector.";
Set<String> names = NLP.companyNames(sentence);
System.out.println("Company names found: " + names);
Company names found: [Apple Computer, Microsoft]
sentence = "Apple Computer is located in Cupertino, California and Microsoft is \
located in Seattle, Washington. He went to Oregon";
System.out.println("\nNew test sentence:\n\n" + sentence + "\n");
Set<String> names1 = NLP.locationNames(sentence);
System.out.println("Location names found: " + names1);
New test sentence:
Apple Computer is located in Cupertino, California and Microsoft is located in Seatt\
le, Washington. He went to Oregon
Location names found: [Oregon, Cupertino, Seattle, California, Washington]
Natural Language Processing Using OpenNLP
sentence = "President Bill Clinton left office.";
System.out.println("\nNew test sentence:\n\n" + sentence + "\n");
Set<String> names2 = NLP.personNames(sentence);
System.out.println("Person names found: " + names2);
New test sentence:
President Bill Clinton left office.
Person names found: [Bill Clinton]
sentence = "The White House is often mentioned in the news about U.S. foreign po\
licy. Members of Congress and the President are worried about the next election and \
may pander to voters by promising tax breaks. Diplomacy with Iran, Iraq, and North K\
orea is non existent in spite of a worry about nuclear weapons. A uni-polar world re\
fers to the hegemony of one country, often a militaristic empire. War started with a\
single military strike. The voting public wants peace not war. Democrats and Republ\
icans argue about policy.";
System.out.println("\nNew test sentence:\n\n" + sentence + "\n");
List<Pair<String, Double>> results = NewsClassifier.allRankedCategories(sentence\
System.out.println("All ranked categories found: " + results);
New test sentence:
The White House is often mentioned in the news about
U.S. foreign policy. Members o\
f Congress and the President are worried about the next election and may pander to v\
oters by promising tax breaks. Diplomacy with Iran, Iraq, and North Korea is non exi\
stent in spite of a worry about nuclear weapons. A uni-polar world refers to the heg\
emony of one country, often a militaristic empire. War started with a single militar\
y strike. The voting public wants peace not war. Democrats and Republicans argue abo\
ut policy.
All ranked categories found: [[COMPUTERS, 0.07257665117660535], [ECONOMY, 0.23559821\
969425127], [HEALTH, 0.29907267186308945], [POLITICS, 0.39275245726605396]]
Natural Language Processing Using OpenNLP
sentence = "The food affects colon cancer and ulcerative colitis. There is some \
evidence that sex helps keep us healthy. Eating antioxidant rich foods may prevent d\
esease. Smoking may raise estrogen levels in men and lead to heart failure.";
System.out.println("\nNew test sentence:\n\n" + sentence + "\n");
String results2 = NewsClassifier.bestCategory(sentence);
System.out.println("Best category found found (HEALTH): " + results2);
List<Pair<String, Double>> results3 = NewsClassifier.allRankedCategories(sentenc\
System.out.println("All ranked categories found (HEALTH): " + results3);
System.out.println("Best category with score: " + NewsClassifier.classifyWithSco\
re(sentence));
New test sentence:
The food affects colon cancer and ulcerative colitis. There is some evidence that se\
x helps keep us healthy. Eating antioxidant rich foods may prevent desease. Smoking \
may raise estrogen levels in men and lead to heart failure.
Best category found found (HEALTH): HEALTH
All ranked categories found (HEALTH): [[COMPUTERS, 0.1578880260493374], [ECONOMY, 0.\
2163099374638936], [HEALTH, 0.35546925520388845], [POLITICS, 0.27033278128288035]]
Best category with score: [HEALTH, 0.35546926]
As this example shows it is simple to train new classifier models once you have prepared training
data. In my work I have often needed to train models customized for specific topics.
Using the OpenNLP Parsing Model
We will use the parsing model that is included in the OpenNLP distribution to parse English language
input text into syntax trees. You are unlikely to use a statistical parsing model in your work but I
think you will enjoy the material in the section.
The following example code listing is long (68 lines) but I will explain the interesting parts after the
listing:
Natural Language Processing Using OpenNLP
package com.markwatson.opennlp;
import opennlp.tools.cmdline.parser.ParserTool;
import opennlp.tools.parser.Parse;
import opennlp.tools.parser.ParserModel;
import opennlp.tools.parser.chunking.Parser;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
* Experiments with the chunking parser model
public class ChunkingParserDemo {
public Parse[] parse(String text) {
Parser parser = new Parser(parserModel);
return ParserTool.parseLine(text, parser, 5);
public void prettyPrint(Parse p) {
StringBuffer sb = new StringBuffer();
p.show(sb);
String s = sb.toString() + " ";
int depth = 0;
for (int i = 0, size = s.length() - 1; i < size; i++) {
if (s.charAt(i) == ' ' && s.charAt(i + 1) == '(') {
System.out.print("\n");
for (int j = 0; j < depth; j++) System.out.print("
} else if (s.charAt(i) == '(') System.out.print(s.charAt(i));
else if (s.charAt(i) != ')' || s.charAt(i + 1) == ')')
System.out.print(s.charAt(i));
else {
System.out.print(s.charAt(i));
for (int j = 0; j < depth; j++) System.out.print("
if (s.charAt(i) == '(') depth++;
if (s.charAt(i) == ')') depth--;
System.out.println();
static ParserModel parserModel = null;
Natural Language Processing Using OpenNLP
static {
InputStream modelIn = new FileInputStream("models/en-parser-chunking.bin");
parserModel = new ParserModel(modelIn);
} catch (IOException e) {
e.printStackTrace();
public static void main(String[] args) {
ChunkingParserDemo cpd = new ChunkingParserDemo();
Parse[] possibleParses =
cpd.parse("John Smith went to the store on his way home from work ");
for (Parse p : possibleParses) {
System.out.println("parse:\n");
p.show();
//p.showCodeTree();
System.out.println("\npretty printed parse:\n");
cpd.prettyPrint(p);
System.out.println("\n");
The OpenNLP parsing model is read from a file in lines 45 through 53. The static variable
parserModel (instance of class ParserModel) if created in line 49 and used in lines 17 and 18 to
parse input text. It is instructive to look at the intermediate calculation results. The value for variable
parser defined in line 17 has a value of:
Note that the parser returned 5 different results because we specified this number in line 18. For
a long sentence the parser generates a very large number of possible parses for the sentence and
returns, in order of probability of being correct, the number of results we requested.
The OpenNLP chunking parser code prints out results in a flat list, one result on a line. This is
difficult to read which is why I wrote the method prettyPrint (lines 21 through 41) to print the
parse results indented. Here is the output from the last code example (the first parse shown is all on
one line but line wraps in the following listing):
Natural Language Processing Using OpenNLP
parse:
(TOP (S (NP (NNP John) (NNP Smith)) (VP (VBD went) (PP (TO to) (NP (DT the) (NN stor\
e))) (PP (IN on) (NP (NP (NP (PRP$ his) (NN way)) (ADVP (NN home))) (PP (IN from) (N\
P (NN work))))))))
pretty printed parse:
(NNP John)
(NNP Smith))
(VBD went)
(TO to)
(DT the)
(NN store)))
(IN on)
(PRP$ his)
(NN way))
(NN home)))
(IN from)
(NN work))))))))
In the 1980s I spent much time on syntax level parsing. I no longer find these models very relevant
to my own work.
OpenNLP is a good resource for Java programmers and its Apache 2 license is “business friendly.” If
you can use software with a GPL license then please also look at the Stanford NLP libraries⁴⁵.
⁴⁵https://nlp.stanford.edu/software/
Combining the WordNet Linguistic
Database With OpenNLP
Here we build on the material from the last chapter by using OpenNLP to process input text to
identify parts of speech and then looking up words with their parts of speech in WordNet. You can
use WordNet to look up all uses of a word so using OpenNLP as we do here is not required but I
think makes a good example of blending libraries.
The WordNet linguistic database is complex and you may want to review the WordNet documenta-
tion⁴⁶ after working through the following example of finding synonyms and hypernyms for nouns
in text input.
Using the WordNet Linguistic Database
The Maven pom.xml file (that we will look at later) for this example is configured to download
both the WordNet data and the extjwnl⁴⁷ library. The home page for the WordNet project is
http://wordnet.princeton.edu⁴⁸.
Tutorial on WordNet
The WordNet lexical database is an ongoing research project spanning decades that includes many
years of effort by professional linguists. My own use of WordNet over the last twenty years has been
simple, mainly using the database to determine synonyms (called synsets in WordNet) and looking
at the possible parts of speech of words. For reference from a Wikipedia article on WordNet⁴⁹, here is
a small subset of the type of relationships contained in WordNet for nouns (we will look at relations
for verbs, adjectives, and adverbs at the end of this chapter):
⁴⁶https://wordnet.princeton.edu/documentation
⁴⁷https://github.com/extjwnl/extjwnl
⁴⁸http://wordnet.princeton.edu
⁴⁹https://en.wikipedia.org/wiki/WordNet
Combining the WordNet Linguistic Database With OpenNLP
hypernyms
parent is a hypernym of mother since every mother is of type parent
hyponyms
father (less general) is a hyponym of parent (more general)
holonym
building is a holonym of window because a window is part of a
building
meronym
window is a meronym of building because a window is part of a
building
I find the WordNet book (WordNet: An Electronic Lexical Database (Language, Speech, and
Communication) by Christiane Fellbaum, 1998) to be a detailed reference for WordNet but there
have been several new releases of WordNet since the book was published. The WordNet site and the
Wikipedia article on WordNet are also good sources of information if you decide to make WordNet
part of your toolkit.
When you look up words in WordNet you can optionally specify one of the following parts of speech
(POS):
POS.NOUN
POS.VERB
POS.ADJECTIVE
POS.ADVERB
Rather than using Open NLP in this example to find nouns, we could try all four possible parts of
speech on every input word. I was motivated to use OpenNLP partly because I wanted to provide
an example of reusing a library developed in a different chapter, in this case the previous chapter on
OpenNLP.
What are hypernyms? A hypernym is in a semantic type-of relationship with another word that is
more general. As an example, the word “country” is a hypernym of the word “Mexico.” The opposite
of hypernym is a hyponym, that is a hyponym has a more narrow meaning. The word “country” is
a hyponym of the word “Mexico.”
The following example is simple. We only look up WordNet hypernyms and synonyms entries for
nouns and for nouns. In the chapter wrap-up I will give you some ideas for more extended projects
that you may want to do using WordNet.
Before diving into the code I want to show you what the generated synonym and hypernym data
looks like.
Combining the WordNet Linguistic Database With OpenNLP
The following two figures show the generated structured output data in a IntelliJ Community Edition
debug session, in particular look at the structure of synonymMap. While I rarely use the debugger
to actually debug code, I frequently us it to inspect data.
WordNet example: examining generated synonym data
Here is a closeup showing generated data in the variable hypernymMap:
Combining the WordNet Linguistic Database With OpenNLP
{width: “80%”}
Installing the Libraries and Linguistic Data for this
Example
The maven pom.xml configuration file for this project contains the following requirements:
<dependency>
<groupId>net.sf.extjwnl</groupId>
<artifactId>extjwnl</artifactId>
<version>2.0.2</version>
</dependency>
<!-- Princeton WordNet 3.1 data dependency -->
<dependency>
<groupId>net.sf.extjwnl</groupId>
<artifactId>extjwnl-data-wn31</artifactId>
<version>1.2</version>
</dependency>
The second dependency loads the WordNet linguistic database.
I assume that you have performed a maven install for the project in the last chapter:
Combining the WordNet Linguistic Database With OpenNLP
push ~../opennlp
mvn install -DskipTests
We also need the OpenNLP library and my OpenNLP wrapper library developed in the last chapter:
<dependency>
<groupId>opennlp</groupId>
<artifactId>tools</artifactId>
<version>1.5.0</version>
</dependency>
<dependency>
<groupId>com.markwatson</groupId>
<artifactId>opennlp</artifactId>
<version>1.0-SNAPSHOT</version>
</dependency>
There is a Makefile for running the example:
example:
mvn install
mvn exec:java
-Dexec.mainClass="com.markwatson.wordnet_example.WordNetAndOpenNlpExample"
Run the example using:
make example
The example identifies all nouns (in the example code they are: President, cat, Bill, Mexico, dog, and
Clinton) and for each noun finds all WordNet “word senses.” The noun “Mexico” only has one word
sense while the noun “cat” has eight word senses.
For each word sense (for each noun) the example code returns a list of strings where the first element
is a descriptive “word gloss” for the particular sense and the remaining strings are hypernyms or
synonyms depending whether we are calling the functions getHypernyms or getSynonyms whose
method signatures are:
static public Map<String, List<List<String>>> getHypernyms(String s)
throws JWNLException {
static public Map<String, List<List<String>>> getSynonyms(String s)
throws JWNLException {
Combining the WordNet Linguistic Database With OpenNLP
The following UML class diagram shows this public API for the example code. There is only
one source file in this example WordNetAndOpenNlpExample.java because our goal here is not
building a reusable library, rather to provide a short example and a “starter project” for your own
experiments. That said this project does get installed as a local Maven library so you can require
the library and call getHypernyms and getSynonyms in other projects. I will later suggest further
projects in the wrap-up.
UML class diagram for WordNet example
The values returned from both getHypernyms and getSynonyms are maps where the keys are
nouns in the input text. The map value for each key (a noun) will be a list, one element for each
word sense for the noun. Each of these sub-lists is a list of strings where the first element is the
gloss for the word sense and the remaining strings are either hypernyms or synonyms depending of
which function is called.
Let’s look at the word “Clinton” which has four different word senses; here we list the gloss for each
word sense:
• wife of President Clinton and later a woman member of the United States Senate (1947-)
• 42nd President of the United States (1946-)
• United States politician who as governor of New York supported the project to build the Erie
Canal (1769-1828)
• a town in east central Iowa
As an example, the hypernyms for the first word sense are: legislator, state senator, Clinton, Hiliary
Clinton, etc.
WordNet is a linguistic database for automating the processing of text and does contain some “real
world” knowledge but does not have the wide coverage of semantic web knowledge graphs like
Combining the WordNet Linguistic Database With OpenNLP
DBPedia and WikiData. We will discuss the semantic web and Knowledge Graphs in later chapters
as a source of information. Here we seek ways to understand words in text.
Here is a small segment of the output of the example program:
**** Hypernyms:
{President=[[an executive officer of a firm or corporation, executive, executive dir\
ector, chairman of the board, chief executive officer, CEO, chief operating officer,\
chief financial officer, CFO, insider, president],
**** Synonyms:
{President=[[an executive officer of a firm or corporation, president], [the person \
who holds the office of head of state of the United States government; "the Presiden\
t likes to jog every morning", President of the United States, United States Preside\
nt, President, Chief Executive], [the chief executive of a republic, president],
Implementation
You will never need to instantiate an instance of the class WordNetAndOpenNlpExample because
all data is static class data and the two public methods are both static. The methods getHypernyms
(lines 27-58) and getSynonyms (lines 60-81) are similar except the APIs for getting synonyms (lines
68-69) differ than those for getting hypernyms (lines 39-43). These two methods could be combined
into a single method with an argument to control which APIs to use but I thought the code looked
simpler as it is.
package com.markwatson.wordnet_example;
import com.markwatson.opennlp.NLP;
import net.sf.extjwnl.JWNLException;
import net.sf.extjwnl.data.*;
import net.sf.extjwnl.data.list.PointerTargetNode;
import net.sf.extjwnl.dictionary.Dictionary;
import java.util.*;
public class WordNetAndOpenNlpExample {
Combining the WordNet Linguistic Database With OpenNLP
private WordNetAndOpenNlpExample() {
static Dictionary dictionary = null;
static {
dictionary = Dictionary.getDefaultResourceInstance();
} catch (JWNLException e) {
e.printStackTrace();
System.exit(1);
static public Map<String, List<List<String>>> getHypernyms(String s)
throws JWNLException {
Map hypernymMap = new HashMap<String, List<List<String>>>();
String[] tokens = NLP.tokenize(s);
String[] pos = NLP.POS(s);
for (int i = 0; i < tokens.length; i++) {
if (pos[i].startsWith("N")) {
IndexWord iw = dictionary.getIndexWord(POS.NOUN, tokens[i]);
List hypernymList = new ArrayList<List<List<String>>>();
for (Synset aSense : iw.getSenses()) {
List lemmaList = new ArrayList<List<String>>();
lemmaList.add(aSense.getGloss());
for (PointerTargetNode ptn : PointerUtils.getDirectHypernyms(aSense)) {
List<PointerTarget> pthTargets = ptn.getPointerTarget().getTargets();
for (Object pt : pthTargets) {
Synset spt = (Synset) pt;
List<Word> words = spt.getWords();
for (Word word : words) {
lemmaList.add(word.getLemma());
} catch (Exception ignore) {
hypernymList.add(lemmaList);
if (hypernymList.size() > 0) hypernymMap.put(tokens[i], hypernymList);
Combining the WordNet Linguistic Database With OpenNLP
return hypernymMap;
static public Map<String, List<List<String>>> getSynonyms(String s)
throws JWNLException {
Map synonymMap = new HashMap<String, List<List<String>>>();
String[] tokens = NLP.tokenize(s);
String[] pos = NLP.POS(s);
for (int i = 0; i < tokens.length; i++) {
if (pos[i].startsWith("N")) {
List<List<String>> lemmaList = new ArrayList<List<String>>();
IndexWord iw = dictionary.getIndexWord(POS.NOUN, tokens[i]);
for (Synset aSense : iw.getSenses()) {
List<String> lemmas = new ArrayList<String>();
lemmas.add(aSense.getGloss());
for (Word word : aSense.getWords()) {
lemmas.add(word.getLemma());
lemmaList.add(lemmas);
if (lemmaList.size() > 0) synonymMap.put(tokens[i], lemmaList);
return synonymMap;
public static void main(String[] args)
throws JWNLException {
String s = "The cat, and dog, ran after President Bill Clinton in Mexico";
System.out.println("\n**** Hypernyms:\n");
System.out.println(getHypernyms(s));
System.out.println("\n**** Synonyms:\n");
System.out.println(getSynonyms(s));
The Java class PointerTargetNode (line 39) contains two instance variables: a Sysnset and a pointer
type (e.g., noun or verb). The class Synset contains a part of speech (e.g., a noun), pointers to related
synsets, descriptive words (or lemmas) for the synset, and a descriptive gloss that we have seen
examples of. The class IndexWord contains a part of speech for the current word sense, a descriptive
word (lemma) and a list of available synsets.
Earlier you saw two screen shots examining the output data in a IntelliJ Community Edition debug
Combining the WordNet Linguistic Database With OpenNLP
session. To better understand the Java interface to WordNet, I suggest also setting breakpoints
to examine the data structures used by the net.sf.extjwnl library, especially the Java classes
net.sf.extjwnl.data and net.sf.extjwnl.data.IndexWord.
Other Type Relationships Supported by WordNet
For reference (from the Wikipedia article on WordNet⁵⁰), here is a small subset of the type of
relationships contained in WordNet for verbs shown by examples (also from the Wikipedia article⁵¹):
• hypernym: travel (less general) is an hypernym of movement (more general)
• entailment: to sleep is entailed by to snore because you must be asleep to snore
Here are a few of the relations supported for nouns:
• hypernyms: canine is a hypernym of dog since every dog is of type canine
• hyponyms: dog (less general) is a hyponym of canine (more general)
• holonym : building is a holonym of window because a window is part of a building
• meronym: window is a meronym of building because a window is part of a building
Some of the related information maintained for adjectives is:
• related nouns:
• similar to
As mentioned before, I find the book WordNet: An Electronic Lexical Database (Language,
Speech, and Communication) useful as is the WordNet site⁵² and the Wikipedia article on
WordNet⁵³. I hope that I have motivated you, dear reader, to make WordNet and OpenNLP parts of
your toolkit.
Wrap-up and Ideas for Using WordNet
WordNet provides a rich linguistic database for human linguists but although I have been using
WordNet since 1999, I do not often use it in automated systems. I tend to use it for manual reference
and sometimes for simple tasks like augmenting a list of terms with synonyms.
The following three sub-sections are suggested projects.
⁵⁰http://en.wikipedia.org/wiki/WordNet
⁵¹http://en.wikipedia.org/wiki/WordNet
⁵²http://wordnet.princeton.edu/
⁵³http://en.wikipedia.org/wiki/WordNet
Combining the WordNet Linguistic Database With OpenNLP
Process all possible word senses
We only used WordNet entries for nouns. You might want to make copies of the file Word-
NetAndOpenNlpExample.java and instead of looking up entries for nouns, you might want
to try other parts of speech: verbs, adjectives, and adverbs. You might also make a copy of
WordNetAndOpenNlpExample.java and don’t use OpenNLP to tag text but rather look up all
four supported parts of speech for each word in the input text.
WordNet is a powerful tool for automating natural language processing but it is not easy to work
with. I hope that with this simple example you are now motivated to dive in deeper and consider
using WordNet for your projects, where it is appropriate to do so.
Using a Part of Speech Tagger to Use the Correct WordNet
Synonyms
WordNet will give us both synonyms and antonyms (opposite meaning) of words. The problem is
that we can only get words with similar and opposite meanings for specific “senses” of a word. Using
for example synonyms of the word “bank” in the sense of a verb meaning “have confidence or faith
in” are:
• trust
• swear
• rely
while synonyms for “bank” in the sense of a noun meaning “a financial institution that accepts
deposits and channels the money into lending activities” are:
• depository financial institution
• banking concern
• banking company
So it does not make too much sense to try to maintain a data map of synonyms for a given word
unless we use the word sense or a word in the context of a sentence.
Using WordNet Synonyms to Improve Document Clustering
Another suggestion for a OpenNLP and WordNet-based project is to use the OpenNLP part of
speech to identify the probable part of speech for each word in all text documents that you want
to cluster, and augment the documents with WordNet synset (synonym) data. You can then cluster
the documents similarly to how we used a “bag of words” in the earlier chapter Natural Language
Processing but instead of counting word frequencies, count composite word/part-of-speech token
frequencies as well as implied synonym-word/part-of-speech token frequencies.
Information Gathering
I often write software to automatically collect and use data from the web and other sources. In this
chapter I have collected utility code that I have written over the years into a small library supporting
two approaches to web scraping, DBPedia lookup, and GeoNames lookup. This code is simple but I
hope you will find useful.
The following UML class diagram shows the public APIs the libraries developed in this chapter:
UML class diagram for DBPedia lookup, Geonames, and web spiders
Information Gathering
Web Scraping Examples
As a practical matter, much of the data that many people use for machine learning either comes
from the web or from internal data sources. This section provides some guidance and examples for
getting text data from the web.
Before we start a technical discussion about web scraping I want to point out to you that much of
the information on the web is copyright and the first thing that you should do is to read the terms
of service for web sites to insure that your use of “scraped” or “spidered” data conforms with the
wishes of the persons or organizations who own the content and pay to run scraped web sites.
Motivation for Web Scraping
There is a huge amount of structured data available on the web via web services, semantic web/linked
data markup, and APIs. That said, you will frequently find data that is useful to pull raw text from
web sites but this text is usually fairly unstructured and in a messy (and frequently changing) format
as web pages meant for human consumption and not meant to be ingested by software agents.
In this chapter we will cover useful “web scraping” techniques. You will see that there is often a
fair amount of work in dealing with different web design styles and layouts. To make things even
more inconvenient you might find that your software information gathering agents will often break
because of changes in web sites.
I tend to use one of three general techniques for scraping web sites. Only the first two will be covered
in this chapter:
• Use an HTML parsing library that strips all HTML markup and Javascript from a page
and returns a “pure text” block of text. The text in navigation menus, headers, etc. will be
interspersed with what we might usually think of a “content” from a web site.
• Exploit HTML DOM (Document Object Model) formatting information on web sites to pick
out headers, page titles, navigation menus, and large blocks of content text.
• Use a tool like Selenium⁵⁴ to programatically control a web browser so your software agents
can login to site and otherwise perform navigation. In other words your software agents can
simulate a human using a web browser.
I seldom need to use tools like Selenium but as the saying goes “when you need them, you need them.”
For simple sites I favor extracting all text as a single block and use DOM processing as needed.
I am not going to cover the use of Selenium and the Java Selenium Web-Driver APIs in this chapter
because, as I mentioned, I tend to not use it frequently and I think that you are unlikely to need to
do so either. I refer you to the Selenium documentation if the first two approaches in the last list
do not work for your application. Selenium is primarily intended for building automating testing of
complex web applications, so my occasional use in web spidering is not the common use case.
I assume that you have some experience with HTML and DOM. DOM is a tree data structure.
⁵⁴http://docs.seleniumhq.org/
Information Gathering
Web Scraping Using the Jsoup Library
We will use the MIT licensed library jsoup⁵⁵. One reason I selected jsoup for the examples in
this chapter out of many fine libraries that provide similar functionality is the particularly nice
documentation, especially The jsoup Cookbook⁵⁶ which I urge you to bookmark as a general
reference. In this chapter I will concentrate on just the most frequent web scraping use cases that I
use in my own work.
The following bit of example code uses jsoup to get the text inside all P (paragraph) elements that
are direct children of any DIV element. On line 14 we use the jsoup library to fetch my home web
package com.markwatson.web_scraping;
import org.jsoup.*;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
* Examples of using jsoup
public class MySitesExamples {
public static void main(String[] args) throws Exception {
Document doc = Jsoup.connect("https://markwatson.com")
.userAgent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10.0; rv:77.0) Gecko/2010\
0101 Firefox/77.0")
.timeout(2000).get();
Elements newsHeadlines = doc.select("div p");
for (Element element : newsHeadlines) {
System.out.println(" next element text: " + element.text());
String all_page_text = doc.text();
System.out.println("All text on web page:\n" + all_page_text);
Elements anchors = doc.select("a[href]");
for (Element anchor : anchors) {
String uri = anchor.attr("href");
System.out.println(" next anchor uri: " + uri);
System.out.println(" next anchor text: " + anchor.text());
⁵⁵http://jsoup.org/
⁵⁶http://jsoup.org/cookbook/
Information Gathering
Elements absolute_uri_anchors = doc.select("a[href]");
for (Element anchor : absolute_uri_anchors) {
String uri = anchor.attr("abs:href");
System.out.println(" next anchor absolute uri: " + uri);
System.out.println(" next anchor absolute text: " + anchor.text());
In line 18 I am selecting the pattern that returns all P elements that are direct children of any DIV
element and in lines 19-21 print the text inside these P elements.
For training data for machine learning it is useful to just grab all text on a web page and assume that
common phrases dealing with web navigation, etc. will be dropped from learned models because
they occur in many different training examples for different classifications. In the above listing, line
22 shows how to fetch the plain text from an entire web page. The code on line 24 fetched anchor
elements and the loop in lines 25-29 prints out this anchor data as URI and text. The code in lines
30-35 does the same thing except we are converting relative URIs to absolute URIs.
Output might look like (most of the output is not shown from running this example file MySitesEx-
amples.java):
next element text: I am the author of 20+ books on Artificial Intelligence, Common \
Lisp, Deep Learning, Haskell, Java, Ruby, JavaScript, and the Semantic Web. I have 5\
5 US Patents.
next element text: My customer list includes: Google, Capital One, CompassLabs, Dis\
ney, SAIC, Americast, PacBell, CastTV, Lutris Technology, Arctan Group, Sitescout.co\
m, Embed.ly, and Webmind Corporation.
All text on web page:
Mark Watson: consultant specializing in Common Lisp, deep learning and natural langu\
age processing
learning Toggle navigation Mark Watson consultant and author specializing in Common \
Lisp development and AI
next anchor uri: #
next anchor text: Mark Watson consultant and author specializing in Common Lisp dev\
elopment and AI research projects and commercial products
next anchor uri: /
next anchor text: Home page
next anchor uri: /consulting
next anchor text: Consulting
next anchor uri: /blog
Information Gathering
next anchor text: My Blog
next anchor uri: /books
next anchor text: My Books
next anchor uri: /opensource
next anchor text: Open Source
next anchor absolute uri: https://markwatson.com#
next anchor absolute text: Mark Watson consultant and author specializing in Common\
Lisp development and AI research projects and commercial products
next anchor absolute uri: https://markwatson.com/
next anchor absolute text: Home page
next anchor absolute uri: https://markwatson.com/consulting
The 2gram (i.e., two words in sequence) “Toggle navigation” in the last listing has nothing to do with
the real content in my site and is an artifact of using the Bootstrap CSS and Javascript tools. Often
“noise” like this is simply ignored by machine learning models if it appears on many different sites
but beware that this might be a problem and you might need to precisely fetch text from specific
DOM elements. Similarly, notice that this last listing picks up the plain text from the navigation
menus.
Notice that there are different types of URIs like #, relative, and absolute. Any characters following
a # character do not affect the routing of which web page is shown (or which API is called) but the
characters after the # character are available for use in specifying anchor positions on a web page
or extra parameters for API calls. Relative APIs like consulting/ are understood to be relative to the
base URI of the web site.
I often require that URIs be absolute URIs (i.e., starts with a protocol like “http:” or “https:”) and
lines 28-33 show how to select just absolute URI anchors. In line 31 I am specifying the attribute as
“abs:href” to be more selective.
## Web Spidering Using the Jericho Library
Here is another web spidering example that is different than the earlier example using the jsoup
library. Here we will implement a spider using built in Java standard library network classes and
also the Jericho HTML parser library.
Information Gathering
package com.markwatson.info_spiders;
import net.htmlparser.jericho.*;
import java.io.InputStream;
import java.net.URL;
import java.net.URLConnection;
import java.util.*;
* This simple web spider returns a list of lists, each containing two strings
* representing "URL" and "text".
* Specifically, I do not return links on each page.
* Copyright Mark Watson 2008-2020. All Rights Reserved.
* License: Apache 2
public class WebSpider {
public WebSpider(String root_url, int max_returned_pages) throws Exception {
String host = new URL(root_url).getHost();
System.out.println("+ host: " + host);
List<String> urls = new ArrayList<String>();
Set<String> already_visited = new HashSet<String>();
urls.add(root_url);
int num_fetched = 0;
while (num_fetched <= max_returned_pages && !urls.isEmpty()) {
System.out.println("+ urls: " + urls);
String url_str = urls.remove(0);
System.out.println("+ url_str: " + url_str);
if (url_str.toLowerCase().indexOf(host) > -1 &&
!already_visited.contains(url_str)) {
already_visited.add(url_str);
URL url = new URL(url_str);
URLConnection connection = url.openConnection();
connection.setAllowUserInteraction(false);
InputStream ins = url.openStream();
Source source = new Source(ins);
num_fetched++;
TextExtractor te = new TextExtractor(source);
Information Gathering
String text = te.toString();
// Skip any pages where text on page is identical to existing
// page (e.g., http://example.com and http://exaple.com/index.html
boolean process = true;
for (List<String> ls : url_content_lists) {
if (text.equals(ls.get(1))) {
process = false;
break;
if (process) {
Thread.sleep(500);
} catch (Exception ignore) {
List<StartTag> anchorTags = source.getAllStartTags("a ");
ListIterator iter = anchorTags.listIterator();
while (iter.hasNext()) {
StartTag anchor = (StartTag) iter.next();
Attributes attr = anchor.parseAttributes();
Attribute link = attr.get("href");
String link_str = link.getValue();
if (link_str.indexOf("http:") == -1) {
String path = url.getPath();
if (path.endsWith("/")) path = path.substring(0, path.length() - 1);
int index = path.lastIndexOf("/");
if (index > -1) path = path.substring(0, index);
link_str = url.getHost() + "/" + path + "/" + link_str;
link_str = "http://" + link_str.replaceAll("///", "/").replaceAll("/\
/", "/");
urls.add(link_str);
List<String> ls = new ArrayList<String>(2);
ls.add(url_str);
ls.add(text);
url_content_lists.add(ls);
} catch (Exception ex) {
System.out.println("Error: " + ex);
ex.printStackTrace();
Information Gathering
public List<List<String>> url_content_lists = new ArrayList<List<String>>();
The test class WebClientTest shows how to use this class:
WebSpider client = new WebSpider("http://pbs.org", 10);
System.out.println("Found URIs: " + client.url_content_lists);
Here is the output for the test class WebClientTest:
+ host: pbs.org
+ urls: [http://pbs.org]
+ url_str: http://pbs.org
Found URIs: [[http://pbs.org, ]]
DBPedia Entity Lookup
DBPedia contains structured data for the WikiPedia web site. In later chapters we will learn how
to use the SPARQL query language to access DBPedia. Here we use a simple lookup service for any
entity name. If an entity name is found in DBPedia then information on the entity is returned as an
XML payload.
The implementation file is DBpediaLookupClient.java:
package com.markwatson.info_spiders;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;
import javax.xml.parsers.SAXParser;
import javax.xml.parsers.SAXParserFactory;
import java.util.*;
* Copyright Mark Watson 2008-2020. All Rights Reserved.
Information Gathering
* License: Apache-2.0
// Use Georgi Kobilarov's DBpedia lookup web service
ref: http://lookup.dbpedia.org/api/search.asmx?op=KeywordSearch
example:
// http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?QueryString=Flagstaff
* Searches return results that contain any of the search terms. I am going to filter
* the results to ignore results that do not contain all search terms.
public class DBpediaLookupClient extends DefaultHandler {
public DBpediaLookupClient(String query) throws Exception {
this.query = query;
CloseableHttpClient client = HttpClients.createDefault();
String query2 = query.replaceAll(" ", "+");
SAXParserFactory factory = SAXParserFactory.newInstance();
SAXParser sax = factory.newSAXParser();
sax.parse(
"http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?QueryString=" +
query2, this);
private List<Map<String, String>> variableBindings =
new ArrayList<Map<String, String>>();
private Map<String, String> tempBinding = null;
private String lastElementName = null;
public void startElement(String uri, String localName, String qName,
Attributes attributes) throws SAXException {
if (qName.equalsIgnoreCase("result")) {
tempBinding = new HashMap<String, String>();
lastElementName = qName;
public void endElement(String uri, String localName, String qName)
Information Gathering
throws SAXException {
if (qName.equalsIgnoreCase("result")) {
if (!variableBindings.contains(tempBinding) &&
containsSearchTerms(tempBinding))
variableBindings.add(tempBinding);
public void characters(char[] ch, int start, int length) throws SAXException {
String s = new String(ch, start, length).trim();
if (s.length() > 0) {
if ("Description".equals(lastElementName)) {
if (tempBinding.get("Description") == null) {
tempBinding.put("Description", s);
tempBinding.put("Description", "" + tempBinding.get("Description") +
" " + s);
if ("URI".equals(lastElementName) && s.indexOf("Category")==-1 &&
tempBinding.get("URI") == null) {
tempBinding.put("URI", s);
if ("Label".equals(lastElementName)) tempBinding.put("Label", s);
public List<Map<String, String>> variableBindings() {
return variableBindings;
private boolean containsSearchTerms(Map<String, String> bindings) {
StringBuilder sb = new StringBuilder();
for (String value : bindings.values()) sb.append(value);
// no white space
String text = sb.toString().toLowerCase();
StringTokenizer st = new StringTokenizer(this.query);
while (st.hasMoreTokens()) {
if (text.indexOf(st.nextToken().toLowerCase()) == -1) {
return false;
return true;
private String query = "";
Information Gathering
The unit test class DBpediaLookupClientTest shows how to call this library:
DBpediaLookupClient client =
new DBpediaLookupClient("London UK");
List<Map<String, String>> results = client.variableBindings();
System.out.println("# query results: " + results.size());
for (Map<String, String> map : results) {
for (Map.Entry<String, String> entry : map.entrySet()) {
System.out.println(entry.getKey() + " - " + entry.getValue());
Here is the output from this test code (with some output not shown):
# query results: 2
Description - The O2 Arena (visually typeset in branding as The O2 arena, referred t\
o as North Greenwich Arena in context of the 2012 Summer Olympics and Paralympics) i\
s a multi-purpose indoor arena located in the centre of The O2, a large entertainmen\
t complex on the Greenwich peninsula in London, England.
Label - Sports venues in London
URI - http://dbpedia.org/resource/The_O2_Arena_(London)
Description - The City of London was a United Kingdom Parliamentary constituency. It\
was a constituency of the House of Commons of the Parliament of England then of the\
Parliament of Great Britain from 1707 to 1800 and of the Parliament of the United K\
ingdom from 1801 to 1950. The City of London was a United Kingdom Parliamentary cons\
tituency.
Label - United Kingdom Parliamentary constituencies represented by a sitting Prime M\
inister
URI - http://dbpedia.org/resource/City_of_London_(UK_Parliament_constituency)
Client for GeoNames Service
The GeoNames service looks up information about locations. You need to [sign up for a free
account])http://www.geonames.org/login) and the access key needs to be stored in an environment
variable GEONAMES which is accessed in Java code using:
System.getenv("GEONAMES")
The implementation file is GeoNamesClient.java uses the utility class GeoNameData that we will
look at later:
Information Gathering
package com.markwatson.info_spiders;
import org.geonames.*;
import java.util.ArrayList;
import java.util.List;
* Copyright Mark Watson 2008-2020. All Rights Reserved.
* License: Apache 2
// You will need a free GeoNames account. Sign up:
https://www.geonames.org/login
// Then, set an environment variable: export GEONAMES=your-geonames-account-name
public class GeoNamesClient {
public GeoNamesClient() {
private List<GeoNameData> helper(String name, String type) throws Exception {
List<GeoNameData> ret = new ArrayList<GeoNameData>();
String geonames_account_name = System.getenv("GEONAMES");
if (geonames_account_name == null) {
System.err.println("You will need a free GeoNames account.");
System.err.println("Sign up:
https://www.geonames.org/login");
System.err.println("Then, set an environment variable:");
System.err.println("
export GEONAMES=your-geonames-account-name");
throw new Exception("Need API key");
WebService.setUserName(System.getenv("GEONAMES"));
ToponymSearchCriteria searchCriteria = new ToponymSearchCriteria();
searchCriteria.setStyle(Style.LONG);
searchCriteria.setQ(name);
ToponymSearchResult searchResult = WebService.search(searchCriteria);
for (Toponym toponym : searchResult.getToponyms()) {
if (toponym.getFeatureClassName() != null &&
toponym.getFeatureClassName().toString().indexOf(type) > -1 &&
toponym.getName().indexOf(name) > -1 &&
valid(toponym.getName())) {
ret.add(new GeoNameData(toponym));
Information Gathering
return ret;
private boolean valid(String str) {
if (str.contains("0")) return false;
if (str.contains("1")) return false;
if (str.contains("2")) return false;
if (str.contains("3")) return false;
if (str.contains("4")) return false;
if (str.contains("5")) return false;
if (str.contains("6")) return false;
if (str.contains("7")) return false;
if (str.contains("8")) return false;
return !str.contains("9");
public List<GeoNameData> getCityData(String city_name) throws Exception {
return helper(city_name, "city");
public List<GeoNameData> getCountryData(String country_name) throws Exception {
return helper(country_name, "country");
public List<GeoNameData> getStateData(String state_name) throws Exception {
List<GeoNameData> states = helper(state_name, "state");
for (GeoNameData state : states) {
state.geoType = GeoNameData.GeoType.STATE;
return states;
public List<GeoNameData> getRiverData(String river_name) throws Exception {
return helper(river_name, "stream");
public List<GeoNameData> getMountainData(String mountain_name) throws Exception {
return helper(mountain_name, "mountain");
The class GeoNamesClient in the last listing uses the class GeoNameData which processes the
Information Gathering
structured data returned from the GeoNames service and provides public fields to access this
information and an implementation of toString to pretty-print the data to a string:
package com.markwatson.info_spiders;
import org.geonames.Toponym;
* Copyright Mark Watson 2008-2020. All Rights Reserved.
* License: Apache-2.0
public class GeoNameData {
public enum GeoType {
CITY, COUNTRY, STATE, RIVER, MOUNTAIN, UNKNOWN
public int geoNameId = 0;
public GeoType geoType = GeoType.UNKNOWN;
public String name = "";
public double latitude = 0;
public double longitude = 0;
public String countryCode = "";
public GeoNameData(Toponym toponym) {
geoNameId = toponym.getGeoNameId();
latitude = toponym.getLatitude();
longitude = toponym.getLongitude();
name = toponym.getName();
countryCode = toponym.getCountryCode();
if (toponym.getFeatureClassName().startsWith("city")) geoType = GeoType.CITY;
if (toponym.getFeatureClassName().startsWith("country"))
geoType = GeoType.COUNTRY;
if (toponym.getFeatureClassName().startsWith("state")) geoType = GeoType.STATE;
if (toponym.getFeatureClassName().startsWith("stream")) geoType = GeoType.RIVER;
if (toponym.getFeatureClassName().startsWith("mountain"))
geoType = GeoType.MOUNTAIN;
public GeoNameData() {
public String toString() {
return "[GeoNameData: " + name + ", type: " + geoType + ", country code: " + cou\
ntryCode +
Information Gathering
", ID: " + geoNameId + ", latitude: " + latitude +
", longitude: " + longitude + "]";
The test class GeoNamesClientTest shows how to use these two classes:
GeoNamesClient client = new GeoNamesClient();
System.out.println(client.getCityData("Paris"));
pause();
System.out.println(client.getCountryData("Canada"));
pause();
System.out.println(client.getStateData("California")); pause();
System.out.println(client.getRiverData("Amazon"));
pause();
System.out.println(client.getMountainData("Whitney"));
The output from this test is shown below:
[[GeoNameData: Paris, type: CITY, country code: FR, ID: 2988507, latitude: 48.85341,\
longitude: 2.3488], [GeoNameData: Le Touquet-Paris-Plage, type: CITY, country code:\
FR, ID: 2999139, latitude: 50.52432, longitude: 1.58571], [GeoNameData: Paris, type\
: CITY, country code: US, ID: 4717560, latitude: 33.66094, longitude: -95.55551], [G\
eoNameData: Balneario Nuevo Paris, type: CITY, country code: UY, ID: 3441475, latitu\
de: -34.85, longitude: -56.23333], [GeoNameData: Paris, type: CITY, country code: BY\
, ID: 8221628, latitude: 55.15464, longitude: 27.38456], [GeoNameData: Paris, type: \
CITY, country code: TG, ID: 2364431, latitude: 7.15, longitude: 1.08333]]
[[GeoNameData: Canada, type: COUNTRY, country code: CA, ID: 6251999, latitude: 60.10\
867, longitude: -113.64258], [GeoNameData: Canada Bay, type: COUNTRY, country code: \
AU, ID: 7839706, latitude: -33.8659, longitude: 151.11591]]
[[GeoNameData: Baja California Sur, type: STATE, country code: MX, ID: 4017698, lati\
tude: 25.83333, longitude: -111.83333], [GeoNameData: Baja California, type: STATE, \
country code: MX, ID: 4017700, latitude: 30.0, longitude: -115.0], [GeoNameData: Cal\
ifornia, type: STATE, country code: US, ID: 5332921, latitude: 37.25022, longitude: \
-119.75126]]
[[GeoNameData: Amazon Bay, type: RIVER, country code: PG, ID: 2133985, latitude: -10\
.30264, longitude: 149.36313]]
[[GeoNameData: Mount Whitney, type: MOUNTAIN, country code: US, ID: 5409018, latitud\
e: 36.57849, longitude: -118.29194], [GeoNameData: Whitney Peak, type: MOUNTAIN, cou\
ntry code: AQ, ID: 6628058, latitude: -76.43333, longitude: -126.05], [GeoNameData: \
Whitney Point, type: MOUNTAIN, country code: AQ, ID: 6628059, latitude: -66.25, long\
itude: 110.51667], [GeoNameData: Whitney Island, type: MOUNTAIN, country code: RU, I\
D: 1500850, latitude: 81.01149, longitude: 60.88737], [GeoNameData: Whitney Island, \
type: MOUNTAIN, country code: AQ, ID: 6628055, latitude: -69.66187, longitude: -68.5\
0341], [GeoNameData: Whitney Meadow, type: MOUNTAIN, country code: US, ID: 5409010, \
Information Gathering
latitude: 36.43216, longitude: -118.26648], [GeoNameData: Whitney Peak, type: MOUNTA\
IN, country code: US, ID: 5444110, latitude: 39.43276, longitude: -106.47309], [GeoN\
ameData: Whitney Portal, type: MOUNTAIN, country code: US, ID: 5409011, latitude: 36\
.58882, longitude: -118.22592], [GeoNameData: Whitney Mountain, type: MOUNTAIN, coun\
try code: US, ID: 4136375, latitude: 36.40146, longitude: -93.91742], [GeoNameData: \
Whitney Bridge Dip, type: MOUNTAIN, country code: AU, ID: 11878190, latitude: -28.61\
241, longitude: 153.16546], [GeoNameData: Whitney Point, type: MOUNTAIN, country cod\
e: US, ID: 5815920, latitude: 47.76037, longitude: -122.85127], [GeoNameData: Whitne\
y Pass, type: MOUNTAIN, country code: US, ID: 5409024, latitude: 36.55577, longitude\
: -118.2812], [GeoNameData: Whitney Island, type: MOUNTAIN, country code: CA, ID: 61\
81293, latitude: 58.6505, longitude: -78.71621]]
Wrap-up for Information Gathering
Access to data is an advantage large companies usually have over individuals and small organiza-
tions. That said, there is a lot of free information on the web and I hope my simple utility classes we
have covered here will be of some use to you.
I respect the rights of people and organizations who put information on the web. This includes:
• Read the terms of service on web sites to make sure your your of the site’s data is compliant
and also avoid accessing any one web site too frequently.
• When you access services like DBpedia and Geonames consider caching the results so that you
don’t ask the service for the same information repeatedly. This is particularly important during
development and testing. In a later chapter we will see how to use the Apache Derby database
to cache SPARQL queries to the DBPedia service.
Resolve Entity Names to DBPedia
References
As a personal research project I have collected a large data set that maps entity names (e.g., people’s
names, city names, names of music groups, company names, etc.) to the DBPedia URI for each entity.
I have developed libraries to use this data in Common Lisp⁵⁷, Haskell⁵⁸, and Java. Here we use the
Java version of this library.
The Java library is found in the directory ner_dbpedia in the GitHub repository. The raw data for
these entity to URI mappings are found in the directory ner_dbpedia/dbpedia_as_text.
This example shows the use of a standard Java and Maven packaging technique: building a JAR file
that contains resource files in addition to compiled Java code. The example code reads the required
data resources from the JAR file (or the temporary target directory during development). This makes
the JAR file self contained when we use this example library in later chapters.
DBPedia Entities
DBPedia is the structured RDF database that is automatically created from WikiPedia info boxes. We
will go into some detail on RDF data in the later chapter Semantic Web. The raw data for these entity
to URI mappings is found in the directory ner_dbpedia/dbpedia_as_text files have the format (for
people in this case):
Al Stewart
<http://dbpedia.org/resource/Al_Stewart>
Alan Watts
<http://dbpedia.org/resource/Alan_Watts>
If you visit any or these URIs using a web browser, for example http://dbpedia.org/page/Al_Stewart⁵⁹
you will see the DBPedia data for the entity formatted for human reading but to be clear the primary
purpose of information in DBPedia is for use by software, not humans.
There are 58953 entities defined with their DBPedia URI and the following listing shows the
breakdown of number of entities by entity type by counting the number of lines in each resource
⁵⁷https://leanpub.com/lovinglisp
⁵⁸https://leanpub.com/haskell-cookbook
⁵⁹http://dbpedia.org/page/Al_Stewart
Resolve Entity Names to DBPedia References
ner_dbpedia: $ wc -l ./src/main/resources/*.txt
108 ./src/main/resources/BroadcastNetworkNamesDbPedia.txt
2580 ./src/main/resources/CityNamesDbpedia.txt
1786 ./src/main/resources/CompanyNamesDbPedia.txt
167 ./src/main/resources/CountryNamesDbpedia.txt
14315 ./src/main/resources/MusicGroupNamesDbPedia.txt
35606 ./src/main/resources/PeopleDbPedia.txt
555 ./src/main/resources/PoliticalPartyNamesDbPedia.txt
351 ./src/main/resources/TradeUnionNamesDbPedia.txt
3485 ./src/main/resources/UniversityNamesDbPedia.txt
58953 total
The URI for each entity defines a unique identifier for real world entities as well as concepts. ##
Library Implementation
The following UML class diagram shows the APIs and fields for the two classes in the package
com.markwatson.ner_dbpedia for this example: NerMaps and TextToDbpediaUris:
Resolve Entity Names to DBPedia References
Resolve Entity Names to DBPedia References
As you see in the following figure showing the IntelliJ Community Edition project for this example,
there are nine text files, one for each entity type in the directory src/main/resources. Later we will
look at the code required to read these files in two cases:
• During development these files are read from target/classes.
• During client application use of the JAR file (created using mvn install) these files are read as
resources from the Java class loader.
IDE View of Project
The class com.markwatson.ner_dbpedia.NerMaps is a utility for reading the raw entity mapping
data files and creating hash tables for these mappings:
package com.markwatson.ner_dbpedia;
import java.io.*;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Stream;
Resolve Entity Names to DBPedia References
* Copyright Mark Watson 2020. Apache 2 license,
public class NerMaps {
private static String enforceAngleBrackets(String s) {
if (s.startsWith("<")) return s;
return "<" + s + ">";
private static Map<String, String> textFileToMap(String nerFileName) {
Map<String, String> ret = new HashMap<String, String>();
InputStream in = ClassLoader.getSystemResourceAsStream(nerFileName);
BufferedReader reader = new BufferedReader(new InputStreamReader(in));
List<String> lines = new ArrayList<String>();
String line2;
while((line2 = reader.readLine()) != null) {
lines.add(line2);
reader.close();
lines.forEach(line -> {
String[] tokens = line.split("\t");
if (tokens.length > 1) {
ret.put(tokens[0], enforceAngleBrackets(tokens[1]));
} catch (Exception ex) {
ex.printStackTrace();
return ret;
static public final Map<String, String> broadcastNetworks =
textFileToMap("BroadcastNetworkNamesDbPedia.txt");
static public final Map<String, String> cityNames =
textFileToMap("CityNamesDbpedia.txt");
static public final Map<String, String> companyames =
textFileToMap("CompanyNamesDbPedia.txt");
static public final Map<String, String> countryNames =
textFileToMap("CountryNamesDbpedia.txt");
static public final Map<String, String> musicGroupNames =
textFileToMap("MusicGroupNamesDbPedia.txt");
Resolve Entity Names to DBPedia References
static public final Map<String, String> personNames =
textFileToMap("PeopleDbPedia.txt");
static public final Map<String, String> politicalPartyNames =
textFileToMap("PoliticalPartyNamesDbPedia.txt");
static public final Map<String, String> tradeUnionNames =
textFileToMap("TradeUnionNamesDbPedia.txt");
static public final Map<String, String> universityNames =
textFileToMap("UniversityNamesDbPedia.txt");
The class com.markwatson.ner_dbpedia.TextToDbpediaUris processes an input string and uses
public fields to output found entity names and matching DBPedia URIs. We will use this code later
in the chapter Automatically Generating Data for Knowledge Graphs.
The code in the class TextToDbpediaUris is simple and repeats two common patterns for each entity
type. We will look at some of the code here.
package com.markwatson.ner_dbpedia;
import java.util.ArrayList;
import java.util.List;
public class TextToDbpediaUris {
private TextToDbpediaUris() {
public List<String> personUris = new ArrayList<String>();
public List<String> personNames = new ArrayList<String>();
public List<String> companyUris = new ArrayList<String>();
public List<String> companyNames = new ArrayList<>();
The empty constructor is private since it makes no sense to create an instance of TextToDbpediaUris
without text input. The code supports nine entity types. Here we show the definition of public output
fields for just two entity types (people and companies).
As a matter of programming style I generally no longer use getter and setter methods, preferring a
more concise coding style. I usually make output fields package default visibility (i.e., no private or
public specification so the fields are public within a package and private from other packages). Here
I make them public because the package nerdbpedia developed here is meant to be used by other
packages. If you prefer using getter and setter methods, modern IDEs like IntelliJ and Eclipse can
generate those for you for the example code in this book.
We will handle entity names comprised of one, two, and three word sequences. We check for longer
word sequences before shorter sequences:
Resolve Entity Names to DBPedia References
public TextToDbpediaUris(String text) {
String[] tokens = tokenize(text + " . . .");
String uri = "";
for (int i = 0, size = tokens.length - 2; i < size; i++) {
String n2gram = tokens[i] + " " + tokens[i + 1];
String n3gram = n2gram + " " + tokens[i + 2];
// check for 3grams:
if ((uri = NerMaps.personNames.get(n3gram)) != null) {
log("person", i, i + 2, n3gram, uri);
i += 2;
continue;
The class NerMaps that we previously saw listed converts text files of entities to DBPedia URIs
mappings to Java hash maps. The method log does two things:
• Prints out the entity type, the word indices from the original tokenized text, the entity name
as a single string (combine tokens for an entity to a string), and the DBPedia URI.
• Saves entity mapping in the public fields personUris, personNames, etc.
After we check for three word entity names, we process two word names, and one word names.
Here is an example:
// check for 2grams:
if ((s = NerMaps.personNames.get(n2gram)) != null) {
log("person", i, i + 1, n2gram, s);
i += 1;
continue;
The following listing shows the log method that write descriptive output and saves entity mappings.
We only show the code for the entity type person:
Resolve Entity Names to DBPedia References
public void log(String nerType, int index1, int index2, String ngram, String uri) {
System.out.println(nerType + "\t" + index1 + "\t" + index2 + "\t" +
ngram + "\t" + uri);
if (!uri.startsWith("<")) uri = "<" + uri + ">";
if (nerType.equals("person")) {
if (!personUris.contains(uri)) {
personUris.add(uri);
personNames.add(ngram);
For some NLP applications I will use a standard tokenizer like the OpenNLP tokenizer that we used
in two previous chapters. Here, I simply add spaces around punctuation characters and use the Java
string split method:
private String[] tokenize(String s) {
return s.replaceAll("\\.", " \\. ").
replaceAll(",", " , ").
replaceAll("\\?", " ? ").
replaceAll("\n", " ").
replaceAll(";", " ; ").split(" ");
The following listing shows the code snippet from the unit test code in the class TextToDbpediaU-
risTest that calls the TextToDbpediaUris constructor with a text sample (junit boilerplate code is
not shown):
package com.markwatson.ner_dbpedia;
* Test that is just for side effect printouts:
public void test1() throws Exception {
String s = "PTL Satellite Network covered President Bill Clinton going to "
+ " Guatemala and visiting the Coca Cola Company.";
TextToDbpediaUris test = new TextToDbpediaUris(s);
On line 11, the object test contains public fields for accessing the entity names and corresponding
URIs. We will use these fields in the later chapters Automatically Generating Data for Knowledge
Graphs and Knowledge Graph Navigator.
Resolve Entity Names to DBPedia References
Here is the output from running the unit test code:
broadcastNetwork 0 2 PTL Satellite Network <http://dbpedia.org/resource/PTL_Satellit\
e_Network>
person
Bill Clinton
<http://dbpedia.org/resource/Bill_Clinton>
country
Guatemala
<http://dbpedia.org/resource/Guatemala>
company
Coca Cola
<http://dbpedia.org/resource/Coca-Cola>
Wrap-up for Resolving Entity Names to DBPedia
References
The idea behind this example is simple but useful for information processing applications using raw
text input. We will use this library later in two semantic web examples.
Semantic Web
We will start with a tutorial on semantic web data standards like RDF, RDFS, and OWL, then
implement a wrapper for the Apache Jena library, and finally take a deeper dive into some examples.
You will learn how to do the following:
• Understand RDF data formats.
• Understand SPARQL queries for RDF data stores (both local and remote).
• Use the Apache Jena library to use local RDF data and perform SPARQL queries that return
pure Java data structures.
• Use the Apache Jena library to query remote SPARQL endpoints like DBPedia and WikiData.
• Use the Apache Derby relational database to cache SPARQL remote queries for both efficiency
and for building systems that may have intermittent access to the Internet.
• Take a deeper dive into RDF, RDFS, and OWL reasoners.
The semantic web is intended to provide a massive linked set of data for use by software systems
just as the World Wide Web provides a massive collection of linked web pages for human reading
and browsing. The semantic web is like the web in that anyone can generate any content that they
want. This freedom to publish anything works for the web because we use our ability to understand
natural language to interpret what we read – and often to dismiss material that based upon our own
knowledge we consider to be incorrect.
Semantic web and linked data technologies are also useful for smaller amounts of data, an example
being a Knowledge Graph containing information for a business. We will further explore Knowledge
Graphs in the next two chapters.
The core concept for the semantic web is data integration and use from different sources. As we will
soon see, the tools for implementing the semantic web are designed for encoding data and sharing
data from many different sources.
I cover the semantic web in this book because I believe that semantic web technologies are
complementary to AI systems for gathering and processing data on the web. As more web pages
are generated by applications (as opposed to simply showing static HTML files) it becomes easier to
produce both HTML for human readers and semantic data for software agents.
There are several very good semantic web toolkits for the Java language and platform. Here we use
Apache Jena because it is what I often use in my own work and I believe that it is a good starting
technology for your first experiments with semantic web technologies. This chapter provides an
incomplete coverage of semantic web technologies and is intended as a gentle introduction to a
few useful techniques and how to implement those techniques in Java. This chapter is the start of
Semantic Web
a journey in the technology that I think is as important as technologies like deep learning that get
more public mindshare.
The following figure shows a layered hierarchy of data models that are used to implement semantic
web applications. To design and implement these applications we need to think in terms of physical
models (storage and access of RDF, RDFS, and perhaps OWL data), logical models (how we use
RDF and RDFS to define relationships between data represented as unique URIs and string literals
and how we logically combine data from different sources) and conceptual modeling (higher level
knowledge representation and reasoning using OWL). Originally RDF data was serialized as XML
data but other formats have become much more popular because they are easier to read and manually
create. The top three layers in the figure might be represented as XML, or as LD-JSON (linked data
JSON) or formats like N-Triples and N3 that we will use later.
Semantic Web Data Models
This chapter is meant to get you interested in this technology but is not intended as a complete guide.
RDF data is the bedrock of the semantic web. I am also lightly covering RDFS/OWL modeling, and
Descriptive Logic Reasoners which are important topics for more advanced semantic web projects.
Available Tools
In the previous edition of this book I used the open source Sesame library for the material on RDF.
Sesame is now called RDF4J and is part of the Eclipse organization’s projects.
I decided to use the Apache Jena project in this new edition because I think Jena is slightly easier to
set up a light weight development environment. If you need to set up an RDF server I recommend
Semantic Web
using the Fuseki⁶⁰ server which is part of the Apache Jena project. For client applications we will use
the Jena library for working with RDF and performing SPARQL queries using the example classss
JenaApis that we implement later and also for querying remote SPARQL endpoints (i.e., public RDF
data sources with SPARQL query interfaces) like DBPedia and WikiData.
Relational Database Model Has Problems Dealing with
Rapidly Changing Data Requirements
When people are first introduced to semantic web technologies their first reaction is often something
like, “I can just do that with a database.” The relational database model is an efficient way to express
and work with slowly changing data schemas. There are some clever tools for dealing with data
change requirements in the database world (ActiveRecord and migrations being a good example)
but it is awkward to have end users and even developers tagging on new data attributes to relational
database tables.
This same limitation also applies to object oriented programming and object modeling. Even with
dynamic languages that facilitate modifying classes at runtime, the options for adding attributes
to existing models are just too limiting. The same argument can be made against the use of XML
constrained by conformance to either DTDs or XML Schemas. It is true that RDF and RDFS can be
serialized to XML using many pre existing XML namespaces for different knowledge sources and
schemas but it turns out that this is done in a way that does not reduce the flexibility for extending
data models. XML storage is really only a serialization of RDF and many developers who are just
starting to use semantic web technologies initially get confused trying to read XML serialization of
RDF – almost like trying to read a PDF file with a plain text editor and something to be avoided. We
will use the N-Triple and N3 formats that are simpler to read and understand.
One goal for the rest of this chapter is convincing you that modeling data with RDF and RDFS
facilitates freely extending data models and also allows fairly easy integration of data from different
sources using different schemas without explicitly converting data from one schema to another for
reuse. You are free to add new data properties and add information to existing graphs (which we
refer to a models).
RDF: The Universal Data Format
The Resource Description Framework (RDF) is used to encode information and the RDF Schema
(RDFS) facilitates using data with different RDF encodings without the need to convert one set
of schemas to another. Later, using OWL we can simply declare that one predicate is the same as
another, that is, one predicate is a sub-predicate of another (e.g., a property containsCity can be
declared to be a sub-property of containsPlace so if something contains a city then it also contains
a place), etc. The predicate part of an RDF statement often refers to a property.
⁶⁰https://jena.apache.org/documentation/fuseki2/
Semantic Web
RDF data was originally encoded as XML and intended for automated processing. In this chapter we
will use two simple to read formats called “N-Triples” and “N3.” Apache Jena can be used to convert
between all RDF formats so we might as well use formats that are easier to read and understand.
RDF data consists of a set of triple values:
• subject
• predicate
• object
Some of my work with semantic web technologies deals with processing news stories, extracting
semantic information from the text, and storing it in RDF. I will use this application domain for the
examples in this chapter and the next chapter when we implement code to automatically generate
RDF for Knowledge Graphs. I deal with triples like:
• subject: a URL (or URI) of a news article.
• predicate: a relation like “containsPerson”.
• object: a literal value like “Bill Clinton” or a URI representing Bill Clinton.
In the next chapter we will use the entity recognition library we developed in an earlier chapter to
create RDF from text input.
We will use either URIs or string literals as values for objects. We will always use URIs for
representing subjects and predicates. In any case URIs are usually preferred to string literals. We
will see an example of this preferred use but first we need to learn the N-Triple and N3 RDF formats.
I proposed the idea that RDF was more flexible than Object Modeling in programming languages,
relational databases, and XML with schemas. If we can tag new attributes on the fly to existing
data, how do we prevent what I might call “data chaos” as we modify existing data sources? It turns
out that the solution to this problem is also the solution for encoding real semantics (or meaning)
with data: we usually use unique URIs for RDF subjects, predicates, and objects, and usually with
a preference for not using string literals. The definitions of predicates are tied to a namespace and
later with OWL we will state the equivalence of predicates in different namespaces with the same
semantic meaning. I will try to make this idea more clear with some examples and Wikipedia has a
good writeup on RDF⁶¹.
Any part of a triple (subject, predicate, or object) is either a URI or a string literal. URIs encode
namespaces. For example, the containsPerson predicate in the last example could be written as:
http://knowledgebooks.com/ontology/#containsPerson
The first part of this URI is considered to be the namespace for this predicate “containsPerson.”
When different RDF triples use this same predicate, this is some assurance to us that all users of this
⁶¹https://en.wikipedia.org/wiki/Resource_Description_Framework
Semantic Web
predicate understand to the same meaning. Furthermore, we will see later that we can use RDFS to
state equivalency between this predicate (in the namespace http://knowledgebooks.com/ontology/)
with predicates represented by different URIs used in other data sources. In an “artificial intelligence”
sense, software that we write does not understand predicates like “containsCity”, “containsPerson”,
or “isLocation” in the way that a human reader can by combining understood common meanings
for the words “contains”, “city”, “is”, “person”, and “location” but for many interesting and useful
types of applications that is fine as long as the predicate is used consistently. We will see shortly
that we can define abbreviation prefixes for namespaces which makes RDF and RDFS files shorter
and easier to read.
The Jena library supports most serialization formats for RDF:
• Turtle
• N-Triples
• NQuads
• TriG
• JSON-LD
• RDF/XML
• RDF/JSON
• TriX
• RDF Binary
A statement in N-Triple format consists of three URIs (two URIs and a string literals for the object)
followed by a period to end the statement. While statements are often written one per line in a source
file they can be broken across lines; it is the ending period which marks the end of a statement. The
standard file extension for N-Triple format files is *.nt and the standard format for N3 format files
is *.n3.
My preference is to use N-Triple format files as output from programs that I write to save data as
RDF. N-Triple files don’t use any abbreviations and each RDF statement is self-contained. I often
use tools like the command line commands in Jena or RDF4J to convert N-Triple files to N3 or other
formats if I will be reading them or even hand editing them. Here is an example using the N3 syntax:
@prefix kb:
<http://knowledgebooks.com/ontology#>
<http://news.com/201234/> kb:containsCountry "China" .
The N3 format adds prefixes (abbreviations) to the N-Triple format. In practice it would be better to
use the URI http://dbpedia.org/resource/China instead of the literal value “China.”
Here we see the use of an abbreviation prefix “kb:” for the namespace for my company Knowledge-
Books.com ontologies. The first term in the RDF statement (the subject) is the URI of a news article.
The second term (the predicate) is “containsCountry” in the “kb:” namespace. The last item in the
Semantic Web
statement (the object) is a string literal “China.” I would describe this RDF statement in English as,
“The news article at URI http://news.com/201234 mentions the country China.”
This was a very simple N3 example which we will expand to show additional features of the N3
notation. As another example, let’s look at the case if this news article also mentions the USA. Instead
of adding a whole new statement like this we can combine them using N3 notation. Here we have
two separate RDF statements:
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/China>
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/United_States>
We can collapse multiple RDF statements that share the same subject and optionally the same
predicate:
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/China> ,
<http://dbpedia.org/resource/United_States>
The indentation and placement on separate lines is arbitrary - use whatever style you like that is
readable. We can also add in additional predicates that use the same subject (I am going to use string
literals here instead of URIs for objects to make the following example more concise but in practice
prefer using URIs):
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/>
kb:containsCountry "China" ,
"USA" .
kb:containsOrganization "United Nations" ;
kb:containsPerson "Ban Ki-moon" , "Gordon Brown" ,
"Hu Jintao" , "George W. Bush" ,
"Pervez Musharraf" ,
"Vladimir Putin" ,
"Mahmoud Ahmadinejad" .
Semantic Web
This single N3 statement represents ten individual RDF triples. Each section defining triples with
the same subject and predicate have objects separated by commas and ending with a period. Please
note that whatever RDF storage system you use (we will be using Jena) it makes no difference if we
load RDF as XML, N-Triple, of N3 format files: internally subject, predicate, and object triples are
stored in the same way and are used in the same way. RDF triples in a data store represent directed
graphs that may not all be connected.
I promised you that the data in RDF data stores was easy to extend. As an example, let us assume
that we have written software that is able to read online news articles and create RDF data that
captures some of the semantics in the articles. If we extend our program to also recognize dates
when the articles are published, we can simply reprocess articles and for each article add a triple to
our RDF data store using a form like:
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/> kb:datePublished "2008-05-11" .
Here we just represent the date as a string. We can add a type to the object representing a specific
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/> kb:datePublished "2008-05-11"^^xsd:date .
Furthermore, if we do not have dates for all news articles that is often acceptable because when
constructing SPARQL queries you can match optional patterns. If for example you are looking up
articles on a specific subject then some results may have a publication date attached to the results
for that article and some might not. In practice RDF supports types and we would use a date type as
seen in the last example, not a string. However, in designing the example programs for this chapter
I decided to simplify our representation of URIs and often use string literals as simple Java strings.
For many applications this isn’t a real limitation.
Extending RDF with RDF Schema
RDF Schema (RDFS) supports the definition of classes and properties based on set inclusion. In
RDFS classes and properties are orthogonal. Let’s start with looking at an example using additional
namespaces:
Semantic Web
@prefix kb:
<http://knowledgebooks.com/ontology#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
@prefix rdfs:
<http://www.w3.org/2000/01/rdf-schema#>
@prefix dbo: <http://dbpedia.org/ontology/>
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/China>
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/United_States>
<http://dbpedia.org/resource/China>
rdfs:label "China"@en,
rdf:type dbo:Place ,
rdf:type dbo:Country .
Because the semantic web is intended to be processed automatically by software systems it is
encoded as RDF. There is a problem that must be solved in implementing and using the semantic
web: everyone who publishes semantic web data is free to create their own RDF schemas for storing
data; for example, there is usually no single standard RDF schema definition for topics like news
stories and stock market data. The SKOS⁶² is a namespace containing standard schemas and the
most widely used standard is schema.org⁶³. Understanding the ways of integrating different data
sources using different schemas helps to understand the design decisions behind the semantic web
applications. In this chapter I often use my own schemas in the knowledgebooks.com namespace for
the simple examples you see here. When you build your own production systems part of the work is
searching through schema.org and SKOS to use standard name spaces and schemas when possible.
The use of standard schemas helps when you link internal proprietary Knowledge Graphs used in
organization with public open data from sources like WikiData⁶⁴ and DBPedia⁶⁵.
We will start with an example that is an extension of the example in the last section that also uses
RDFS. We add a few additional RDF statements:
⁶²https://www.w3.org/2009/08/skos-reference/skos.html
⁶³https://schema.org/docs/schemas.html
⁶⁴https://www.wikidata.org/wiki/Wikidata:Main_Page
⁶⁵https://wiki.dbpedia.org/about
Semantic Web
@prefix kb:
<http://knowledgebooks.com/ontology#> .
@prefix rdfs:
<http://www.w3.org/2000/01/rdf-schema#> .
kb:containsCity rdfs:subPropertyOf kb:containsPlace .
kb:containsCountry rdfs:subPropertyOf kb:containsPlace .
kb:containsState rdfs:subPropertyOf kb:containsPlace .
The last three lines declare that:
• The property containsCity is a sub-property of containsPlace.
• The property containsCountry is a sub-property of containsPlace.
• The property containsState is a sub-property of containsPlace.
Why is this useful? For at least two reasons:
• You can query an RDF data store for all triples that use property containsPlace and also match
triples with properties equal to containsCity, containsCountry, or containsState. There may not
even be any triples that explicitly use the property containsPlace.
• Consider a hypothetical case where you are using two different RDF data stores that use
different properties for naming cities: cityName and city. You can define cityName to be a
sub-property of city and then write all queries against the single property name city. This
removes the necessity to convert data from different sources to use the same Schema. You can
also use OWL to state property and class equivalency.
In addition to providing a vocabulary for describing properties and class membership by properties,
RDFS is also used for logical inference to infer new triples, combine data from different RDF data
sources, and to allow effective querying of RDF data stores. We will see examples of all of these
features of RDFS when we later start using the Jena libraries to perform SPARQL queries.
The SPARQL Query Language
SPARQL is a query language used to query RDF data stores. While SPARQL may initially look
like SQL, we will see that there are some important differences like support for RDFS and OWL
inferencing and graph-based instead of relational matching operations. We will cover the basics of
SPARQL in this section and then see more examples later when we learn how to embed Jena in Java
applications, and see more examples in the last chapter Knowledge Graph Navigator.
We will use the N3 format RDF file test_data/news.n3 for the examples. I created this file
automatically by spidering Reuters news stories on the news.yahoo.com web site and automatically
extracting named entities from the text of the articles. We saw techniques for extracting named
entities from text in earlier chapters. In this chapter we use these sample RDF files.
You have already seen snippets of this file and I list the entire file here for reference, edited to fit
line width: you may find the file news.n3 easier to read if you are at your computer and open the
file in a text editor so you will not be limited to what fits on a book page:
Semantic Web
@prefix kb:
<http://knowledgebooks.com/ontology#> .
@prefix rdfs:
<http://www.w3.org/2000/01/rdf-schema#> .
kb:containsCity rdfs:subPropertyOf kb:containsPlace .
kb:containsCountry rdfs:subPropertyOf kb:containsPlace .
kb:containsState rdfs:subPropertyOf kb:containsPlace .
<http://yahoo.com/20080616/usa_flooding_dc_16/>
kb:containsCity "Burlington" , "Denver" ,
"St. Paul" ," Chicago" ,
"Quincy" , "CHICAGO" ,
"Iowa City" ;
kb:containsRegion "U.S. Midwest" , "Midwest" ;
kb:containsCountry "United States" , "Japan" ;
kb:containsState "Minnesota" , "Illinois" ,
"Mississippi" , "Iowa" ;
kb:containsOrganization "National Guard" ,
"U.S. Department of Agriculture" ,
"White House" ,
"Chicago Board of Trade" ,
"Department of Transportation" ;
kb:containsPerson "Dena Gray-Fisher" ,
"Donald Miller" ,
"Glenn Hollander" ,
"Rich Feltes" ,
"George W. Bush" ;
kb:containsIndustryTerm "food inflation" , "food" ,
"finance ministers" ,
"oil" .
<http://yahoo.com/78325/ts_nm/usa_politics_dc_2/>
kb:containsCity "Washington" , "Baghdad" ,
"Arlington" , "Flint" ;
kb:containsCountry "United States" ,
"Afghanistan" ,
"Iraq" ;
kb:containsState "Illinois" , "Virginia" ,
"Arizona" , "Michigan" ;
kb:containsOrganization "White House" ,
"Obama administration" ,
"Iraqi government" ;
Semantic Web
kb:containsPerson "David Petraeus" ,
"John McCain" ,
"Hoshiyar Zebari" ,
"Barack Obama" ,
"George W. Bush" ,
"Carly Fiorina" ;
kb:containsIndustryTerm "oil prices" .
<http://yahoo.com/10944/ts_nm/worldleaders_dc_1/>
kb:containsCity "WASHINGTON" ;
kb:containsCountry "United States" , "Pakistan" ,
"Islamic Republic of Iran" ;
kb:containsState "Maryland" ;
kb:containsOrganization "University of Maryland" ,
"United Nations" ;
kb:containsPerson "Ban Ki-moon" , "Gordon Brown" ,
"Hu Jintao" , "George W. Bush" ,
"Pervez Musharraf" ,
"Vladimir Putin" ,
"Steven Kull" ,
"Mahmoud Ahmadinejad" .
<http://yahoo.com/10622/global_economy_dc_4/>
kb:containsCity "Sao Paulo" , "Kuala Lumpur" ;
kb:containsRegion "Midwest" ;
kb:containsCountry "United States" , "Britain" ,
"Saudi Arabia" , "Spain" ,
"Italy" , India" ,
""France" , "Canada" ,
"Russia" , "Germany" , "China" ,
"Japan" , "South Korea" ;
kb:containsOrganization "Federal Reserve Bank" ,
"European Union" ,
"European Central Bank" ,
"European Commission" ;
kb:containsPerson "Lee Myung-bak" , "Rajat Nag" ,
"Luiz Inacio Lula da Silva" ,
"Jeffrey Lacker" ;
kb:containsCompany "Development Bank Managing" ,
"Reuters" ,
"Richmond Federal Reserve Bank" ;
kb:containsIndustryTerm "central bank" , "food" ,
"energy costs" ,
Semantic Web
"finance ministers" ,
"crude oil prices" ,
"oil prices" ,
"oil shock" ,
"food prices" ,
"Finance ministers" ,
"Oil prices" , "oil" .
In the following examples, we will use the main method in the class JenaApi (developed in the
next section) that allows us to load multiple RDF input files and then to interactively enter SPARQL
queries.
We will start with a simple SPARQL query for subjects (news article URLs) and objects (matching
countries) with the value for the predicate equal to containsCountry. Variables in queries start with
a question mark character and can have any names:
SELECT ?subject ?object
WHERE {
?subject
<http://knowledgebooks.com/ontology#containsCountry>
?object .
It is important for you to understand what is happening when we apply the last SPARQL query to
our sample data. Conceptually, all the triples in the sample data are scanned, keeping the ones where
the predicate part of a triple is equal to http://knowledgebooks.com/ontology#containsCountry.
In practice RDF data stores supporting SPARQL queries index RDF data so a complete scan of the
sample data is not required. This is analogous to relational databases where indices are created to
avoid needing to perform complete scans of database tables.
In practice, when you are exploring a Knowledge Graph like DBPedia or WikiData (that are just
very large collections of RDF triples), you might run a query and discover a useful or interesting
entity URI in the triple store, then drill down to find out more about the entity. In a later chapter
Knowledge Graph Navigator we attempt to automate this exploration process using the DBPedia
data as a Knowledge Graph.
We will be using the same code to access the small example of RDF statements in our sample data
as we will for accessing DBPedia or WikiData.
We can make this last query easier to read and reduce the chance of misspelling errors by using a
namespace prefix:
Semantic Web
PREFIX kb:
<http://knowledgebooks.com/ontology#>
SELECT ?subject ?object
WHERE {
?subject kb:containsCountry ?object .
Using the command line option in the Jena wrapper example
We will later implement the Java class JenaApis. You can run the method main in the Java class
JenaApis using the following to load RDF input files and interactively make SPARQL queries against
the RDF data in the input files:
$ mvn exec:java -Dexec.mainClass="com.markwatson.semanticweb.JenaApis" \
-Dexec.args="data/news.n3 data/sample_news.nt"
The command line argument in line 3 starting with -Dexec.args= is one way to pass command line
arguments to the method main. The backslash character at the end of line 2 is the way to continue
a long command line request in bash or zsh.
Here is an interactive example of the last SPARQL example:
$ mvn exec:java -Dexec.mainClass="com.markwatson.semanticweb.JenaApis" \
-Dexec.args="data/news.n3"
Multi-line queries are OK but don't use blank lines.
Enter a blank line to process query.
Enter a SPARQL query:
PREFIX kb:
<http://knowledgebooks.com/ontology#>
SELECT ?subject ?object
WHERE {
?subject kb:containsCountry ?object .
[QueryResult vars:[subject, object]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Russia]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, Japan]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, India]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/, United States]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_politics_dc_2/, Afghanistan]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Saudi Arabia]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, United States]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, France]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_politics_dc_2/, Iraq]
Semantic Web
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/, Pakistan]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Spain]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Italy]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
Islamic Republic of Iran]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Canada]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Britain]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_politics_dc_2/, United States]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, South Korea]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Germany]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, United States]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, China]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Japan]
Enter a SPARQL query:
We could have filtered on any other predicate, for instance containsPlace. Here is another example
using a match against a string literal to find all articles exactly matching the text “Maryland.”
PREFIX kb:
<http://knowledgebooks.com/ontology#>
SELECT ?subject WHERE { ?subject kb:containsState "Maryland" . }
The output is:
Enter a SPARQL query:
PREFIX kb:
<http://knowledgebooks.com/ontology#>
SELECT ?subject WHERE { ?subject kb:containsState "Maryland" . }
[QueryResult vars:[subject]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/]
We can also match partial string literals against regular expressions:
Semantic Web
PREFIX kb: <http://knowledgebooks.com/ontology#>
SELECT ?subject ?object
WHERE {
?subject
kb:containsOrganization
?object FILTER regex(?object, "University") .
The output is:
Enter a SPARQL query:
PREFIX kb: <http://knowledgebooks.com/ontology#>
SELECT ?subject ?object
WHERE {
?subject
kb:containsOrganization
?object FILTER regex(?object, "University") .
[QueryResult vars:[subject, object]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
University of Maryland]
Prior to this last example query we only requested that the query return values for subject and
predicate for triples that matched the query. However, we might want to return all triples whose
subject (in this case a news article URI) is in one of the matched triples. Note that there are two
matching triples, each terminated with a period:
PREFIX kb: <http://knowledgebooks.com/ontology#>
SELECT DISTINCT ?subject ?a_predicate ?an_object
WHERE {
?subject kb:containsOrganization ?object FILTER regex(?object,"University") .
?subject ?a_predicate ?an_object .
ORDER BY ?a_predicate ?an_object
LIMIT 10
OFFSET 5
When WHERE clauses contain more than one triple pattern to match, this is equivalent to a Boolean
“and” operation. The DISTINCT clause removes duplicate results. The ORDER BY clause sorts the
output in alphabetical order: in this case first by predicate (containsCity, containsCountry, etc.) and
Semantic Web
then by object. The LIMIT modifier limits the number of results returned and the OFFSET modifier
sets the number of matching results to skip.
The output is:
Enter a SPARQL query:
PREFIX kb: <http://knowledgebooks.com/ontology#>
SELECT DISTINCT ?subject ?a_predicate ?an_object
WHERE {
?subject kb:containsOrganization ?object FILTER regex(?object,"University") .
?subject ?a_predicate ?an_object .
ORDER BY ?a_predicate ?an_object
LIMIT 10
OFFSET 5
[QueryResult vars:[subject, a_predicate, an_object]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsOrganization,
University of Maryland]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Ban Ki-moon]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, George W. Bush]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Gordon Brown]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Hu Jintao]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Mahmoud Ahmadinejad]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Pervez Musharraf]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Steven Kull]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsPerson, Vladimir Putin]
[http://news.yahoo.com/s/nm/20080616/ts_nm/worldleaders_trust_dc_1/,
http://knowledgebooks.com/ontology#containsState, Maryland]
We are finished with our quick tutorial on using the SELECT query form. There are three other
query forms that I am not covering in this chapter:
Semantic Web
• CONSTRUCT⁶⁶ – returns a new RDF graph of query results
• ASK⁶⁷ – returns Boolean true or false indicating if a query matches any triples
• DESCRIBE⁶⁸ – returns a new RDF graph containing matched resources
A common matching pattern that I don’t cover in this chapter is optional⁶⁹ but the optional matching
pattern is used in the examples in the later chapter Knowledge Graph Navigator.
Using Jena
Apache Jena is a complete Java library for developing RDF/RDFS/OWL applications and we will use
it in this chapter. Other available libraries that we don’t use here include RDF4J (used to be Sesame),
OWLAPI, AllegroGraph, Protege library, etc.
The following figure shows a UML diagram for the wrapper classes and interface that I wrote for
Jena to make it easier for you to get started. My wrapper uses an in-memory RDF repository that
supports inference, loading RDF/RDFS/OWL files, and performing both local and remote SPARQL
queries. If you decide to use semantic web technologies in your development you will eventually
want to use the full Jena APIs for programmatically creating new RDF triples, finer control of the type
of repository (options are in-memory, disk based, and database), type definitions⁷⁰ and inferencing,
and programmatically using query results. That said, using my wrapper library is a good place for
you to start experimenting.
Referring to the following figure, the class constructor JenaApis opens a new in-memory RDF triple
store and supplies the public APIs we will use later. The data class QueryResults has public class
variables for variable names used in a query and a list or rows, one row for each query result. The
class Cache is used internally to cache SPARQL query results for later to improve performance and
use without having online access a remote SPARQL endpoint like DBPedia or WikiData.
⁶⁶https://www.w3.org/TR/rdf-sparql-query/#construct
⁶⁷https://www.w3.org/TR/rdf-sparql-query/#ask
⁶⁸https://www.w3.org/TR/rdf-sparql-query/#describe
⁶⁹https://www.w3.org/TR/rdf-sparql-query/#optionals
⁷⁰https://www.w3.org/TR/swbp-xsch-datatypes/
Semantic Web
UML Class Diagram for Apache Jena Wrapper Classes
We will look in some detail at the code in this UML Class Diagram. To improve portability to
alternative RDF libraries, I wrote two wrapper classes for Jena, one class to represent query results
Semantic Web
and the other to wrap the Jena APIs that I use.
The following screen shot shows the free IntelliJ Community Edition IDE used to edit one of the
unit tests and run it:
Example query in the unit test class
We will now look at the Java implementation of the examples for this chapter.
Java Wrapper for Jena APIs and an Example
For portability to other RDF and semantic web libraries, when we wrap the Jena APIs we want the
results to be in standard Java data classes. The following listing shows the class QueryResult that
contains the variables used in a SPARQL query and a list or rows containing matched value bindings
for these variables:
package com.markwatson.semanticweb;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.List;
public class QueryResult implements Serializable {
private QueryResult() { }
public QueryResult(List<String> variableList) {
this.variableList = variableList;
Semantic Web
public List<String> variableList;
public List<List<String>> rows = new ArrayList();
public String toString() {
StringBuilder sb = new StringBuilder("[QueryResult vars:" + variableList +
"\nRows:\n");
for (List<String> row : rows) {
sb.append("\t" + row + "\n");
return sb.toString();
I defined a toString method so when you print an instance of the class QueryResult you see the
contained data.
The following listing shows the wrapper class JenaApis:
package com.markwatson.semanticweb;
import org.apache.commons.lang3.SerializationUtils;
import org.apache.jena.query.*;
import org.apache.jena.rdf.model.*;
import org.apache.jena.riot.RDFDataMgr;
import org.apache.jena.riot.RDFFormat;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.sql.Blob;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.List;
import java.util.Scanner;
public class JenaApis {
public JenaApis() {
//model = ModelFactory.createDefaultModel(); // if OWL reasoning not required
model = ModelFactory.createOntologyModel(); // use OWL reasoner
public Model model() {
Semantic Web
return model;
public void loadRdfFile(String fpath) {
model.read(fpath);
public void saveModelToTurtleFormat(String outputPath) throws IOException {
FileOutputStream fos = new FileOutputStream(outputPath);
RDFDataMgr.write(fos, model, RDFFormat.TRIG_PRETTY);
fos.close();
public void saveModelToN3Format(String outputPath) throws IOException {
FileOutputStream fos = new FileOutputStream(outputPath);
RDFDataMgr.write(fos, model, RDFFormat.NTRIPLES);
fos.close();
public QueryResult query(String sparqlQuery) {
Query query = QueryFactory.create(sparqlQuery);
QueryExecution qexec = QueryExecutionFactory.create(query, model);
ResultSet results = qexec.execSelect();
QueryResult qr = new QueryResult(results.getResultVars());
for (; results.hasNext(); ) {
QuerySolution solution = results.nextSolution();
List<String> newResultRow = new ArrayList();
for (String var : qr.variableList) {
newResultRow.add(solution.get(var).toString());
qr.rows.add(newResultRow);
return qr;
public QueryResult queryRemote(String service, String sparqlQuery)
throws SQLException, ClassNotFoundException {
if (cache == null) cache = new Cache();
byte [] b = cache.fetchResultFromCache(sparqlQuery);
if (b != null) {
System.out.println("Found query in cache.");
QueryResult l = SerializationUtils.deserialize(b);
return l;
Semantic Web
Query query = QueryFactory.create(sparqlQuery);
QueryExecution qexec = QueryExecutionFactory.sparqlService(service, sparqlQuery);
ResultSet results = qexec.execSelect();
QueryResult qr = new QueryResult(results.getResultVars());
for (; results.hasNext(); ) {
QuerySolution solution = results.nextSolution();
List<String> newResultRow = new ArrayList();
for (String var : qr.variableList) {
newResultRow.add(solution.get(var).toString());
qr.rows.add(newResultRow);
byte [] b3 = SerializationUtils.serialize(qr);
cache.saveQueryResultInCache(sparqlQuery, b3);
return qr;
private Cache cache = null;
private Model model;
public static void main(String[] args) {
Execute using, for example:
mvn exec:java -Dexec.mainClass="com.markwatson.semanticweb.JenaApis" \
-Dexec.args="data/news.n3"
JenaApis ja = new JenaApis();
if (args.length == 0) {
// no RDF input file names on command line so use a default file:
ja.loadRdfFile("data/news.n3");
} else {
for (String fpath : args) {
ja.loadRdfFile(fpath);
System.out.println("Multi-line queries are OK but don't use blank lines.");
System.out.println("Enter a blank line to process query.");
while (true) {
System.out.println("Enter a SPARQL query:");
Scanner sc = new Scanner(System.in);
StringBuilder sb = new StringBuilder();
while (sc.hasNextLine()) {
// process all inputs
String s = sc.nextLine();
Semantic Web
if (s.equals("quit") || s.equals("QUIT") ||
s.equals("exit") || s.equals("EXIT"))
System.exit(0);
if (s.length() < 1) break;
sb.append(s);
sb.append("\n");
QueryResult qr = ja.query(sb.toString());
System.out.println(qr);
This code is largely self-explanatory. Line 21 or 22 should be commented out, depending on whether
you want to enable OWL reasoning. In method queryRemote on line 62 we check to see if an
instance of Cache has been created and if not, create one. The argument service for the method
queryRemote is a SPARQL endpoint (e.g., “https://dbpedia.org/sparql”). The class QueryResult
implemented Serializable so it can be converted and stored in the Derby cache database.
The method main implements a command line interface for accepting multiple lines of input. When
the user enters a blank line then the previously entered non-blank lines are passed as a SPARQL
local query. When run from the command line, you can enter one or more RDF input files to load
prior to the SPARQL query loop.
The following class shows the unit test class JenaApisTest that provides examples for:
• Create an instance of JenaApis.
• Run a SPARQL query against the remote public DBPedia service endpoint.
• Repeat the remote SPARQL query to show query caching using the Apache Derby relational
database.
• Load three input data files in N-Triple and N3 format.
• Run a SPARQL query against the RDF data that we just loaded.
• Save the current model as RDF text files in both N-Triple and N3 format.
• Making SPARQL queries that require OWL reasoning.
Semantic Web
package com.markwatson.semanticweb;
import junit.framework.Test;
import junit.framework.TestCase;
import junit.framework.TestSuite;
public class JenaApisTest extends TestCase {
* Create the test case
* @param testName name of the test case
public JenaApisTest(String testName)
super( testName );
* @return the suite of tests being tested
public static Test suite()
return new TestSuite( JenaApisTest.class );
* Test that is just for side effect printouts:
public void test1() throws Exception {
JenaApis jenaApis = new JenaApis();
// test remote SPARQL queries against DBPedia SPARQL endpoint
QueryResult qrRemote = jenaApis.queryRemote(
"https://dbpedia.org/sparql",
"select distinct ?s { ?s ?p <http://dbpedia.org/resource/Parks> } LIMIT 10");
System.out.println("qrRemote:" + qrRemote);
System.out.println("Repeat query to test caching:");
qrRemote = jenaApis.queryRemote(
"https://dbpedia.org/sparql",
"select distinct ?s { ?s ?p <http://dbpedia.org/resource/Parks> } LIMIT 10");
System.out.println("qrRemote (hopefully from cache):" + qrRemote);
jenaApis.loadRdfFile("data/rdfs_business.nt");
jenaApis.loadRdfFile("data/sample_news.nt");
Semantic Web
jenaApis.loadRdfFile("data/sample_news.n3");
QueryResult qr = jenaApis.query(
"select ?s ?o where { ?s <http://knowledgebooks.com/title> ?o } limit 15");
System.out.println("qr:" + qr);
jenaApis.saveModelToTurtleFormat("model_save.nt");
jenaApis.saveModelToN3Format("model_save.n3");
* Test that is just for side effect printouts:
public void testOwlReasoning() throws Exception {
JenaApis jenaApis = new JenaApis();
jenaApis.loadRdfFile("data/news.n3");
QueryResult qr = jenaApis.query(
"prefix kb:
<http://knowledgebooks.com/ontology#> \n" +
"select ?s ?o where { ?s kb:containsCity ?o } ");
System.out.println("qr:" + qr);
qr = jenaApis.query(
"prefix kb:
<http://knowledgebooks.com/ontology#> \n" +
"select ?s ?o where { ?s kb:containsPlace ?o }");
System.out.println("qr:" + qr);
qr = jenaApis.query(
// count for each place name
"prefix kb:
<http://knowledgebooks.com/ontology#> \n" +
"select ?o (count(*) as ?count) where { ?s kb:containsPlace ?o } " +
"group by ?o");
System.out.println("qr:" + qr);
To reuse the example code in this section, I recommend that you clone the entire directory semantic_-
web_apache_jena because it is set up for using Maven and and default logging. If you want to use
the code in an existing Java project then copy the dependencies from the file pom.xml to your
project. If you run mvn install then you will have a local copy installed on your system and can
just install the dependency with Maven group ID com.markwatson and artifact semanticweb.
Semantic Web
OWL: The Web Ontology Language
We have already seen a few examples of using RDFS to define sub-properties in this chapter. The
Web Ontology Language (OWL) extends the expressive power of RDFS. We now look at a few OWL
examples and then look at parts of the Java unit test showing three SPARQL queries that use OWL
reasoning. The following RDF data stores support at least some level of OWL reasoning:
• ProtegeOwlApis - compatible with the Protege Ontology editor
• Pellet - DL reasoner
• Owlim - OWL DL reasoner compatible with some versions of Sesame
• Jena - General purpose library
• OWLAPI - a simpler API using many other libraries
• Stardog - a commercial OWL and RDF reasoning system and datastore
• Allegrograph - a commercial RDF+ and RDF reasoning system and datastore
OWL is more expressive than RDFS in that it supports cardinality, richer class relationships, and
Descriptive Logic (DL) reasoning. OWL treats the idea of classes very differently than object oriented
programming languages like Java and Smalltalk, but similar to the way PowerLoom (see chapter on
Reasoning) uses concepts (PowerLoom’s rough equivalent to a class). In OWL, instances of a class
are referred to as individuals and class membership is determined by a set of properties that allow
a DL reasoner to infer class membership of an individual (this is called entailment.)
We saw an example of expressing transitive relationships when we were using PowerLoom in the
chapter on Reasoning where we defined a PowerLoom rule to express that the relation “contains” is
transitive. We will now look at a similar example using OWL.
We have been using the RDF file news.n3 in previous examples and we will layer new examples by
adding new triples that represent RDF, RDFS, and OWL. We saw in news.n3 the definition of three
triples using rdfs:subPropertyOf properties to create a more general kb:containsPlace property:
kb:containsCity rdfs:subPropertyOf kb:containsPlace .
kb:containsCountry rdfs:subPropertyOf kb:containsPlace .
kb:containsState rdfs:subPropertyOf kb:containsPlace .
kb:containsPlace rdf:type owl:transitiveProperty .
kbplace:UnitedStates kb:containsState kbplace:Illinois .
kbplace:Illinois kb:containsCity kbplace:Chicago .
We can also infer that:
Semantic Web
kbplace:UnitedStates kb:containsPlace kbplace:Chicago .
We can also model inverse properties in OWL. For example, here we add an inverse property
kb:containedIn, adding it to the example in the last listing:
kb:containedIn owl:inverseOf kb:containsPlace .
Given an RDF container that supported extended OWL DL SPARQL queries, we can now execute
SPARQL queries matching the property kb:containedIn and “match” triples in the RDF triple store
that have never been asserted but are inferred by the OWL reasoner.
OWL DL is a very large subset of full OWL. From reading the chapter on Reasoning and the very
light coverage of OWL in this section, you should understand the concept of class membership not
by explicitly stating that an object (or individual) is a member of a class, but rather because an
individual has properties that can be used to infer class membership.
The World Wide Web Consortium has defined three versions of the OWL language that are in
increasing order of complexity: OWL Lite, OWL DL, and OWL Full. OWL DL (supports Description
Logic) is the most widely used (and recommended) version of OWL. OWL Full is not computationally
decidable since it supports full logic, multiple class inheritance, and other things that probably make
it computationally intractable for all but smaller problems.
We will now look at some Java code from the method testOwlReasoning in the unit test class
JenaApisTest.
The following is not affected by using an OWL reasoner because the property kb:containsCity
occurs directly in the input RDF data:
JenaApis jenaApis = new JenaApis();
jenaApis.loadRdfFile("data/news.n3");
QueryResult qr = jenaApis.query(
"prefix kb:
<http://knowledgebooks.com/ontology#> \n" +
"select ?s ?o where { ?s kb:containsCity ?o } ");
System.out.println("qr:" + qr);
The following has been edited to keep just a few output lines per result set:
Semantic Web
qr:[QueryResult vars:[s, o]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, St. Paul]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_politics_dc_2/, FLINT]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, CHICAGO]
... output removed. note: there were 15 results for query
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, Quincy]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, Iowa City]
Here we use a query that is affected by using an OWL reasoner (i.e., if OWL is not enabled there
will be no query results):
qr = jenaApis.query(
"prefix kb:
<http://knowledgebooks.com/ontology#> \n" +
"select ?s ?o where { ?s kb:containsPlace ?o }");
System.out.println("qr:" + qr);
The code in the GitHub repo for this book is configured to use OWL by default. If you edited lines
21-22 in the file JenaApis.jav to disable OWL reasoning then revert your changes and rebuild the
project.
The following has been edited to just keep a few output lines per result set:
qr:[QueryResult vars:[s, o]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, St. Paul]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_politics_dc_2/, FLINT]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, CHICAGO]
[http://news.yahoo.com/s/nm/20080616/bs_nm/global_economy_dc_4/, Kuala Lumpur]
... output removed. note: there were 46 results for query
global_economy_dc_4/, United States]
[http://news.yahoo.com/s/nm/20080616/bs_nmglobal_economy_dc_4/, Germany]
[http://news.yahoo.com/s/nm/20080616/ts_nm/usa_flooding_dc_16/, United States]
We now group (aggregate) query results and count the number of times each place name has occurred
in the result (this query requires an OWL reasoner):
Semantic Web
qr = jenaApis.query(
// count for each place name
"prefix kb:
<http://knowledgebooks.com/ontology#> \n" +
"select ?o (count(*) as ?count) where { ?s kb:containsPlace ?o } " +
"group by ?o");
System.out.println("qr:" + qr);
qr:[QueryResult vars:[o, count]
[Chicago, 1^^http://www.w3.org/2001/XMLSchema#integer]
[Illinois, 2^^http://www.w3.org/2001/XMLSchema#integer]
[Arizona, 1^^http://www.w3.org/2001/XMLSchema#integer]
... output removed. note: there were 40 results for query
[United States, 4^^http://www.w3.org/2001/XMLSchema#integer]
[Iowa, 1^^http://www.w3.org/2001/XMLSchema#integer]
[Japan, 2^^http://www.w3.org/2001/XMLSchema#integer]
[Spain, 1^^http://www.w3.org/2001/XMLSchema#integer]
Note the type http://www.w3.org/2001/XMLSchema#integer using the notation for integer values
bound to the variable count.
Semantic Web Wrap-up
Writing semantic web applications in Java is a very large topic, worthy of an entire book. I have
covered in this chapter what for my work has been the most useful semantic web techniques: storing
and querying RDF and RDFS for a specific application and using OWL when required. We will see
in the next two chapters the use of RDF when automatically creating Knowledge Graphs from text
data and for automatic navigation of Knowledge Graphs.
Automatically Generating Data for
Knowledge Graphs
Here we develop a complete application using the package developed in the earlier chapter Resolve
Entity Names to DBPedia References. The Knowledge Graph Creator (KGcreator) is a tool for
automating the generation of data for Knowledge Graphs from raw text data. Here we generate
RDF data for a Knowledge Graph. You might also be interested in the Knowledge Graph Creator
implementation in my Common Lisp book⁷¹ that generates data for the Neo4J open source graph
database in addition to generating RDF data.
Data created by KGcreator generates data in RDF triples suitable for loading into any linked
data/semantic web data store.
This example application works by identifying entities in text. Example entity types are people,
companies, country names, city names, broadcast network names, political party names, and
university names. We saw earlier code for detecting entities in the chapter on making named entities
to DBPedia URIs and we will reuse this code.
I originally wrote KGCreator as two research prototypes, one in Common Lisp (see my Common
Lisp book⁷²) and one in Haskell⁷³. The example in this chapter is a port of these systems to Java.
Implementation Notes
The implementation is contained in a single Java class KGC and the junit test class KgcTest is used
to process the test files included with this example.
As can be seen in the following figure I have defined final static strings for each type of entity type
URI. For example, personTypeUri has the value http://www.w3.org/2000/01/rdf-schema#person.
⁷¹https://leanpub.com/lovinglisp
⁷²https://leanpub.com/lovinglisp
⁷³https://leanpub.com/haskell-cookbook/
Automatically Generating Data for Knowledge Graphs
Overview of Java Class UML Diagram for the Knowledge Graph Creator
Automatically Generating Data for Knowledge Graphs
The following figure shows a screen shot of this example project in the free Community Edition of
IntelliJ.
IDE View of Project
Notice in this screen shot that there are several test files in the directory test_data. The files with the
file extension .meta contain a single line which is the URI for the source of the text in the matching
text file. For example, the meta file test1.meta provides the URI for the source of the text in the file
test1.txt.
Generating RDF Data
RDF data is comprised of triples, where the value for each triple are a subject, a predicate, and an
object. Subjects are URIs, predicates are usually URIs, and objects are either literal values or URIs.
Here are two triples written by this example application:
Automatically Generating Data for Knowledge Graphs
<http://dbpedia.org/resource/The_Wall_Street_Journal>
<http://knowledgebooks.com/schema/aboutCompanyName>
"Wall Street Journal" .
<https://newsshop.com/june/z902.html>
<http://knowledgebooks.com/schema/containsCountryDbPediaLink>
<http://dbpedia.org/resource/Canada> .
The following listing of the file KGC.java contains the implementation the main Java class for
generating RDF data. Code for different entity types is similar so the following listing only shows
the code for handling entity types for people and companies. The following is reformatted to fit the
page width:
package com.knowledgegraphcreator;
import com.markwatson.ner_dbpedia.TextToDbpediaUris;
import java.io.*;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Paths;
* Java implementation of Knowledge Graph Creator.
* Copyright 2020 Mark Watson. All Rights Reserved. Apache 2 license.
* For documentation see my book "Practical Artificial Intelligence Programming
* With Java", chapter "Automatically Generating Data for Knowledge Graphs"
* at https://leanpub.com/javaai that can be read free online.
public class KGC
static String subjectUri =
"<http://www.w3.org/1999/02/22-rdf-syntax-ns#/subject>";
static String labelUri =
"<http://www.w3.org/1999/02/22-rdf-syntax-ns#/label>";
static String personTypeUri =
"<http://www.w3.org/2000/01/rdf-schema#person>";
static String companyTypeUri =
"<http://www.w3.org/2000/01/rdf-schema#company>";
Automatically Generating Data for Knowledge Graphs
static String typeOfUri =
"<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>";
private KGC() { }
public KGC(String directoryPath, String outputRdfPath) throws IOException {
System.out.println("KGN");
PrintStream out = new PrintStream(outputRdfPath);
File dir = new File(directoryPath);
File[] directoryListing = dir.listFiles();
if (directoryListing != null) {
for (File child : directoryListing) {
System.out.println("child:" + child);
if (child.toString().endsWith(".txt")) {
// try to open the meta file with the same extension:
String metaAbsolutePath = child.getAbsolutePath();
File meta =
new File(metaAbsolutePath.substring(0,
metaAbsolutePath.length()
+ ".meta");
System.out.println("meta:" + meta);
String [] text_and_meta =
readData(child.getAbsolutePath(), meta.getAbsolutePath());
String metaData = "<" + text_and_meta[1].strip() + ">";
TextToDbpediaUris kt =
new TextToDbpediaUris(text_and_meta[0]);
for (int i=0; i<kt.personNames.size(); i++) {
out.println(metaData + " " + subjectUri + " " +
kt.personUris.get(i) + " .");
out.println(kt.personUris.get(i) + " " + labelUri +
" \"" + kt.personNames.get(i) + "\" .");
out.println(kt.personUris.get(i) + " " + typeOfUri +
" " + personTypeUri + " .");
for (int i=0; i<kt.companyNames.size(); i++) {
out.println(metaData + " " +
subjectUri + " " +
kt.companyUris.get(i) + " .");
out.println(kt.companyUris.get(i) + " " +
labelUri + " \"" +
kt.companyNames.get(i) + "\" .");
out.println(kt.companyUris.get(i) + " " + typeOfUri
" " + companyTypeUri + " .");
Automatically Generating Data for Knowledge Graphs
out.close();
private String [] readData(String textPath, String metaPath) throws IOException {
String text = Files.readString(Paths.get(textPath), StandardCharsets.UTF_8);
String meta = Files.readString(Paths.get(metaPath), StandardCharsets.UTF_8);
System.out.println("\n\n** text:\n\n" + text);
return new String[] { text, meta };
This code works on a list of paired files for text data and the meta data for each text file. As an
example, if there is an input text file test123.txt then there would be a matching meta file test123.meta
that contains the source of the data in the file test123.txt. This data source will be a URI on the web
or a local file URI. The class contractor for KGC takes an output file path for writing the generated
RDF data and a list of pairs of text and meta file paths.
The junit test class KgcTest will process the local directory test_data and generate an RDF output
package com.knowledgegraphcreator;
import junit.framework.Test;
import junit.framework.TestCase;
import junit.framework.TestSuite;
public class KgcTest extends TestCase {
public KgcTest(String testName) {
super(testName);
public static Test suite() {
return new TestSuite(KgcTest.class);
public void testKGC() throws Exception {
assertTrue(true);
KGC client = new KGC("test_data/", "output_with_duplicates.rdf");
Automatically Generating Data for Knowledge Graphs
private static void pause() {
try { Thread.sleep(2000);
} catch (Exception ignore) { }
If specific entity names occur in multiple input files there will be a few duplicated RDF statements
generated. The simplest way to deal with this is to add a one line call to the awk utility to efficiently
remove duplicate lines in the RDF output file. Here is a listing of the Makefile for this example:
create_data_and_remove_duplicates:
mvn test
echo "Removing duplicate RDF statements"
awk '!visited[$$0]++' output_with_duplicates.rdf > output.rdf
rm -f output_with_duplicates.rdf
If you are not familiar with awk and want to learn the basics then I recommend this short tutorial⁷⁴.
KGCreator Wrap Up
When developing applications or systems using Knowledge Graphs it is useful to be able to quickly
generate test data which is the primary purpose of KGCreator. A secondary use is to generate
Knowledge Graphs for production use using text data sources. In this second use case you will want
to manually inspect the generated data to verify its correctness or usefulness for your application.
⁷⁴http://www.hcs.harvard.edu/~dholland/computers/awk.html
Knowledge Graph Navigator
The Knowledge Graph Navigator (which I will often refer to as KGN) is a tool for processing a
set of entity names and automatically exploring the public Knowledge Graph DBPedia⁷⁵ using
SPARQL queries. I started to write KGN for my own use, to automate some things I used to do
manually when exploring Knowledge Graphs, and later thought that KGN might be also useful for
educational purposes. KGN shows the user the auto-generated SPARQL queries so hopefully the
user will learn by seeing examples. KGN uses code developed in the earlier chapter Resolve Entity
Names to DBPedia References and we will reuse here as well as the two Java classes JenaAPis and
QueryResults (which wrap the Apache Jena library) from the chapter Semantic Web.
I have a web site devoted to different versions of KGN⁷⁶ that you might find interesting. The most full
featured version of KGN, including a full user interface, is featured in my book Loving Common Lisp,
or the Savvy Programmer’s Secret Weapon⁷⁷ that you can read free online. That version performs
more speculative SPARQL queries to find information compared to the example here that I designed
for ease of understanding, modification, and embedding in larger Java projects.
I chose to use DBPedia instead of WikiData for this example because DBPedia URIs are human
readable. The following URIs represent the concept of a person. The semantic meanings of DBPedia
and FOAF (friend of a friend) URIs are self-evident to a human reader while the WikiData URI is
http://www.wikidata.org/entity/Q215627
http://dbpedia.org/ontology/Person
http://xmlns.com/foaf/0.1/name
I frequently use WikiData in my work and WikiData is one of the most useful public knowledge
bases. I have both DBPedia and WikiData Sparql endpoints in the file Sparql.java that we will look
at later, with the WikiData endpoint comment out. You can try manually querying WikiData at
the WikiData SPARL endpoint⁷⁸. For example, you might explore the WikiData URI for the person
concept using:
select ?p ?o where { <http://www.wikidata.org/entity/Q215627> ?p ?o
} limit 10
For the rest of this chapter we will just use DBPedia.
After looking an interactive session using the example program for this chapter (that also includes
listing automatically generated SPARQL queries) we will look at the implementation.
⁷⁵http://dbpedia.org
⁷⁶http://www.knowledgegraphnavigator.com/
⁷⁷https://leanpub.com/lovinglisp
⁷⁸https://query.wikidata.org
Knowledge Graph Navigator
Entity Types Handled by KGN
To keep this example simple we handle just four entity types:
• People
• Companies
• Cities
• Countries
The entity detection library that we use from an earlier chapter also supports the following entity
types that we don’t use here:
• Broadcast Networks
• Music Groups
• Political Parties
• Trade Unions
• Universities
In addition to finding detailed information for people, companies, cities, and countries we will also
search for relationships between person entities and company entities. This search process consists
of generating a series of SPARQL queries and calling the DBPedia SPARQL endpoint.
As we look at the KGN implementation I will point out where and how you can easily add support
for more entity types and in the wrap-up I will suggest further projects that you might want to try
implementing with this example.
General Design of KGN with Example Output
The example application works by first having the user enter names of people and companies. Using
libraries written in two previous chapters, we find entities in the user’s input text, and generate
SPARQL queries to DBPedia to find information about the entities and relationships between them.
We will start with looking at sample output so you have some understanding on what this
implementation of KGN will and will not do. Here is the console output for the example query “Bill
Gates, Melinda Gates and Steve Jobs at Apple Computer, IBM and Microsoft” (with some output
removed for brevity). As you remember from the chapter Semantic Web, SPAQRL query results are
expressed in class QueryResult that contains the variables (labelled as vars) in a query and a list
of rows (one query result per row). Starting at line 117 in the following listing we see discovered
relationships between entities in the input query.
Knowledge Graph Navigator
Enter entities query:
Bill Gates, Melinda Gates and Steve Jobs at Apple Computer, IBM and Microsoft
Processing query:
Bill Gates, Melinda Gates and Steve Jobs at Apple Computer, IBM and Microsoft
person
Bill Gates
<http://dbpedia.org/resource/Bill_Gates>
person
Melinda Gates
<http://dbpedia.org/resource/Melinda_Gates>
person
Steve Jobs
<http://dbpedia.org/resource/Steve_Jobs>
company
Apple Computer
<http://dbpedia.org/resource/Apple_Inc.>
company
<http://dbpedia.org/resource/IBM>
company
Microsoft
<http://dbpedia.org/resource/Microsoft>
Individual People:
Bill Gates
: http://dbpedia.org/resource/Bill_Gates
[QueryResult vars:[birthplace, label, comment, almamater, spouse]
[http://dbpedia.org/resource/Seattle, Bill Gates, William Henry \"Bill\" Gates III\
(born October 28, 1955) is an American business magnate, investor, author and phila\
nthropist. In 1975, Gates and Paul Allen co-founded Microsoft, which became the worl\
d's largest PC software company. During his career at Microsoft, Gates held the posi\
tions of chairman, CEO and chief software architect, and was the largest individual \
shareholder until May 2014. Gates has authored and co-authored several books., http:\
//dbpedia.org/resource/Harvard_University, http://dbpedia.org/resource/Melinda_Gates\
Melinda Gates
: http://dbpedia.org/resource/Melinda_Gates
[QueryResult vars:[birthplace, label, comment, almamater, spouse]
[http://dbpedia.org/resource/Dallas | http://dbpedia.org/resource/Dallas,_Texas, M\
elinda Gates, Melinda Ann Gates (née French; born August 15, 1964), DBE is an Americ\
an businesswoman and philanthropist. She is co-founder of the Bill & Melinda Gates F\
oundation. She worked at Microsoft, where she was project manager for Microsoft Bob,\
Microsoft Encarta and Expedia., http://dbpedia.org/resource/Duke_University, http:/\
/dbpedia.org/resource/Bill_Gates]
Steve Jobs
: http://dbpedia.org/resource/Steve_Jobs
[QueryResult vars:[birthplace, label, comment, almamater, spouse]
[http://dbpedia.org/resource/San_Francisco, Steve Jobs, Steven Paul \"Steve\" Jobs\
(/￿d￿￿bz/; February 24, 1955 – October 5, 2011) was an American information technol\
ogy entrepreneur and inventor. He was the co-founder, chairman, and chief executive \
Knowledge Graph Navigator
officer (CEO) of Apple Inc.; CEO and majority shareholder of Pixar Animation Studios\
; a member of The Walt Disney Company's board of directors following its acquisition\
of Pixar; and founder, chairman, and CEO of NeXT Inc. Jobs is widely recognized as \
a pioneer of the microcomputer revolution of the 1970s and 1980s, along with Apple c\
o-founder Steve Wozniak. Shortly after his death, Jobs's official biographer, Walter\
Isaacson, described him as a \"creative entrepreneur whose passion for perfection a\
nd ferocious drive revolutionized six industries: personal computers, animated movie\
s, music, phones, tab, http://dbpedia.org/resource/Reed_College, http://dbpedia.org/\
resource/Laurene_Powell_Jobs]
Individual Companies:
Apple Computer
: http://dbpedia.org/resource/Apple_Inc.
[QueryResult vars:[industry, netIncome, label, comment, numberOfEmployees]
[http://dbpedia.org/resource/Computer_hardware | http://dbpedia.org/resource/Compu\
ter_software | http://dbpedia.org/resource/Consumer_electronics | http://dbpedia.org\
/resource/Corporate_Venture_Capital | http://dbpedia.org/resource/Digital_distributi\
on | http://dbpedia.org/resource/Fabless_manufacturing, 5.3394E10, Apple Inc., Apple\
Inc. is an American multinational technology company headquartered in Cupertino, Ca\
lifornia, that designs, develops, and sells consumer electronics, computer software,\
and online services. Its hardware products include the iPhone smartphone, the iPad \
tablet computer, the Mac personal computer, the iPod portable media player, the Appl\
e Watch smartwatch, and the Apple TV digital media player. Apple's consumer software\
includes the macOS and iOS operating systems, the iTunes media player, the Safari w\
eb browser, and the iLife and iWork creativity and productivity suites. Its online s\
ervices include the iTunes Store, the iOS App Store and Mac App Store, Apple Music, \
and iCloud., 115000]
: http://dbpedia.org/resource/IBM
[QueryResult vars:[industry, netIncome, label, comment, numberOfEmployees]
[http://dbpedia.org/resource/Cloud_computing | http://dbpedia.org/resource/Cogniti\
ve_computing | http://dbpedia.org/resource/Information_technology, 1.319E10, IBM, In\
ternational Business Machines Corporation (commonly referred to as IBM) is an Americ\
an multinational technology company headquartered in Armonk, New York, United States\
, with operations in over 170 countries. The company originated in 1911 as the Compu\
ting-Tabulating-Recording Company (CTR) and was renamed \"International Business Mac\
hines\" in 1924., 377757]
Microsoft
: http://dbpedia.org/resource/Microsoft
[QueryResult vars:[industry, netIncome, label, comment, numberOfEmployees]
Knowledge Graph Navigator
[http://dbpedia.org/resource/Computer_hardware | http://dbpedia.org/resource/Consu\
mer_electronics | http://dbpedia.org/resource/Digital_distribution | http://dbpedia.\
org/resource/Software, , Microsoft, Microsoft Corporation /￿ma￿kr￿￿s￿ft, -ro￿-, -￿s￿\
￿ft/ (commonly referred to as Microsoft or MS) is an American multinational technolo\
gy company headquartered in Redmond, Washington, that develops, manufactures, licens\
es, supports and sells computer software, consumer electronics and personal computer\
s and services. Its best known software products are the Microsoft Windows line of o\
perating systems, Microsoft Office office suite, and Internet Explorer and Edge web \
browsers. Its flagship hardware products are the Xbox video game consoles and the Mi\
crosoft Surface tablet lineup. As of 2011, it was the world's largest software maker\
by revenue, and one of the world's most valuable companies., 114000]
Individual Cities:
Seattle
: http://dbpedia.org/resource/Seattle
[QueryResult vars:[latitude_longitude, populationDensity, label, comment, country]
[POINT(-122.33305358887 47.609722137451), 3150.979715864901, Seattle, Seattle is a\
West Coast seaport city and the seat of King County, Washington. With an estimated \
684,451 residents as of 2015, Seattle is the largest city in both the state of Washi\
ngton and the Pacific Northwest region of North America. As of 2015, it is estimated\
to be the 18th largest city in the United States. In July 2013, it was the fastest-\
growing major city in the United States and remained in the Top 5 in May 2015 with a\
n annual growth rate of 2.1%. The Seattle metropolitan area is the 15th largest metr\
opolitan area in the United States with over 3.7 million inhabitants. The city is si\
tuated on an isthmus between Puget Sound (an inlet of the Pacific Ocean) and Lake Wa\
shington, about 100 miles (160 km) south of the Canada–United States border. A major\
gateway for trade w, ]
Individual Countries:
Relationships between person Bill Gates person Melinda Gates:
[QueryResult vars:[p]
[http://dbpedia.org/ontology/spouse]
Relationships between person Melinda Gates person Bill Gates:
[QueryResult vars:[p]
[http://dbpedia.org/ontology/spouse]
Knowledge Graph Navigator
Relationships between person Bill Gates company Microsoft:
[QueryResult vars:[p]
[http://dbpedia.org/ontology/board]
Relationships between person Steve Jobs company Apple Computer:
[QueryResult vars:[p]
[http://www.w3.org/2000/01/rdf-schema#seeAlso]
[http://dbpedia.org/ontology/board]
[http://dbpedia.org/ontology/occupation]
Since the DBPedia queries are time consuming, we use the caching layer from the earlier chapter
Semantic Web when making SPARQL queries to DBPedia. The cache is especially helpful during
development when the same queries are repeatedly used for testing.
The KGN user interface loop allows you to enter queries and see the results. There are two special
options that you can enter instead of a query:
• sparql - this will print out all previous SPARQL queries used to present results. After entering
this command the buffer of previous SPARQL queries is emptied. This option is useful for
learning SPARQL and you might try pasting a few into the input field for the public DBPedia
SPARQL web app⁷⁹ and modifying them. We will use this command later in an example.
• demo - this will randomly choose a sample query.
UML Class Diagram for Example Application
The following UML Class Diagram for KGN shows you an overview of the Java classes we use and
their public methods and fields.
⁷⁹http://dbpedia.org/sparql
Knowledge Graph Navigator
UML Class Diagram for KGN Example Application
Implementation
We will walk through the classes in the UML Class Diagram for KGN in alphabetical order, the
exception being that we will look at the main program in KGN.java last.
The class EntityAndDescription contains two strings, a name and a URI reference. We also override
the default implementation of toString to format and display the data in an instance of this class:
Knowledge Graph Navigator
package com.knowledgegraphnavigator;
public class EntityAndDescription {
public String entityName;
public String entityUri;
public EntityAndDescription(String entityName, String entityUri) {
this.entityName = entityName;
this.entityUri = entityUri;
public String toString() {
return "[EntityAndDescription name: " + entityName +
" description: " + entityUri + "]";
The class EntityDetail defines SPARQL query templates in lines 80-154 that have slots (using %s for
string replacement) for the URI of an entity. We use different templates for different entity types.
Before we look at these SPARQL query templates, let’s learn two additional features of the SPARQL
language that we will need to use in these entity templates.
We mentioned the OPTIONAL triple matching patterns in the chapter Semantic Web. Before looking
at the Java code, let’s first look at how optional matching works. We will run the KGN application
asking for information on the city Seattle and then use the sparql command to print the generated
SPARQL produced by the method cityResults (most output is not shown here for brevity). On line
2 I enter the query string “Seattle” and on line 22 I enter the command “sparql” to print out the
generated SPARQL:
Enter entities query:
Seattle
Individual Cities:
Seattle
: http://dbpedia.org/resource/Seattle
[QueryResult vars:[latitude_longitude, populationDensity, label, comment, country]
[POINT(-122.33305358887 47.609722137451), 3150.979715864901, Seattle, Seattle is a\
West Coast seaport city and the seat of King County, Washington. With an estimated \
684,451 residents as of 2015, Seattle is the largest city in both the state of Washi\
ngton and the Pacific Northwest region of North America. As of 2015, it is estimated\
to be the 18th largest city in the United States. In July 2013, it was the fastest-\
growing major city in the United States and remained in the Top 5 in May 2015 with a\
n annual growth rate of 2.1%. The Seattle metropolitan area is the 15th largest metr\
opolitan area in the United States with over 3.7 million inhabitants. The city is si\
Knowledge Graph Navigator
tuated on an isthmus between Puget Sound (an inlet of the Pacific Ocean) and Lake Wa\
shington, about 100 miles (160 km) south of the Canada–United States border. A major\
gateway for trade w, ]
Processing query:
sparql
Generated SPARQL used to get current results:
SELECT DISTINCT
(GROUP_CONCAT (DISTINCT ?latitude_longitude2; SEPARATOR=' | ')
AS ?latitude_longitude)
(GROUP_CONCAT (DISTINCT ?populationDensity2; SEPARATOR=' | ')
AS ?populationDensity)
(GROUP_CONCAT (DISTINCT ?label2; SEPARATOR=' | ') AS ?label)
(GROUP_CONCAT (DISTINCT ?comment2; SEPARATOR=' | ') AS ?comment)
(GROUP_CONCAT (DISTINCT ?country2; SEPARATOR=' | ') AS ?country) {
<http://dbpedia.org/resource/Seattle>
<http://www.w3.org/2000/01/rdf-schema#comment>
?comment2 .
FILTER
(lang(?comment2) = 'en') .
OPTIONAL { <http://dbpedia.org/resource/Seattle>
<http://www.w3.org/2003/01/geo/wgs84_pos#geometry>
?latitude_longitude2 } .
OPTIONAL { <http://dbpedia.org/resource/Seattle>
<http://dbpedia.org/ontology/PopulatedPlace/populationDensity>
?populationDensity2 } .
OPTIONAL { <http://dbpedia.org/resource/Seattle>
<http://dbpedia.org/ontology/country>
?country2 } .
OPTIONAL { <http://dbpedia.org/resource/Seattle>
<http://www.w3.org/2000/01/rdf-schema#label>
?label2 . }
} LIMIT 30
This listing was manually edited to fit page width. In lines 34-36, we are trying to find a triple stating
which country Seattle is in. Please note that this triple matching pattern is generated as one line but
I had to manually edit it here to fit the page width.
The triple matching pattern in lines 34-36 must match some triple in DBPedia or no results will be
returned. In other words this matching pattern is mandatory. The four optional matching patterns in
lines 38-49 specify triple patterns that may be matched. In this example there is no triple matching
the following statement in the DBPedia knowledge base so the variable country2 is not bound and
the query returns no results for the variable country:
Knowledge Graph Navigator
<http://dbpedia.org/resource/Seattle> <http://dbpedia.org/ontology/country> ?country2
Notice also the syntax for GROUP_CONCAT used in lines 27-33, for example:
(GROUP_CONCAT (DISTINCT ?country2; SEPARATOR=' | ') AS ?country)
This collects all values assigned to the binding variable ?country2 into a string value using the
separator string “ | “. Using DISTINCT with GROUP_CONCAT conveniently discards duplicate
bindings for binding variables like ?country2.
Now that we have looked at SPARQL examples using OPTIONAL and GROUP_CONCAT, the
templates at the end of the following listing should be easier to understand.
The methods genericResults and genericAsString in the following listing are not currently used in
this example but I leave them as easy way to get information, given any entity URI. You are likely
to use these if you use the code for KGN in your projects.
For each entity type, for example city, I wrote one method like cityResults that returns an instance
of QueryResult calculated by using the JenaApis library from the chapter Semantic Web. For each
entity type there is another method, like cityAsString that converts an instance of QueryResult to
a formatted string for display.
We use the code pattern seen in lines 29-30 for each entity type. We use the static method
String.format to replace occurrences of %s in the entity template string with the string representa-
tion of entity URIs.
package com.knowledgegraphnavigator;
import com.markwatson.semanticweb.QueryResult;
import java.sql.SQLException; // Cache layer in JenaApis library throws this
public class EntityDetail {
static public QueryResult genericResults(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
String query =
String.format(
"select distinct ?p ?o where { %s ?p ?o . " +
FILTER (!regex(str(?p), 'wiki', 'i')) . " +
FILTER (!regex(str(?p), 'wiki', 'i')) } limit 10",
entityUri);
return endpoint.query(query);
Knowledge Graph Navigator
static public String genericAsString(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
QueryResult qr = genericResults(endpoint, entityUri);
return qr.toString();
static public QueryResult cityResults(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
String query =
String.format(cityTemplate, entityUri, entityUri, entityUri,
entityUri, entityUri);
return endpoint.query(query);
static public String cityAsString(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
QueryResult qr = cityResults(endpoint, entityUri);
return qr.toString();
static public QueryResult countryResults(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
String query =
String.format(countryTemplate, entityUri, entityUri, entityUri,
entityUri, entityUri);
return endpoint.query(query);
static public String countryAsString(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
QueryResult qr = countryResults(endpoint, entityUri);
return qr.toString();
static public QueryResult personResults(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
String query =
String.format(personTemplate, entityUri, entityUri, entityUri,
entityUri, entityUri);
return endpoint.query(query);
static public String personAsString(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
Knowledge Graph Navigator
QueryResult qr = personResults(endpoint, entityUri);
return qr.toString();
static public QueryResult companyResults(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
String query =
String.format(companyTemplate, entityUri, entityUri, entityUri,
entityUri, entityUri);
return endpoint.query(query);
static public String companyAsString(Sparql endpoint, String entityUri)
throws SQLException, ClassNotFoundException {
QueryResult qr = companyResults(endpoint, entityUri);
return qr.toString();
static private String companyTemplate =
"SELECT DISTINCT" +
(GROUP_CONCAT (DISTINCT ?industry2; SEPARATOR=' | ') AS ?industry)\n" +
(GROUP_CONCAT (DISTINCT ?netIncome2; SEPARATOR=' | ') AS ?netIncome)\n" +
(GROUP_CONCAT (DISTINCT ?label2; SEPARATOR=' | ') AS ?label)\n" +
(GROUP_CONCAT (DISTINCT ?comment2; SEPARATOR=' | ') AS ?comment)\n" +
(GROUP_CONCAT (DISTINCT ?numberOfEmployees2; SEPARATOR=' | ')\n" +
AS ?numberOfEmployees) {\n" +
%s <http://www.w3.org/2000/01/rdf-schema#comment>
?comment2 .\n" +
FILTER
(lang(?comment2) = 'en') .\n" +
OPTIONAL { %s <http://dbpedia.org/ontology/industry> ?industry2 } .\n" +
OPTIONAL { %s <http://dbpedia.org/ontology/netIncome> ?netIncome2 } .\n" +
OPTIONAL {\n" +
%s <http://dbpedia.org/ontology/numberOfEmployees> ?numberOfEmployees2\n" +
} .\n" +
OPTIONAL { %s <http://www.w3.org/2000/01/rdf-schema#label> ?label2 .\n" +
FILTER (lang(?label2) = 'en') } \n" +
"} LIMIT 30";
static private String personTemplate =
"SELECT DISTINCT\n" +
(GROUP_CONCAT (DISTINCT ?birthplace2; SEPARATOR=' | ') AS ?birthplace)
(GROUP_CONCAT (DISTINCT ?label2; SEPARATOR=' | ') AS ?label)
(GROUP_CONCAT (DISTINCT ?comment2; SEPARATOR=' | ') AS ?comment)
(GROUP_CONCAT (DISTINCT ?almamater2; SEPARATOR=' | ') AS ?almamater)
Knowledge Graph Navigator
(GROUP_CONCAT (DISTINCT ?spouse2; SEPARATOR=' | ') AS ?spouse) {
" %s <http://www.w3.org/2000/01/rdf-schema#comment>
?comment2 .\n" +
" FILTER
(lang(?comment2) = 'en') .
" OPTIONAL { %s <http://dbpedia.org/ontology/birthPlace> ?birthplace2 } .
" OPTIONAL { %s <http://dbpedia.org/ontology/almaMater> ?almamater2 } .
" OPTIONAL { %s <http://dbpedia.org/ontology/spouse> ?spouse2 } .
" OPTIONAL { %s
<http://www.w3.org/2000/01/rdf-schema#label> ?label2 . \n" +
FILTER
(lang(?label2) = 'en') }
" } LIMIT 10";
static private String countryTemplate =
"SELECT DISTINCT" +
(GROUP_CONCAT (DISTINCT ?areaTotal2; SEPARATOR=' | ') AS ?areaTotal)\n" +
(GROUP_CONCAT (DISTINCT ?label2; SEPARATOR=' | ') AS ?label)\n" +
(GROUP_CONCAT (DISTINCT ?comment2; SEPARATOR=' | ') AS ?comment)\n" +
(GROUP_CONCAT (DISTINCT ?populationDensity2; SEPARATOR=' | ')\n" +
AS ?populationDensity) {\n" +
%s <http://www.w3.org/2000/01/rdf-schema#comment>
?comment2 .\n" +
FILTER
(lang(?comment2) = 'en') .\n" +
OPTIONAL { %s <http://dbpedia.org/ontology/areaTotal> ?areaTotal2 } .\n" +
OPTIONAL {\n" +
%s <http://dbpedia.org/ontology/populationDensity> ?populationDensity2\n"\
} .\n" +
OPTIONAL { %s <http://www.w3.org/2000/01/rdf-schema#label> ?label2 . }\n" +
"} LIMIT 30";
static private String cityTemplate =
"SELECT DISTINCT\n" +
(GROUP_CONCAT (DISTINCT ?latitude_longitude2; SEPARATOR=' | ') \n" +
AS ?latitude_longitude) \n" +
(GROUP_CONCAT (DISTINCT ?populationDensity2; SEPARATOR=' | ')\n" +
AS ?populationDensity) \n" +
(GROUP_CONCAT (DISTINCT ?label2; SEPARATOR=' | ') AS ?label) \n" +
(GROUP_CONCAT (DISTINCT ?comment2; SEPARATOR=' | ') AS ?comment) \n" +
(GROUP_CONCAT (DISTINCT ?country2; SEPARATOR=' | ') AS ?country) { \n" +
" %s <http://www.w3.org/2000/01/rdf-schema#comment>
?comment2 .\n" +
FILTER
(lang(?comment2) = 'en') . \n" +
" OPTIONAL {\n" +
%s <http://www.w3.org/2003/01/geo/wgs84_pos#geometry>\n" +
?latitude_longitude2\n" +
" } . \n" +
" OPTIONAL {\n" +
Knowledge Graph Navigator
%s <http://dbpedia.org/ontology/PopulatedPlace/populationDensity>\n" +
?populationDensity2\n" +
" } . \n" +
" OPTIONAL { %s <http://dbpedia.org/ontology/country> ?country2 } . \n" +
" OPTIONAL { %s <http://www.w3.org/2000/01/rdf-schema#label> ?label2 . " +
FILTER
(lang(?label2) = 'en') } \n" +
"} LIMIT 30\n";
The class EntityRelationships in the next listing is used to find property relationships between two
entity URIs. The RDF statement matching FILTER on line 15 prevents matching statements where
the property contains the string “wiki” to avoid WikiData URI references. This class would need to be
rewritten to handle, for example, the WikiData Knowledge Base instead of the DBPedia Knowledge
Base. This class uses the JenaApis library developed in the chapter Semantic Web. The class Sparql
that we will look at later wraps the use of the JenaApis library.
package com.knowledgegraphnavigator;
import com.markwatson.semanticweb.QueryResult;
import java.sql.SQLException;
public class EntityRelationships {
static public QueryResult results(Sparql endpoint,
String entity1Uri, String entity2Uri)
throws SQLException, ClassNotFoundException {
String query =
String.format(
"select ?p where { %s ?p %s . "
FILTER (!regex(str(?p), 'wikiPage', 'i')) } limit 10",
entity1Uri, entity2Uri);
return endpoint.query(query);
The class Log in the next listing defines a shorthand out for calling System.out.println, an instance
of StringBuilder for storing all generated SPARQL queries made to DBPedia, and a utility method
for clearing the stored SPARQL queries. We use the cache of SPARQL queries to support the
interactive command “sparql” in the KGN application that we previously saw in an example when
we saw the use of this command to display all cached SPARQL queries demonstrating the use of
DISTINCT and GROUP_CONCAT.
Knowledge Graph Navigator
package com.knowledgegraphnavigator;
public class Log {
static public void out(String s) { System.out.println(s); }
static public StringBuilder sparql
= new StringBuilder();
static public void clearSparql() { sparql.delete(0, sparql.length()); }
The class PrintEntityResearchResults in the next listing takes results from multiple DBPedia
queries, formats the results, and displays them. The class constructor has no use except for the
side effect of displaying results to a user. The constructor requires the arguments:
• Sparql endpoint - we will look at the definition of class Sparql in the next section.
• List<EntityAndDescription> people - a list of person names and URIs.
• List<EntityAndDescription> companies - a list of company names and URIs.
• List<EntityAndDescription> cities - a list of city names and URIs.
• List<EntityAndDescription> countries - a list of country names and URIs.
I define static string values for a few ANSI terminal escape sequences for changing the default color
of text written to a terminal. If you are running on Windows you may need to set initialization
values for RESET, GREEN, YELLOW, PURPLE, and CYAN to empty strings “”.
package com.knowledgegraphnavigator;
import static com.knowledgegraphnavigator.Log.out;
import static com.knowledgegraphnavigator.Utils.removeBrackets;
import java.sql.SQLException;
import java.util.List;
public class PrintEntityResearchResults {
* Note for Windows users: the Windows console may not render the following
* ANSI terminal escape sequences correctly. If yo have problems, just
* change the following to the empty string "":
public static final String RESET
= "\u001B[0m"; // ANSI characters for styling
public static final String GREEN
= "\u001B[32m";
public static final String YELLOW = "\u001B[33m";
public static final String PURPLE = "\u001B[35m";
public static final String CYAN
= "\u001B[36m";
Knowledge Graph Navigator
private PrintEntityResearchResults() { }
public PrintEntityResearchResults(Sparql endpoint,
List<EntityAndDescription> people,
List<EntityAndDescription> companies,
List<EntityAndDescription> cities,
List<EntityAndDescription> countries)
throws SQLException, ClassNotFoundException {
out("\n" + GREEN + "Individual People:\n" + RESET);
for (EntityAndDescription person : people) {
" + GREEN + String.format("%-25s", person.entityName) +
PURPLE + " : " + removeBrackets(person.entityUri) + RESET);
out(EntityDetail.personAsString(endpoint, person.entityUri));
out("\n" + CYAN + "Individual Companies:\n" + RESET);
for (EntityAndDescription company : companies) {
" + CYAN + String.format("%-25s", company.entityName) +
YELLOW + " : " + removeBrackets(company.entityUri) + RESET);
out(EntityDetail.companyAsString(endpoint, company.entityUri));
out("\n" + GREEN + "Individual Cities:\n" + RESET);
for (EntityAndDescription city : cities) {
" + GREEN + String.format("%-25s", city.entityName) +
PURPLE + " : " + removeBrackets(city.entityUri) + RESET);
out(EntityDetail.cityAsString(endpoint, city.entityUri));
out("\n" + GREEN + "Individual Countries:\n" + RESET);
for (EntityAndDescription country : countries) {
" + GREEN + String.format("%-25s", country.entityName) +
PURPLE + " : " + removeBrackets(country.entityUri) + RESET);
out(EntityDetail.countryAsString(endpoint, country.entityUri));
out("");
The class Sparql in the next listing wraps the JenaApis library from the chapter Semantic Web.
I set the SPARQL endpoint for DBPedia on line 13. I set and commented out the WikiData
SPARQL endpoint on lines 11-12. The KGN application will not work with WikiData without some
modifications. If you enjoy experimenting with KGN then you might want to clone it and enable
it to work simultaneously with DBPedia, WikiData, and local RDF files by using three instances of
the class JenaApis.
Knowledge Graph Navigator
Notice that we are importing the value of a static StringBuffer com.knowledgegraphnavigator.Log.sparql
on line 5. We will use this for storing SPARQL queries for display to the user.
package com.knowledgegraphnavigator;
import com.markwatson.semanticweb.QueryResult;
import com.markwatson.semanticweb.JenaApis;
import static com.knowledgegraphnavigator.Log.sparql;
import static com.knowledgegraphnavigator.Log.out;
import java.sql.SQLException;
public class Sparql {
//static private String endpoint =
"https://query.wikidata.org/bigdata/namespace/wdq/sparql";
static private String endpoint = "https://dbpedia.org/sparql";
public Sparql() {
this.jenaApis = new JenaApis();
public QueryResult query(String sparqlQuery)
throws SQLException, ClassNotFoundException {
//out(sparqlQuery); // debug for now...
sparql.append(sparqlQuery);
sparql.append(("\n\n"));
return jenaApis.queryRemote(endpoint, sparqlQuery);
private JenaApis jenaApis;
The class Utils contains one utility method removeBrackets that is used to convert a URI in SPARQL
RDF statement form:
<http://dbpedia.org/resource/Seattle>
http://dbpedia.org/resource/Seattle
The single method removeBrackets is only used in the class PrintEntityResearchResults.
Knowledge Graph Navigator
package com.knowledgegraphnavigator;
public class Utils {
static public String removeBrackets(String s) {
if (s.startsWith("<")) return s.substring(1, s.length() - 1);
return s;
Finally we get to the main program implemented in the class KGN. The interactive program is
implemented in the class constructor with the heart of the code being the while loop in lines 26-119
that accepts text input from the user, detects entity names and the corresponding entity types in
the input text, and uses the Java classes we just looked at to find information on DBPedia for the
entities in the input text as well as finding relations between these entities. Instead of entering a list
of entity names the user can also enter either of the commands sparql (which we saw earlier in an
example) or demo (to use a randomly chosen example query).
We use the class TextToDbpediaUris on line 38 to get the entity names and types found in the input
text. You can refer back to chapter Resolve Entity Names to DBPedia References for details on using
the class TextToDbpediaUris.
The loops in lines 39-70 store entity details that are displayed by calling PrintEntityResearchResults
in lines 72-76. The nested loops over person entities in lines 78-91 calls EntityRelationships.results
to look for relationships between two different person URIs. The same operation is done in the nested
loops in lines 93-104 to find relationships between people and companies. The nested loops in lines
105-118 finds relationships between different company entities.
The static method main in lines 134-136 simply creates an instance of class KGN which has the side
effect of running the example KGN program.
package com.knowledgegraphnavigator;
import com.markwatson.ner_dbpedia.TextToDbpediaUris;
import com.markwatson.semanticweb.QueryResult;
import static com.knowledgegraphnavigator.Log.out;
import static com.knowledgegraphnavigator.Log.sparql;
import static com.knowledgegraphnavigator.Log.clearSparql;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Scanner;
Knowledge Graph Navigator
public class KGN {
private static List<String> demosList =
Arrays.asList(
"Bill Gates and Melinda Gates worked at Microsoft",
"IBM opened an office in Canada",
"Steve Jobs worked at Apple Computer and visited IBM and Microsoft in Seattle");
public KGN() throws Exception {
Sparql endpoint = new Sparql();
while (true) {
String query = getUserQueryFromConsole();
out("\nProcessing query:\n" + query + "\n");
if (query.equalsIgnoreCase("sparql")) {
out("Generated SPARQL used to get current results:\n");
out(sparql.toString());
out("\n");
clearSparql();
} else {
if (query.equalsIgnoreCase("demo")) {
query = demosList.get((int) (Math.random() * (demosList.size() + 1)));
TextToDbpediaUris kt = new TextToDbpediaUris(query);
List<EntityAndDescription> userSelectedPeople = new ArrayList();
if (kt.personNames.size() > 0) {
for (int i = 0; i < kt.personNames.size(); i++) {
userSelectedPeople.add(
new EntityAndDescription(kt.personNames.get(i),
kt.personUris.get(i)));
List<EntityAndDescription> userSelectedCompanies = new ArrayList();
if (kt.companyNames.size() > 0) {
for (int i = 0; i < kt.companyNames.size(); i++) {
userSelectedCompanies.add(
new EntityAndDescription(kt.companyNames.get(i),
kt.companyUris.get(i)));
List<EntityAndDescription> userSelectedCities = new ArrayList();
if (kt.cityNames.size() > 0) {
out("+++++ kt.cityNames:" + kt.cityNames.toString());
Knowledge Graph Navigator
for (int i = 0; i < kt.cityNames.size(); i++) {
userSelectedCities.add(
new EntityAndDescription(kt.cityNames.get(i), kt.cityUris.get(i)));
List<EntityAndDescription> userSelectedCountries = new ArrayList();
if (kt.countryNames.size() > 0) {
out("+++++ kt.countryNames:" + kt.countryNames.toString());
for (int i = 0; i < kt.countryNames.size(); i++) {
userSelectedCountries.add(
new EntityAndDescription(kt.countryNames.get(i),
kt.countryUris.get(i)));
new PrintEntityResearchResults(endpoint,
userSelectedPeople,
userSelectedCompanies,
userSelectedCities,
userSelectedCountries);
for (EntityAndDescription person1 : userSelectedPeople) {
for (EntityAndDescription person2 : userSelectedPeople) {
if (person1 != person2) {
QueryResult qr =
EntityRelationships.results(endpoint, person1.entityUri,
person2.entityUri);
if (qr.rows.size() > 0) {
out("Relationships between person " + person1.entityName +
" person " + person2.entityName + ":");
out(qr.toString());
for (EntityAndDescription person : userSelectedPeople) {
for (EntityAndDescription company : userSelectedCompanies) {
QueryResult qr =
EntityRelationships.results(endpoint, person.entityUri,
company.entityUri);
if (qr.rows.size() > 0) {
out("Relationships between person " + person.entityName +
" company " + company.entityName + ":");
Knowledge Graph Navigator
out(qr.toString());
for (EntityAndDescription company1 : userSelectedCompanies) {
for (EntityAndDescription company2 : userSelectedCompanies) {
if (company1 != company2) {
QueryResult qr =
EntityRelationships.results(endpoint, company1.entityUri,
company2.entityUri);
if (qr.rows.size() > 0) {
out("Relationships between company " + company1.entityName +
" company " + company2.entityName + ":");
out(qr.toString());
private String getUserQueryFromConsole() {
out("Enter entities query:");
Scanner input = new Scanner(System.in);
String ret = "";
while (input.hasNext()) {
ret = input.nextLine();
break;
return ret;
public static void main(String[] args) throws Exception {
new KGN();
This KGN example was hopefully both interesting to you and simple enough in its implementation
(because we relied heavily on code from the last two chapters) that you feel comfortable modifying
it and reusing it as a part of your own Java applications.
Knowledge Graph Navigator
Wrap-up
If you enjoyed running and experimenting with this example and want to modify it for your own
projects then I hope that I provided a sufficient road map for you to do so.
I suggest further projects that you might want to try implementing with this example:
• Write a web application that processes news stories and annotates them with additional data
from DBPedia and/or WikiData.
• In a web or desktop application, detect entities in text and display additional information when
the user’s mouse cursor hovers over a word or phrase that is identified as an entity found in
DBPedia or WikiData.
• Clone this KGN example and enable it to work simultaneously with DBPedia, WikiData, and
local RDF files by using three instances of the class JenaApis and in the main application loop
access all three data sources.
I had the idea for the KGN application because I was spending quite a bit of time manually
setting up SPARQL queries for DBPedia (and other public sources like WikiData) and I wanted
to experiment with partially automating this process. I have experimented with versions of KGN
written in Java, Hy language (Lisp running on Python that I wrote a short book on⁸⁰), Swift,
and Common Lisp and all four implementations take different approaches as I experimented with
different ideas. You might want to check out my web site devoted to different versions of KGN:
www.knowledgegraphnavigator.com⁸¹.
⁸⁰https://leanpub.com/hy-lisp-python/read
⁸¹http://www.knowledgegraphnavigator.com/
Conclusions
The material in this book was informed by my own work experience designing systems and writing
software for artificial intelligence and information processing. If you enjoyed reading this book and
you make practical use of at least some of the material I covered, then I consider my effort to be
worthwhile.
Writing software is a combination of a business activity, promoting good for society, and an
exploration to try out new ideas for self improvement. I believe that there is sometimes a fine line
between spending too many resources tracking many new technologies versus getting stuck using
old technologies at the expense of lost opportunities. My hope is that reading this book was an
efficient and pleasurable use of your time, letting you try some new techniques and technologies
that you had not considered before.
When we do expend resources to try new things it is almost always best to perform many small
experiments and then dig deeper into areas that have a good chance of providing high value and
capturing your interest.
Fail fast is a common meme but failure that we do not learn from is a waste.
I have been using the Java platform from the very beginning and although I also use many other
programming languages in my work and studies, both the Java language and the Java platform
provide high efficiency, scalability, many well-trained developers, and a wealth of existing infras-
tructure software and libraries. Investment in Java development also pays when using alternative
JVM languages like JRuby, Scala, and Clojure.
If we never get to meet in person or talk on the telephone, then I would like to thank you now for
taking the time to read my book.
LangChain and LlamaIndex
Projects Lab Book: Hooking
Large Language Models Up to
the Real World
Using GPT-3, ChatGPT, and Hugging
Face Models in Applications.
Mark Watson
This book is for sale at http://leanpub.com/langchain
This version was published on 2023-06-08
This is a Leanpub book. Leanpub empowers authors and
publishers with the Lean Publishing process. Lean Publishing is
the act of publishing an in-progress ebook using lightweight tools
and many iterations to get reader feedback, pivot until you have
the right book and build traction once you do.
This work is licensed under a Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International License
Contents
1 Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 About the Author . . . . . . . . . . . . . . . . . . . . .
1.2 Book Cover . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Acknowledgements . . . . . . . . . . . . . . . . . . . .
1.4 Requirements for Running and Modifying Book Ex-
amples . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Issues and Workarounds for Using the Material in this
. . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Large Language Model Overview . . . . . . . . . . . . . .
2.1 Big Tech Businesses vs. Small Startups Using Large
Language Models . . . . . . . . . . . . . . . . . . . .
3 Getting Started With LangChain . . . . . . . . . . . . . .
3.1 Installing Necessary Packages . . . . . . . . . . . . . .
3.2 Creating a New LangChain Project . . . . . . . . . . .
3.3 Basic Usage and Examples . . . . . . . . . . . . . . . .
3.4 Creating Embeddings . . . . . . . . . . . . . . . . . . .
3.5 Using LangChain Vector Stores to Query Documents
3.6 LangChain Overview Wrap Up . . . . . . . . . . . . .
4 Overview of LlamaIndex . . . . . . . . . . . . . . . . . . .
4.1 Using LlamaIndex to Search Local Documents Using
GPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Using LlamaIndex for Question Answering from a List
of Web Sites . . . . . . . . . . . . . . . . . . . . . . .
CONTENTS
4.3 LlamaIndex/GPT-Index Case Study Wrap Up . . . . .
5 Using Google’s Knowledge Graph APIs With LangChain 28
5.1 Setting Up To Access Google Knowledge Graph APIs
6 Using DBPedia and WikiData as Knowledge Sources . .
6.1 Using DBPedia as a Data Source
. . . . . . . . . . . .
6.2 Using Wikidata as a Data Source . . . . . . . . . . . .
7 Using LLMs To Organize Information in Our Google
Drives . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.1 Setting Up Requirements.
. . . . . . . . . . . . . . . .
7.2 Write Utility To Fetch All Text Files From Top Level
Google Drive Folder
. . . . . . . . . . . . . . . . . .
7.3 Generate Vector Indices for Files in Specific Google
Drive Directories . . . . . . . . . . . . . . . . . . . .
7.4 Google Drive Example Wrap Up
. . . . . . . . . . . .
8 Using Zapier Integrations With GMail and Google
Calendar . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.1 Set Up Development Environment . . . . . . . . . . .
8.2 Sending a Test GMail . . . . . . . . . . . . . . . . . . .
8.3 Google Calendar Integration Example . . . . . . . . .
9 Natural Language SQLite Database Queries With
LangChain . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.1 Natural Language Database Query Wrap Up
. . . . .
10 Examples Using Hugging Face Open Source Models . .
10.1 Using LangChain as a Wrapper for Hugging Face
Prediction Model APIs . . . . . . . . . . . . . . . . .
10.2 Creating a Custom LlamaIndex Hugging Face LLM
Wrapper Class That Runs on Your Laptop . . . . . .
11 Using Large Language Models to Write Recipes . . . .
11.1 Preparing Recipe Data . . . . . . . . . . . . . . . . . .
CONTENTS
11.2 A Prediction Model Using the OpenAI text-davinci-
002 Model . . . . . . . . . . . . . . . . . . . . . . . .
11.3 Cooking Recipe Generation Wrap Up . . . . . . . . .
12 Book Wrap Up
. . . . . . . . . . . . . . . . . . . . . . . .
1 Preface
I have been working in the field of artificial intelligence since 1982
and without a doubt Large Language Models (LLMs) like GPT-3 and
infrastructure projects like LangChain are the largest technology
breakthroughs that I have lived through. This book will use the
LangChain1 and GPT Index (LlamaIndex)2 projects along with the
OpenAI GPT-3 and ChatGPT APIs to solve a series of interesting
problems.
Harrison Chase started the LangChain project in October 2022 and
as I write this book in February 2023 the GitHub repository for
LangChain https://github.com/hwchase17/langchain has 171 con-
tributors. Jerry Liu started the GPT Index project (recently renamed
to LlamaIndex) at the end of 2022 and the GitHub Repository for
LlamaIndex https://github.com/jerryjliu/gpt_index currently has
54 contributors.
The GitHub repository for examples in this book is https://github.
com/mark-watson/langchain-book-examples.git. Please note that I
usually update the code in the examples repository fairly frequently
for library version updates, etc.
While the documentation and examples online for LangChain and
LlamaIndex are excellent, I am still motivated to write this book
to solve interesting problems that I like to work on involving
information retrieval, natural language processing (NLP), dialog
agents, and the semantic web/linked data fields. I hope that you,
dear reader, will be delighted with these examples and that at least
some of them will inspire your future projects.
1https://github.com/hwchase17/langchain
2https://github.com/jerryjliu/gpt_index
Preface
1.1 About the Author
I have written over 20 books, I have over 50 US patents, and I have
worked at interesting companies like Google, Capital One, SAIC,
Mind AI, and others. You can find links for reading most of my
recent books free on my web site https://markwatson.com. If I had
to summarize my career the short take would be that I have had a
lot of fun and enjoyed my work. I hope that what you learn here
will be both enjoyable and help you in your work.
If you would like to support my work please consider purchasing
my books on Leanpub3 and star my git repositories that you find
useful on GitHub4. You can also interact with me on social media
on Mastodon5 and Twitter6. I am also available as a consultant:
https://markwatson.com.
1.2 Book Cover
I live in Sedona, Arizona. I took the book cover photo in January
2023 from the street that I live on.
1.3 Acknowledgements
This picture shows me and my wife Carol who helps me with book
production and editing.
3https://leanpub.com/u/markwatson
4https://github.com/mark-watson?tab=repositories&q=&type=public
5https://mastodon.social/@mark_watson
6https://twitter.com/mark_l_watson
Preface
Mark and Carol Watson
I would also like to thank the following readers who reported errors
or typos in this book: Armando Flores, Peter Solimine, and David
1.4 Requirements for Running and
Modifying Book Examples
I show full source code and a fair amount of example output for
each book example so if you don’t want to get access to some of
the following APIs then you can still read along in the book.
To use OpenAI’s GPT-3 and ChatGPT models you will need to sign
Preface
up for an API key (free tier is OK) at https://openai.com/api/ and set
the environment variable OPENAI_API_KEY to your key value.
You will need to get an API key for examples using Google’s
Knowledge Graph APIs.
Reference: Google Knowledge Graph APIs7.
The example programs using Google’s Knowledge Graph APIs
assume that you have the file ~/.google_api_key in your home
directory that contains your key from https://console.cloud.google.
com/apis.
You will need to install SerpApi for examples integrating web
search. Please reference: PyPi project page8.
You can sign up for a free non-commercial 100 searches/month
account with an email address and phone number at https://serpapi.
com/users/welcome.
You will also need Zapier9 account for the GMail and Google
Calendar examples.
After reading though this book, you can review the website
LangChainHub10 which contains prompts, chains and agents that
are useful for building LLM applications.
1.5 Issues and Workarounds for
Using the Material in this Book
The libraries that I use in this book are frequently updated and
sometimes the documentation or code links change, invalidating
links in this book. I will try to keep everything up to date. Please
report broken links to me.
7https://cloud.google.com/enterprise-knowledge-graph/docs/search-api
8https://pypi.org/project/google-search-results/
9https://zapier.com
10https://github.com/hwchase17/langchain-hub
Preface
In some cases you will need to use specific versions or libraries for
some of the code examples.
Because the Python code listings use colorized text you may find
that copying code from this eBook may drop space characters. All
of the code listings are in the GitHub repository for this book so you
should clone the repository to experiment with the example code.
2 Large Language Model
Overview
Large language models1 are a subset of artificial intelligence that
use deep learning and neural networks to process natural language.
Transformers2 are a type of neural network architecture that can
learn context in sequential data using self-attention mechanisms.
They were introduced in 2017 by a team at Google Brain and have
become popular for LLM research. Some examples of transformer-
based3 LLMs are BERT, GPT-3, T5 and Megatron-LM4.
The main points we will discuss in this book are:
• LLMs are deep learning algorithms that can understand and
generate natural language based on massive datasets.
• LLMs use techniques such as self-attention, masking, and
fine-tuning to learn complex patterns and relationships in
language. LLMs can understand and generate natural lan-
guage because they use transformer models, which are a
type of neural network that can process sequential data such
as text using attention mechanisms. Attention mechanisms
allow the model to focus on relevant parts of the input and
output sequences while ignoring irrelevant ones.
• LLMs can perform various natural language processing (NLP)
and natural language generation (NLG) tasks, such as sum-
marization, translation, prediction, classification, and ques-
tion answering.
1https://blogs.nvidia.com/blog/2022/10/10/llms-ai-horizon/
2https://www.linkedin.com/pulse/chatgpt-tip-iceberg-paul-golding
3https://factored.ai/transformer-based-language-models/
4https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
Large Language Model Overview
• Even though LLMs were initially developed for NLP applica-
tions, LLMs have also shown potential in other domains such
as computer vision and computational biology by leveraging
their generalizable knowledge and transfer learning abilities.
BERT models5 are one of the first types of transformer models that
were widely used. BERT was developed by Google AI Language in
2018. BERT models are a family of masked language models that
use transformer architecture to learn bidirectional representations
of natural language. BERT models can understand the meaning
of ambiguous words by using the surrounding text as context. The
“magic trick” here is that training data comes almost free because in
masking models, you programatically chose random words, replace
them with a missing word token, and the model is trained to predict
the missing words. This process is repeated with massive amounts
of training data from the web, books, etc.
Here are some “papers with code” links for BERT (links are for code,
paper links in the code repositories):
• https://github.com/allenai/scibert
• https://github.com/google-research/bert
2.1 Big Tech Businesses vs. Small
Startups Using Large Language
Models
Both Microsoft and Google play both sides of this business game:
they want to sell cloud LLM services to developers and small startup
companies and they would also like to achieve lock-in for their
consumer services like Office 365, Google Docs and Sheets, etc.
5https://en.wikipedia.org/wiki/BERT_(Language_model)
Large Language Model Overview
Microsoft has been integrating AI technology into workplace
emails, slideshows, and spreadsheets as part of its ongoing
partnership
OpenAI,
company
behind
ChatGPT.
Microsoft’s Azure OpenAI service offers a powerful tool to enable
these outcomes when leveraged with their data lake of more than
two billion metadata and transactional elements.
As I write this, Google has just opened a public wait list for their
Bard AI/chat search service. I have used various Google APIs for
years in code I write. I have no favorites in the battle between tech
giants, rather I am mostly interested in what they build that I can
use in my own projects.
Hugging Face, which makes AI products and hosts those developed
by other companies, is working on open-source rivals to ChatGPT
and will use AWS6 for that as well. Cohere AI, Anthropic, Hugging
Face, and Stability AI are some of the startups that are using
OpenAI and Hugging Face APIs for their products.
As I write
this chapter, I view Hugging Face as a great source of specialized
models. I love that Hugging Face models can be run via their APIs
and also self-hosted on our own servers and sometimes even on
our laptops. Hugging Face is a fantastic resource and even though
I use their models much less frequently in this book than OpenAI
APIs, you should embrace the hosting and open source flexibility
of Hugging Face. Here I use OpenAI APIs because I want you to
get set up and creating your own projects quickly.
Dear reader, I didn’t write this book for developers working at
established AI companies (although I hope such people find the
material here useful!). I wrote this book for small developers who
want to scratch their own itch by writing tools that save them time.
I also wrote this book hoping that it would help developers build
capabilities into the programs they design and write that rival what
the big tech companies are doing.
6https://iblnews.org/aws-partners-with-hugging-face-an-ai-startup-rival-to-chatgpt-
working-on-open-source-models/
3 Getting Started With
LangChain
Harrison Chase started the LangChain project in October 2022
and as I write this book in February 2023 the GitHub repository
for LangChain https://github.com/hwchase17/langchain has 171
contributors.
LangChain1 is a framework for building applications with large
language models (LLMs) through chaining different components
together.
Some of the applications of LangChain are chatbots,
generative question-answering, summarization, data-augmented
generation and more. LangChain can save time in building chat-
bots and other systems by providing a standard interface for chains,
agents and memory, as well as integrations with other tools and
end-to-end examples. We refer to “chains” as sequences of calls
(to an LLMs and a different program utilities, cloud services, etc.)
that go beyond just one LLM API call.
LangChain provides a
standard interface for chains, many integrations with other tools,
and end-to-end chains for common applications. Often you will
find existing chains already written that meet the requirements for
your applications.
For example, one can create a chain that takes user input, formats
it using a PromptTemplate, and then passes the formatted response
to a Large Language Model (LLM) for processing.
While LLMs are very general in nature which means that while
they can perform many tasks effectively, they often can not directly
provide specific answers to questions or tasks that require deep
domain knowledge or expertise. LangChain provides a standard
1https://langchain.readthedocs.io/en/latest/index.html
Getting Started With LangChain
interface for agents, a library of agents to choose from, and exam-
ples of end-to-end agents.
LangChain Memory is the concept of persisting state between calls
of a chain or agent. LangChain provides a standard interface for
memory, a collection of memory implementations, and examples
of chains/agents that use memory². LangChain provides a large
collection of common utils to use in your application. Chains go
beyond just a single LLM call, and are sequences of calls (whether
to an LLM or a different utility). LangChain provides a standard
interface for chains, lots of integrations with other tools, and end-
to-end chains for common applications.
LangChain can be integrated with one or more model providers,
data stores, APIs, etc.
LangChain can be used for in-depth
question-and-answer chat sessions, API interaction, or action-
taking.
LangChain can be integrated with Zapier’s platform
through a natural language API interface (we have an entire
chapter dedicated to Zapier integrations).
3.1 Installing Necessary Packages
For the purposes of examples in this book, you might want to create
a new Anaconda or other Python environment and install:
pip install langchain llama_index openai
pip install kor pydrive pandas rdflib
pip install google-search-results SPARQLWrapper
For the rest of this chapter we will use the subdirectory langchain_-
getting_started and in the next chapter use llama-index_case_-
study in the GitHub repository for this book.
Getting Started With LangChain
3.2 Creating a New LangChain
Project
Simple LangChain projects are often just a very short Python script
file. As you read this book, when any example looks interesting or
useful, I suggest copying the requirements.txt and Python source
files to a new directory and making your own GitHub private
repository to work in. Please make the examples in this book “your
code,” that is, freely reuse any code or ideas you find here.
3.3 Basic Usage and Examples
While I try to make the material in this book independent, some-
thing you can enjoy with no external references, you should also
take advantage of the high quality [documentation](Langchain
Quickstart Guide) and the individual detailed guides for prompts,
chat, document loading, indexes, etc.
As we work through some examples please keep in mind what it
is like to use the ChatGPT web application: you enter text and get
repsonses. The way you prompt ChatGPT is obviously important
if you want to get useful responses. In code examples we automate
and formalize this manual process.
You need to choose a LLM to use. We will usually choose the GPT-
3.5 API from OpenAI because it is general purpose and much less
expensive than OpenAI’s previous model APIs. You will need to
sign up2 for an API key and set it as an environment variable:
export OPENAI_API_KEY="YOUR KEY GOES HERE"
Both the libraries openai and langchain will look for this environ-
ment variable and use it. We will look at a few simple examples in a
2https://platform.openai.com/account/api-keys
Getting Started With LangChain
Python REPL. We will start by just using OpenAI’s text prediction
$ python
>>> from langchain.llms import OpenAI
>>> llm = OpenAI(temperature=0.8)
>>> s = llm("John got into his new sports car, and he dro\
ve it")
' to work\n\nJohn started up his new sports car and drove\
it to work. He had a huge smile on his face as he drove,
excited to show off his new car to his colleagues. The w
ind blowing through his hair, and the sun on his skin, he
felt a sense of freedom and joy as he cruised along the
road. He arrived at work in no time, feeling refreshed an
d energized.'
>>> s = llm("John got into his new sports car, and he dro\
ve it")
" around town\n\nJohn drove his new sports car around tow\
n, enjoying the feeling of the wind in his hair. He stopp
ed to admire the view from a scenic lookout, and then spe
d off to the mall to do some shopping. On the way home, h
e took a detour down a winding country road, admiring the
scenery and enjoying the feel of the car's powerful engi
ne. By the time he arrived back home, he had a huge smile
on his face."
Notice how when we ran the same input text prompt twice that
we see different results.Setting the temperature in line 3 to a higher
value increases the randomness.
Our next example is in the source file directions_template.py
and uses the PromptTemplate class.
A prompt template is a
reproducible way to generate a prompt. It contains a text string
(“the template”), that can take in a set of parameters from the end
Getting Started With LangChain
user and generate a prompt. The prompt template may contain
language model instructions, few-shot examples to improve the
model’s response, or specific questions for the model to answer.
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
llm = OpenAI(temperature=0.9)
def get_directions(thing_to_do):
prompt = PromptTemplate(
input_variables=["thing_to_do"],
template="How do I {thing_to_do}?",
prompt_text = prompt.format(thing_to_do=thing_to_do)
print(f"\n{prompt_text}:")
return llm(prompt_text)
print(get_directions("get to the store"))
print(get_directions("hang a picture on the wall"))
You could just write Python string manipulation code to create a
prompt but using the utiltiy class PromptTemplate is more legible
and works with any number of prompt input variables.
The output is:
$ python directions_template.py
How do I get to the store?:
To get to the store, you will need to use a mode of trans\
portation such as walking, driving, biking, or taking pub
lic transportation. Depending on the location of the stor
e, you may need to look up directions or maps to determin
e the best route to take.
Getting Started With LangChain
How do I hang a picture on the wall?:
1. Find a stud in the wall, or use two or three wall anch\
ors for heavier pictures.
2. Measure and mark the wall where the picture hanger wil\
3. Pre-drill the holes and place wall anchors if needed.
4. Hammer the picture hanger into the holes.
5. Hang the picture on the picture hanger.
The next example in the file country_information.py is derived
from an example in the LangChain documentation. In this example
we use PromptTemplate that contains the pattern we would like
the LLM to use when returning a response.
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
llm = OpenAI(temperature=0.9)
def get_country_information(country_name):
print(f"\nProcessing {country_name}:")
global prompt
if "prompt" not in globals():
print("Creating prompt...")
prompt = PromptTemplate(
input_variables=["country_name"],
template = """
Predict the capital and population of a country.
Country: {country_name}
Capital:
Population:""",
prompt_text = prompt.format(country_name=country_name)
print(prompt_text)
Getting Started With LangChain
return llm(prompt_text)
print(get_country_information("Canada"))
print(get_country_information("Germany"))
You can use the ChatGPT web interface to experiment with
prompts and when you find a pattern that works well then write
a Python script like the last example, but changing the data you
supply in the PromptTemplate instance.
The output of the last example is:
$ python country_information.py
Processing Canada:
Creating prompt...
Predict the capital and population of a country.
Country: Canada
Capital:
Population:
Capital: Ottawa
Population: 37,058,856 (as of July 2020)
Processing Germany:
Predict the capital and population of a country.
Country: Germany
Capital:
Population:
Getting Started With LangChain
Capital: Berlin
Population: 83,02 million (est. 2019)
3.4 Creating Embeddings
We will reference the LangChain embeddings documentation3. We
can use a Python REPL to see what text to vector space embeddings
might look like:
$ python
Python 3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6\
] on darwin
Type "help", "copyright", "credits" or "license" for more\
information.
>>> from langchain.embeddings import OpenAIEmbeddings
>>> embeddings = OpenAIEmbeddings()
>>> text = "Mary has blond hair and John has brown hair. \
Mary lives in town and John lives in the country."
>>> doc_embeddings = embeddings.embed_documents([text])
>>> doc_embeddings
[[0.007727328687906265, 0.0009025644976645708, -0.0033224\
383369088173, -0.01794492080807686, -0.017969949170947075
, 0.028506645932793617, -0.013414892368018627, 0.00466768
16418766975, -0.0024965214543044567, -0.02662956342101097
>>> query_embedding = embeddings.embed_query("Does John l\
ive in the city?")
>>> query_embedding
[0.028048301115632057, 0.011499025858938694, -0.009440079\
33139801, -0.020809611305594444, -0.023904507979750633, 0
.018750663846731186, -0.01626438833773136, 0.018129095435
3https://python.langchain.com/en/latest/reference/modules/embeddings.html
Getting Started With LangChain
142517,
Notice that the doc_embeddings is a list where each list element
is the embeddings for one input text document.
The query_-
embedding is a single embedding. Please read the above linked
embedding documentation.
We will use vector stores to store calculated embeddings for future
use. In the next chapter we will see a document database search
example using LangChain and Llama-Index.
3.5 Using LangChain Vector Stores to
Query Documents
We will reference the LangChain Vector Stores documentation4.
You need to install a few libraries:
pip install chroma
pip install chromadb
pip install unstructured pdf2image pytesseract
The example script is doc_search.py:
4https://python.langchain.com/en/latest/reference/modules/vectorstore.html
Getting Started With LangChain
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import DirectoryLoader
from langchain import OpenAI, VectorDBQA
embeddings = OpenAIEmbeddings()
loader = DirectoryLoader('../data/', glob="**/*.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=2500, ch\
unk_overlap=0)
texts = text_splitter.split_documents(documents)
docsearch = Chroma.from_documents(texts, embeddings)
qa = VectorDBQA.from_chain_type(llm=OpenAI(),
chain_type="stuff",
vectorstore=docsearch)
def query(q):
print(f"Query: {q}")
print(f"Answer: {qa.run(q)}")
query("What kinds of equipment are in a chemistry laborat\
ory?")
query("What is Austrian School of Economics?")
query("Why do people engage in sports?")
query("What is the effect of body chemistry on exercise?")
The DirectoryLoader class is useful for loading a directory full of
input documents. In this example we specified that we only want
to process text files, but the file matching pattern could have also
specified PDF files, etc.
Getting Started With LangChain
The output is:
$ python doc_search.py
Created a chunk of size 1055, which is longer than the sp\
ecified 1000
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transie\
Query: What kinds of equipment are in a chemistry laborat\
Answer:
A chemistry lab would typically include glasswar\
e, such as beakers, flasks, and test tubes, as well as ot
her equipment such as scales, Bunsen burners, and thermom
eters.
Query: What is Austrian School of Economics?
Answer:
The Austrian School is a school of economic thou\
ght that emphasizes the spontaneous organizing power of t
he price mechanism. Austrians hold that the complexity of
subjective human choices makes mathematical modelling of
the evolving market extremely difficult and advocate a "
laissez faire" approach to the economy. Austrian School e
conomists advocate the strict enforcement of voluntary co
ntractual agreements between economic agents, and hold th
at commercial transactions should be subject to the small
est possible imposition of forces they consider to be (in
particular the smallest possible amount of government in
tervention). The Austrian School derives its name from it
s predominantly Austrian founders and early supporters, i
ncluding Carl Menger, Eugen von Bohm-Bawerk and Ludwig vo
n Mises.
Query: Why do people engage in sports?
Answer:
People engage in sports for leisure and entertai\
nment, as well as for physical exercise and athleticism.
Query: What is the effect of body chemistry on exercise?
Answer:
Body chemistry can affect the body's response to\
exercise, as certain hormones and enzymes produced by th
Getting Started With LangChain
e body can affect the energy levels and muscle performanc
e. Chemicals in the body, such as adenosine triphosphate
(ATP) and urea, can affect the body's energy production a
nd muscle metabolism during exercise. Additionally, the b
ody's levels of electrolytes, vitamins, and minerals can
affect exercise performance.
Exiting: Cleaning up .chroma directory
3.6 LangChain Overview Wrap Up
We will continue using LangChain for the rest of this book as well
as the LlamaIndex library that we introduce in the next chapter.
I cover just the subset of LangChain that I use in my own projects
in this book. I urge you to read the LangChain documentation
and to explore public LangChain chains that users have written on
Langchain-hub5.
5https://github.com/hwchase17/langchain-hub
4 Overview of
LlamaIndex
The popular LlamaIndex project used to be called GPT-Index but
has been generalized to work with more models that just GPT-3,
for example using Hugging Face embeddings1.
LlamaIndex2 is a project that provides a central interface to connect
your language models with external data. It was created by Jerry
Liu and his team in the fall of 2022.
It consists of a set of
data structures designed to make it easier to use large external
knowledge bases with language models3. Some of its uses are:
• Querying structured data such as tables or databases using
natural language
• Retrieving relevant facts or information from large text cor-
• Enhancing language models with domain-specific knowledge
LlamaIndex supports a variety of document types, including:
• Text documents are the most common type of document.
They can be stored in a variety of formats, such as .txt, .doc,
and .pdf.
• XML documents are a type of text document that is used to
store data in a structured format.
• JSON documents are a type of text document that is used to
store data in a lightweight format.
1https://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html#
custom-embeddings
2https://github.com/jerryjliu/gpt_index/blob/main/docs/index.rst
3https://gpt-index.readthedocs.io/en/latest/index.html
Overview of LlamaIndex
• HTML documents are a type of text document that is used to
create web pages.
• PDF documents are a type of text document that is used to
store documents in a fixed format.
LlamaIndex can also index data that is stored in a variety of
databases, including:
• SQL databases such as MySQL, PostgreSQL, and Oracle.
NoSQL databases such as MongoDB, Cassandra,
CouchDB.
• Solr is a popular open-source search engine that provides high
performance and scalability.
• Elasticsearch is another popular open-source search engine
that offers a variety of features, including full-text search,
geospatial search, and machine learning.
• Apache Cassandra is a NoSQL database that can be used to
store large amounts of data.
• MongoDB is another NoSQL database that is easy to use and
scale.
• PostgreSQL is a relational database that is widely used in
enterprise applications.
LlamaIndex is a flexible framework that can be used to index a
variety of document types and data sources.
We will look first at a short example derived from the LlamaIndex
documentation and later look at the parts of the LlmaIndex source
code that uses LangChain.
4.1 Using LlamaIndex to Search
Local Documents Using GPT-3
The following example is similar to the last example in the overview
chapter on LangChain.
In line 8 we use a utility data loader
Overview of LlamaIndex
function provided by LlamaIndex to read documents in an input
directory. As a demonstration we save the index (consisting of
document embeddings) to disk and reload it. This technique is
useful when you have a large number of static documents so the
indexing procedure can take a while and require many OpenAI API
calls. As an example, you might have many gigabytes of company
documentation that doesn’t change often so it makes sense to only
occasionally recreate the index.
# make sure you set the following environment variable is\
OPENAI_API_KEY
from llama_index import GPTVectorStoreIndex, SimpleDirect\
oryReader
from llama_index import StorageContext, load_index_from_s\
torage
documents = SimpleDirectoryReader('../data').load_data()
# index = GPTListIndex(documents) # llama_index < 0.5
index = GPTVectorStoreIndex.from_documents(documents)
engine = index.as_query_engine()
print(engine.query("what are key economic indicators?"))
# save to disk
index.storage_context.persist(persist_dir='./cache')
# load from disk
storage_context = StorageContext.from_defaults(persist_di\
r='./cache')
index = load_index_from_storage(storage_context)
engine = index.as_query_engine()
# search for a document
print(engine.query("effect of body chemistry on exercise?\
Overview of LlamaIndex
You may have noticed that the query defined on line 17 is the same
query that we used last chapter.
$ python test_from_docs.py
Key economic indicators are measures of economic activity\
that are used to assess the health of an economy. Exampl
es of key economic indicators include gross domestic prod
uct (GDP), unemployment rate, inflation rate, consumer pr
ice index (CPI), balance of trade, and industrial product
The effect of body chemistry on exercise depends on the t\
ype of exercise being performed. For aerobic exercise, th
e body needs to be able to produce enough energy to susta
in the activity. This requires the body to have adequate
levels of oxygen, glucose, and other nutrients in the blo
odstream. For anaerobic exercise, the body needs to be ab
le to produce energy without relying on oxygen. This requ
ires the body to have adequate levels of lactic acid, cre
atine phosphate, and other energy-producing molecules. In
both cases, the body's chemistry must be balanced in ord
er for the exercise to be effective.
4.2 Using LlamaIndex for Question
Answering from a List of Web Sites
In this example we use the trafilatura and html2text libraries to
get text from a web page that we will index and search. The class
TrafilaturaWebReader does the work of creating local documents
Overview of LlamaIndex
from a list of web page URIs and the index class GPTListIndex
builds a local index for use with OpenAI API calls to implement
search.
The following listing shows the file web_page_QA.py:
# Derived from the example at:
# https://github.com/jerryjliu/gpt_index/blob/main/exampl\
es/data_connectors/WebPageDemo.ipynb
# pip install llama-index, html2text, trafilatura
from llama_index import GPTListIndex
from llama_index import TrafilaturaWebReader
def query_website(url_list, *questions):
documents = TrafilaturaWebReader().load_data(url_list)
# index = GPTListIndex(documents) # llama_index < 0.5
index = GPTListIndex.from_documents(documents)
engine = index.as_query_engine()
for question in questions:
print(f"\n== QUESTION: {question}\n")
response = engine.query(question)
print(f"== RESPONSE: {response}")
if __name__ == "__main__":
url_list = ["https://markwatson.com"]
query_website(url_list, "How many patents does Mark hav\
"How many books has Mark writte\
This example is not efficient because we create a new index for
each web page we want to search. That said, this example (that
was derived from an example in the LlamaIndex documentation)
implements a pattern that you can use, for example, to build a
Overview of LlamaIndex
reusable index of your company’s web site and build an end-user
web search app.
The output for these three test questions in the last code example
$ python web_page_QA.py
documents:
[Document(text='Mark Watson’s Artificial Intelligence Boo\
ks and Blog\n'
'Author of 20+ books on AI and I have 50+ \
US patents. Here I '
'talk about both technology and provide li\
nks to read my '
'published books online.\n'
"By registering you agree to Substack's Te\
rms of Service, our "
'Privacy Policy, and our Information Colle\
ction Notice',
doc_id='c6ed3ddd-b160-472b-9c7d-fd0446762cc8',
embedding=None,
doc_hash='126a335ed76d5d79ad94470dd4dd06ab5556b\
9f1347e5be25ecc595e0290ab57',
extra_info=None)]
== QUESTION: How many patents does Mark have?
== RESPONSE:
Mark Watson has 50+ US patents.
== QUESTION: How many books has Mark written?
== RESPONSE:
Mark has written 20+ books on AI.
Overview of LlamaIndex
4.3 LlamaIndex/GPT-Index Case
Study Wrap Up
LlamaIndex is a set of data structures and library code designed
to make it easier to use large external knowledge bases such as
Wikipedia.
LlamaIndex creates a vectorized index from your
document data, making it highly efficient to query. It then uses
this index to identify the most relevant sections of the document
based on the query.
LlamaIndex is useful because it provides a central interface to
connect your LLM’s with external data and offers data connectors
to your existing data sources and data formats (API’s, PDF’s, docs,
SQL, etc.). It provides a simple, flexible interface between your
external data and LLMs.
Some projects that use LlamaIndex include building personal as-
sistants with LlamaIndex and GPT-3.5, using LlamaIndex for doc-
ument retrieval, and combining answers across documents.
5 Using Google’s
Knowledge Graph APIs
With LangChain
Google’s Knowledge Graph (KG) is a knowledge base that Google
uses to serve relevant information in an infobox beside its search
results. It allows the user to see the answer in a glance, as an
instant answer. The data is generated automatically from a variety
of sources, covering places, people, businesses, and more. I worked
at Google in 2013 on a project that used their KG for an internal
project.
Google’s public Knowledge Graph Search API lets you find en-
tities in the Google Knowledge Graph.
The API uses standard
schema.org types and is compliant with the JSON-LD specification.
It supports entity search and lookup.
You can use the Knowledge Graph Search API to build applications
that make use of Google’s Knowledge Graph. For example, you can
use the API to build a search engine that returns results based on
the entities in the Knowledge Graph.
In the next chapter we also use the public KGs DBPedia and
Wikidata. One limitation of Google’s KG APIs is that it is designed
for entity (people, places, organizations, etc.) lookup. When using
DBPedia and Wikidata it is possible to find a wider range of in-
formation using the SPARQL query language, such as relationships
between entities. You can use the Google KG APIs to find some
entity relationships, e.g., all the movies directed by a particular
director, or all the books written by a particular author. You can
also use the API to find information like all the people who have
worked on a particular movie, or all the actors who have appeared
Using Google’s Knowledge Graph APIs With LangChain
in a particular TV show.
5.1 Setting Up To Access Google
Knowledge Graph APIs
To get an API key for Google’s Knowledge Graph Search API,
you need to go to the Google API Console, enable the Google
Knowledge Graph Search API, and create an API key to use in
your project. You can then use this API key to make requests to
the Knowledge Graph Search API.
To create your application’s API key, follow these steps:
• Go to the API Console.
• From the projects list, select a project or create a new one.
• If the APIs & services page isn’t already open, open the left
side menu and select APIs & services.
• On the left, choose Credentials.
• Click Create credentials and then select API key.
You can then use this API key to make requests to the Knowledge
Graph Search APIs.
When I use Google’s APIs I set the access key in ~/.google_api_key
and read in the key using:
api_key=open(str(Path.home())+"/.google_api_key").read()
You can also use environment variables to store access keys. Here
is a code snippet for making an API call to get information about
Using Google’s Knowledge Graph APIs With LangChain
import json
from urllib.parse import urlencode
from urllib.request import urlopen
from pathlib import Path
from pprint import pprint
api_key =
open(str(Path.home()) + "/.google_api_key").read()
query = "Mark Louis Watson"
service_url =
"https://kgsearch.googleapis.com/v1/entities:search"
params = {
"query": query,
"limit": 10,
"indent": True,
"key": api_key,
url = service_url + "?" + urlencode(params)
response = json.loads(urlopen(url).read())
pprint(response)
The JSON-LD output would look like:
{'@context': {'@vocab': 'http://schema.org/',
'EntitySearchResult':
'goog:EntitySearchResult',
'detailedDescription':
'goog:detailedDescription',
'goog': 'http://schema.googleapis.com/',
'kg': 'http://g.co/kg',
'resultScore': 'goog:resultScore'},
'@type': 'ItemList',
'itemListElement': [{'@type': 'EntitySearchResult',
'result': {'@id': 'kg:/m/0b6_g82',
'@type': ['Thing',
Using Google’s Knowledge Graph APIs With LangChain
'Person'],
'description': 'Author',
'name':
'Mark Louis Watson',
'url':
'http://markwatson.com'},
'resultScore': 43}]}
In order to not repeat the code for getting entity information
from the Google KG, I wrote a utility Google_KG_helper.py that
encapsulates the previous code and generalizes it into a mini-
library:
"""Client for calling Knowledge Graph Search API."""
import json
from urllib.parse import urlencode
from urllib.request import urlopen
from pathlib import Path
from pprint import pprint
api_key =
open(str(Path.home()) + "/.google_api_key").read()
# use Google search API to get information
# about a named entity:
def get_entity_info(entity_name):
service_url =
"https://kgsearch.googleapis.com/v1/entities:search"
params = {
"query": entity_name,
"limit": 1,
"indent": True,
"key": api_key,
Using Google’s Knowledge Graph APIs With LangChain
url = service_url + "?" + urlencode(params)
response = json.loads(urlopen(url).read())
return response
def tree_traverse(a_dict):
ret = []
def recur(dict_2, a_list):
if isinstance(dict_2, dict):
for key, value in dict_2.items():
if key in ['name', 'description',
'articleBody']:
a_list += [value]
recur(value, a_list)
if isinstance(dict_2, list):
for x in dict_2:
recur(x, a_list)
recur(a_dict, ret)
return ret
def get_context_text(entity_name):
json_data = get_entity_info(entity_name)
return ' '.join(tree_traverse(json_data))
if __name__ == "__main__":
get_context_text("Bill Clinton")
The main test script is in the file Google_Knowledge_Graph_-
Search.py:
Using Google’s Knowledge Graph APIs With LangChain
"""Example of Python client calling the
Knowledge Graph Search API."""
from llama_index import GPTListIndex, Document
import Google_KG_helper
def kg_search(entity_name, *questions):
ret = ""
context_text =
Google_KG_helper.get_context_text(entity_name)
print(f"Context text: {context_text}")
doc = Document(context_text)
index = GPTListIndex([doc])
for question in questions:
response = index.query(question)
ret +=
f"QUESTION:
{question}\nRESPONSE: {response}\n"
return ret
if __name__ == "__main__":
s = kg_search("Bill Clinton",
"When was Bill president?")
print(s)
The example output is:
Using Google’s Knowledge Graph APIs With LangChain
$ python Google_Knowledge_Graph_Search.py
Context text: Bill Clinton 42nd U.S. President William Je\
fferson Clinton is an American retired politician who ser
ved as the 42nd president of the United States from 1993
to 2001.
INFO:root:> [build_index_from_documents] Total LLM token \
usage: 0 tokens
INFO:root:> [build_index_from_documents] Total embedding \
token usage: 0 tokens
INFO:root:> [query] Total LLM token usage: 77 tokens
INFO:root:> [query] Total embedding token usage: 0 tokens
QUESTION:
When was Bill president?
RESPONSE:
Bill Clinton was president from 1993 to 2001.
Accessing Knowledge Graphs from Google, DBPedia, and Wikidata
allows you to integrate real world facts and knowledge with your
applications. While I mostly work in the field of deep learning
I frequently also use Knowledge Graphs in my work and in my
personal research.
I think that you, dear reader, might find
accessing highly structured data in KGs to be more reliable and
in many cases simpler than using web scraping.
6 Using DBPedia and
WikiData as Knowledge
Sources
Both DBPedia1 and Wikidata2 are public Knowledge Graphs (KGs)
that store data as Resource Description Framework (RDF)3 and
are accessed through the SPARQL Query Language for RDF4. The
examples for this project are in the GitHub repository for this book5
in the directory kg_search.
I am not going to spend much time here discussing RDF and
SPARQL. Instead I ask you to read online the introductory chapter
Linked Data, the Semantic Web, and Knowledge Graphs in
my book A Lisp Programmer Living in Python-Land: The Hy
Programming Language6.
As we saw in the last chapter, a Knowledge Graph (that I often
abbreviate as KG) is a graph database using a schema to define types
(both objects and relationships between objects) and properties that
link property values to objects.
The term “Knowledge Graph”
is both a general term and also sometimes refers to the specific
Knowledge Graph used at Google which I worked with while
working there in 2013. Here, we use KG to reference the general
technology of storing knowledge in graph databases.
DBPedia and Wikidata are similar, with some important differ-
ences.
Here is a summary of some similarities and differences
1https://www.dbpedia.org
2https://www.wikidata.org/wiki/Wikidata:Main_Page
3https://www.w3.org/RDF/
4https://www.w3.org/TR/rdf-sparql-query/
5https://github.com/mark-watson/langchain-book-examples
6https://leanpub.com/hy-lisp-python/read
Using DBPedia and WikiData as Knowledge Sources
between DBPedia and Wikidata:
• Both projects aim to provide structured data from Wikipedia
in various formats and languages. Wikidata also has data
from other sources so it contains more data and more lan-
guages.
• Both projects use RDF as a common data model and SPARQL
as a query language.
• DBPedia extracts data from the infoboxes in Wikipedia arti-
cles, while Wikidata collects data entered through its inter-
faces by both users and automated bots.
• Wikidata requires sources for its data, while DBPedia does
• DBpedia is more popular in the Semantic Web and Linked
Open Data communities, while Wikidata is more integrated
with Wikimedia projects.
To the last point: I personally prefer DBPedia when experimenting
with the semantic web and linked data, mostly because DBPedia
URIs are human readable while Wikidata URIs are abstract. The
following URIs represent the town I live in, Sedona Arizona:
• DBPedia: https://dbpedia.org/page/Sedona,_Arizona
• Wikidata: https://www.wikidata.org/wiki/Q80041
enclose
brackets
<https://www.wikidata.org/wiki/Q80041>.
If you read the chapter on RDF and SPARQL in my book link that I
mentioned previously, then you know that RDF data is represented
by triples where each part is named:
• subject
• property
Using DBPedia and WikiData as Knowledge Sources
• object
We will look at two similar examples in this chapter, one using
DBPedia and one using Wikidata. Both services have SPARQL
endpoint web applications that you will want to use for exploring
both KGs. We will look at the DBPedia web interface later. Here is
the Wikidata web interface:
In this SPARQL query the prefix wd: stands for Wikidata data
while the prefix wdt: stands for Wikidata type (or property). The
prefix rdfs: stands for RDF Schema.
6.1 Using DBPedia as a Data Source
DBpedia is a community-driven project that extracts structured
content from Wikipedia and makes it available on the web as
a Knowledge Graph (KG). The KG is a valuable resource for
researchers and developers who need to access structured data
from Wikipedia.
With the use of SPARQL queries to DBpedia
Using DBPedia and WikiData as Knowledge Sources
as a data source we can write a variety applications, including
natural language processing, machine learning, and data analytics.
We demonstrate the effectiveness of DBpedia as a data source by
presenting several examples that illustrate its use in real-world
applications. In my experience, DBpedia is a valuable resource for
researchers and developers who need to access structured data from
Wikipedia.
In general you will start projects using DBPedia by exploring
available data using the web app https://dbpedia.org/sparql that can
be seen in this screen shot:
The following listing of file dbpedia_generate_rdf_as_nt.py
shows Python code for making a SPARQL query to DBPedia and
saving the results as RDF triples in NT format in a local text file:
Using DBPedia and WikiData as Knowledge Sources
from SPARQLWrapper import SPARQLWrapper
from rdflib import Graph
sparql = SPARQLWrapper("http://dbpedia.org/sparql")
sparql.setQuery("""
PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>
PREFIX dbpedia: <http://dbpedia.org/resource>
PREFIX dbpprop: <http://dbpedia.org/property>
CONSTRUCT {
?city dbpedia-owl:country ?country .
?city rdfs:label ?citylabel .
?country rdfs:label ?countrylabel .
<http://dbpedia.org/ontology/country> rdfs:label \
"country"@en .
WHERE {
?city rdf:type dbpedia-owl:City .
?city rdfs:label ?citylabel .
?city dbpedia-owl:country ?country .
?country rdfs:label ?countrylabel .
FILTER (lang(?citylabel) = 'en')
FILTER (lang(?countrylabel) = 'en')
LIMIT 50
sparql.setReturnFormat("rdf")
results = sparql.query().convert()
g = Graph()
g.parse(data=results.serialize(format="xml"), format="xml\
print("\nresults:\n")
results = g.serialize(format="nt").encode("utf-8").decode\
Using DBPedia and WikiData as Knowledge Sources
('utf-8')
print(results)
text_file = open("sample.nt", "w")
text_file.write(results)
text_file.close()
Here is the printed output from running this script (most output not
shown, and manually edited to fit page width):
$ python generate_rdf_as_nt.py
results:
<http://dbpedia.org/resource/Ethiopia>
<http://www.w3.org/2000/01/rdf-schema#label>
"Ethiopia"@en .
<http://dbpedia.org/resource/Valentin_Alsina,_Buenos_Aire\
<http://www.w3.org/2000/01/rdf-schema#label>
"Valentin Alsina, Buenos Aires"@en .
<http://dbpedia.org/resource/Davyd-Haradok>
<http://dbpedia.org/ontology/country>
<http://dbpedia.org/resource/Belarus> .
<http://dbpedia.org/resource/Davyd-Haradok>
<http://www.w3.org/2000/01/rdf-schema#label>
"Davyd-Haradok"@en .
<http://dbpedia.org/resource/Belarus>
<http://www.w3.org/2000/01/rdf-schema#label>
"Belarus"@en .
This output was written to a local file sample.nt. I divided this
example into two separate Python scripts because I thought it
would be easier for you, dear reader, to experiment with fetching
RDF data separately from using a LLM to process the RDF data. In
Using DBPedia and WikiData as Knowledge Sources
production you may want to combine KG queries with semantic
analysis.
This code example demonstrates the use of the GPTSimpleVec-
torIndex for querying RDF data and retrieving information about
countries. The function download_loader loads data importers by
string name. While it is not a type safe to load a Python class by
name using a string, if you misspell the name of the class to load the
call to download_loader then a Python ValueError(“Loader class
name not found in library”) error is thrown. The GPTSimpleVec-
torIndex class represents an index data structure that can be used
to efficiently search and retrieve information from the RDF data.
This is similar to other types of LlamaIndex vector index types for
different types of data sources.
Here is the script dbpedia_rdf_query.py:
"Example from documentation"
from llama_index import GPTSimpleVectorIndex, Document
from llama_index import download_loader
RDFReader = download_loader("RDFReader")
doc = RDFReader().load_data("sample.nt")
index = GPTSimpleVectorIndex(doc)
result = index.query("list all countries in a quoted Pyth\
on array, then explain why")
print(result.response)
Here is the output:
Using DBPedia and WikiData as Knowledge Sources
$ python rdf_query.py
INFO:root:> [build_index_from_documents] Total LLM token \
usage: 0 tokens
INFO:root:> [build_index_from_documents] Total embedding \
token usage: 761 tokens
INFO:root:> [query] Total LLM token usage: 921 tokens
INFO:root:> [query] Total embedding token usage: 12 tokens
['Argentina', 'French Polynesia', 'Democratic Republic of\
the Congo', 'Benin', 'Ethiopia', 'Australia', 'Uzbekista
n', 'Tanzania', 'Albania', 'Belarus', 'Vanuatu', 'Armenia
', 'Syria', 'Andorra', 'Venezuela', 'France', 'Vietnam',
'Azerbaijan']
This is a list of all the countries mentioned in the cont\
ext information. All of the countries are listed in the c
ontext information, so this list is complete.
Why are there only 18 countries listed? In the script used to perform
a SPARQL query on DBPedia, we had a statement LIMIT 50 at the
end of the query so only 50 RDF triples were written to the file
sample.nt that only contains data for 18 countries.
6.2 Using Wikidata as a Data Source
It is slightly more difficult exploring Wikidata compared to DB-
Pedia. Let’s revisit getting information about my home town of
Sedona Arizona.
In writing this example, I experimented with SPARQL queries using
the Wikidata SPARQL web app7.
We can start by finding RDF statements with the object value being
“Sedona” using the Wikidata web app:
7https://query.wikidata.org
Using DBPedia and WikiData as Knowledge Sources
select * where {
?s ?p "Sedona"@en
} LIMIT 30
First we write a helper utility to gather prompt text for an entity
name (e.g., name of a person, place, etc.) in the file wikidata_-
generate_prompt_text.py:
from SPARQLWrapper import SPARQLWrapper, JSON
from rdflib import Graph
import pandas as pd
def get_possible_entity_uris_from_wikidata(entity_name):
sparql = SPARQLWrapper("https://query.wikidata.org/spa\
sparql.setQuery("""
SELECT ?entity ?entityLabel WHERE {
?entity rdfs:label "%s"@en .
} limit 5
""" % entity_name)
sparql.setReturnFormat(JSON)
results = sparql.query().convert()
results = pd.json_normalize(results['results']['bindin\
gs']).values.tolist()
results = ["<" + x[1] + ">" for x in results]
return [*set(results)] # remove duplicates
def wikidata_query_to_df(entity_uri):
sparql = SPARQLWrapper("https://query.wikidata.org/spa\
sparql.setQuery("""
SELECT ?description ?is_a_type_of WHERE {
%s schema:description ?description FILTER (lang(?\
Using DBPedia and WikiData as Knowledge Sources
description) = 'en') .
%s wdt:P31 ?instanceOf .
?instanceOf rdfs:label ?is_a_type_of FILTER (lang\
(?is_a_type_of) = 'en') .
} limit 10
""" % (entity_uri, entity_uri))
sparql.setReturnFormat(JSON)
results = sparql.query().convert()
results2 = pd.json_normalize(results['results']['bindi\
ngs'])
prompt_text = ""
for index, row in results2.iterrows():
prompt_text += row['description.value'] + " is a \
type of " + row['is_a_type_of.value'] + "\n"
return prompt_text
def generate_prompt_text(entity_name):
entity_uris = get_possible_entity_uris_from_wikidata(e\
ntity_name)
prompt_text = ""
for entity_uri in entity_uris:
p = wikidata_query_to_df(entity_uri)
if "disambiguation page" not in p:
prompt_text += entity_name + " is " + wikidata\
_query_to_df(entity_uri)
return prompt_text
if __name__ == "__main__":
print("Sedona:", generate_prompt_text("Sedona"))
print("California:",
generate_prompt_text("California"))
print("Bill Clinton:",
generate_prompt_text("Bill Clinton"))
print("Donald Trump:",
Using DBPedia and WikiData as Knowledge Sources
generate_prompt_text("Donald Trump"))
This utility does most of the work in getting prompt text for an
entity.
The GPTTreeIndex class is similar to other LlamaIndex index
classes. This class builds a tree-based index of the prompt texts,
which can be used to retrieve information based on the input
question. In LlamaIndex, a GPTTreeIndex is used to select the
child node(s) to send the query down to. A GPTKeywordTableIn-
dex uses keyword matching, and a GPTVectorStoreIndex uses
embedding cosine similarity. The choice of which index class to use
depends on how much text is being indexed, what the granularity
of subject matter in the text is, and if you want summarization.
GPTTreeIndex is also more efficient than GPTSimpleVectorIndex
because it uses a tree structure to store the data. This allows for
faster searching and retrieval of data compared to a linear list index
class like GPTSimpleVectorIndex.
The LlamaIndex code is relatively easy to implement in the script
wikidata_query.py (edited to fit page width):
from llama_index import StringIterableReader, GPTTreeIndex
from wikidata_generate_prompt_text import generate_prompt\
def wd_query(question, *entity_names):
prompt_texts = []
for entity_name in entity_names:
prompt_texts +=
[generate_prompt_text(entity_name)]
documents =
StringIterableReader().load_data(texts=prompt_texts)
index = GPTTreeIndex(documents)
return index.query(question)
Using DBPedia and WikiData as Knowledge Sources
if __name__ == "__main__":
print("Sedona:", wd_query("What is Sedona?", "Sedona"))
print("California:",
wd_query("What is California?", "California"))
print("Bill Clinton:",
wd_query("When was Bill Clinton president?",
"Bill Clinton"))
print("Donald Trump:",
wd_query("Who is Donald Trump?",
"Donald Trump"))
Here is the test output (with some lines removed):
$ python wikidata_query.py
Total LLM token usage: 162 tokens
INFO:llama_index.token_counter.token_counter:> [build_ind\
ex_from_documents] INFO:llama_index.indices.query.tree.le
af_query:> Starting query: What is Sedona?
INFO:llama_index.token_counter.token_counter:> [query] To\
tal LLM token usage: 154 tokens
Sedona: Sedona is a city in the United States located in \
the counties of Yavapai and Coconino, Arizona. It is also
the title of a 2012 film, a company, and a 2015 single b
y Houndmouth.
Total LLM token usage: 191 tokens
INFO:llama_index.indices.query.tree.leaf_query:> Starting\
query: What is California?
California: California is a U.S. state in the United Stat\
es of America.
Total LLM token usage: 138 tokens
INFO:llama_index.indices.query.tree.leaf_query:> Starting\
query: When was Bill Clinton president?
Bill Clinton: Bill Clinton was the 42nd President of the \
Using DBPedia and WikiData as Knowledge Sources
United States from 1993 to 2001.
Total LLM token usage: 159 tokens
INFO:llama_index.indices.query.tree.leaf_query:> Starting\
query: Who is Donald Trump?
Donald Trump: Donald Trump is the 45th President of the U\
nited States, serving from 2017 to 2021.
7 Using LLMs To Organize
Information in Our
Google Drives
My digital life consists of writing, working as an AI practitioner,
and learning activities that I justify with my self-image of a
“gentleman scientist.” Cloud storage like GitHub, Google Drive,
Microsoft OneDrive, and iCloud are central to my activities.
About ten years ago I spent two months of my time writing a sys-
tem in Clojure that was planned to be my own custom and personal
DropBox, augmented with various NLP tools and a FireFox plugin
to send web clippings directly to my personal system. To be honest,
I stopped using my own project after a few months because the time
it took to organize my information was a greater opportunity cost
than the value I received.
In this chapter I am going to walk you through parts of a new
system that I am developing for my own personal use to help
me organize my material on Google Drive (and eventually other
cloud services). Don’t be surprised if the completed project is an
additional example in a future edition of this book!
With the Google setup directions listed below, you will get a pop-up
web browsing window with a warning like (this shows my Gmail
address, you should see your own Gmail address here assuming
that you have recently logged into Gmail using your default web
browser):
Using LLMs To Organize Information in Our Google Drives
You will need to first click Advanced and then click link Go to
GoogleAPIExamples (unsafe) link in the lower left corner and
then temporarily authorize this example on your Gmail account.
7.1 Setting Up Requirements.
You need to create a credential at https://console.cloud.google.
com/cloud-resource-manager (copied from the PyDrive documen-
tation1, changing application type to “Desktop”):
• Search for ‘Google Drive API’, select the entry, and click
‘Enable’.
• Select ‘Credentials’ from the left menu, click ‘Create Creden-
tials’, select ‘OAuth client ID’.
1https://pythonhosted.org/PyDrive/quickstart.html
Using LLMs To Organize Information in Our Google Drives
• Now, the product name and consent screen need to be set ->
click ‘Configure consent screen’ and follow the instructions.
Once finished:
• Select ‘Application type’ to be Desktop application.
• Enter an appropriate name.
• Input http://localhost:8080 for ‘Authorized JavaScript origins’.
• Input http://localhost:8080/ for ‘Authorized redirect URIs’.
• Click ‘Save’.
• Click ‘Download JSON’ on the right side of Client ID to
download client_secret_.json. Copy the downloaded JSON
credential file to the example directory google_drive_llm for
this chapter.
7.2 Write Utility To Fetch All Text
Files From Top Level Google Drive
Folder
For this example we will just authenticate our test script with
Google, and copy all top level text files with names ending with
“.txt” to the local file system in subdirectory data. The code is in
the directory google_drive_llm in file fetch_txt_files.py (edited
to fit page width):
Using LLMs To Organize Information in Our Google Drives
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from pathlib import Path
# good GD search docs:
# https://developers.google.com/drive/api/guides/search-f\
# Authenticate with Google
gauth = GoogleAuth()
gauth.LocalWebserverAuth()
drive = GoogleDrive(gauth)
def get_txt_files(dir_id='root'):
" get all plain text files with .txt extension in top\
level Google Drive directory "
file_list = drive.ListFile({'q': f"'{dir_id}' in pare\
nts and trashed=false"}).GetList()
for file1 in file_list:
print('title: %s, id: %s' % (file1['title'], file\
1['id']))
return [[file1['title'], file1['id'], file1.GetConten\
tString()]
for file1 in file_list
if file1['title'].endswith(".txt")]
def create_test_file():
" not currently used, but useful for testing. "
# Create GoogleDriveFile instance with title 'Hello.t\
file1 = drive.CreateFile({'title': 'Hello.txt'})
file1.SetContentString('Hello World!')
file1.Upload()
Using LLMs To Organize Information in Our Google Drives
def test():
fl = get_txt_files()
for f in fl:
print(f)
file1 = open("data/" + f[0],"w")
file1.write(f[2])
file1.close()
if __name__ == '__main__':
test()
For testing I just have one text file with the file extension “.txt” on
my Google Drive so my output from running this script looks like
the following listing. I edited the output to change my file IDs and
to only print a few lines of the debug printout of file titles.
$ python fetch_txt_files.py
Your browser has been opened to visit:
https://accounts.google.com/o/oauth2/auth?client_id=5\
29311921932-xsmj3hhiplr0dhqjln13fo4rrtvoslo8.apps.googleu
sercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3B6180
%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive
&access_type=offline&response_type=code
Authentication successful.
title: testdata, id: 1TZ9bnL5XYQvKACJw8VoKWdVJ8jeCszJ
title: sports.txt, id: 18RN4ojvURWt5yoKNtDdAJbh4fvmRpzwb
title: Anaconda blog article, id: 1kpLaYQA4Ao8ZbdFaXU209h\
g-z0tv1xA7YOQ4L8y8NbU
title: backups_2023, id: 1-k_r1HTfuZRWN7vwWWsYqfssl0C96J2x
title: Work notes, id: 1fDyHyZtKI-0oRNabA_P41LltYjGoek21
title: Sedona Writing Group Contact List, id: 1zK-5v9OQUf\
Using LLMs To Organize Information in Our Google Drives
y8Sw33nTCl9vnL822hL1w
['sports.txt', '18RN4ojvURWt5yoKNtDdAJbh4fvmRpzwb', 'Spor\
t is generally recognised as activities based in physical
athleticism or physical dexterity.[3] Sports are usually
governed by rules to ensure fair competition and consist
ent adjudication of the winner.\n\n"Sport" comes from the
Old French desport meaning "leisure", with the oldest de
finition in English from around 1300 being "anything huma
ns find amusing or entertaining".[4]\n\nOther bodies advo
cate widening the definition of sport to include all phys
ical activity and exercise. For instance, the Council of
Europe include all forms of physical exercise, including
those completed just for fun.\n\n']
7.3 Generate Vector Indices for Files
in Specific Google Drive Directories
The example script in the last section should have created copies
of the text files in you home Google Documents directory that end
with “.txt”. Here, we use the same LlamaIndex test code that we
used in a previous chapter. The test script index_and_QA.py is
listed here:
Using LLMs To Organize Information in Our Google Drives
# make sure you set the following environment variable is\
OPENAI_API_KEY
from llama_index import GPTSimpleVectorIndex, SimpleDirec\
toryReader
documents = SimpleDirectoryReader('data').load_data()
index = GPTSimpleVectorIndex(documents)
# save to disk
index.save_to_disk('index.json')
# load from disk
index = GPTSimpleVectorIndex.load_from_disk('index.json')
# search for a document
print(index.query("What is the definition of sport?"))
For my test file, the output looks like:
$ python index_and_QA.py
INFO:llama_index.token_counter.token_counter:> [build_ind\
ex_from_documents] Total LLM token usage: 0 tokens
INFO:llama_index.token_counter.token_counter:> [build_ind\
ex_from_documents] Total embedding token usage: 111 token
INFO:llama_index.token_counter.token_counter:> [query] To\
tal LLM token usage: 202 tokens
INFO:llama_index.token_counter.token_counter:> [query] To\
tal embedding token usage: 7 tokens
Sport is generally recognised as activities based in phys\
ical athleticism or physical dexterity that are governed
by rules to ensure fair competition and consistent adjudi
cation of the winner. It is anything humans find amusing
or entertaining, and can include all forms of physical ex
ercise, even those completed just for fun.
Using LLMs To Organize Information in Our Google Drives
It is interesting to see how the query result is rewritten in a nice
form, compared to the raw text in the file sports.txt on my Google
Drive:
$ cat data/sports.txt
Sport is generally recognised as activities based in phys\
ical athleticism or physical dexterity.[3] Sports are usu
ally governed by rules to ensure fair competition and con
sistent adjudication of the winner.
"Sport" comes from the Old French desport meaning "leisur\
e", with the oldest definition in English from around 130
0 being "anything humans find amusing or entertaining".[4
Other bodies advocate widening the definition of sport to\
include all physical activity and exercise. For instance
, the Council of Europe include all forms of physical exe
rcise, including those completed just for fun.
7.4 Google Drive Example Wrap Up
If you already use Google Drive to store your working notes
and other documents, then you might want to expand the simple
example in this chapter to build your own query system for your
documents. In addition to Google Drive, I also use Microsoft Office
365 and OneDrive in my work and personal projects.
I haven’t written my own connectors yet for OneDrive but this is
on my personal to-do list using the Microsoft library https://github.
com/OneDrive/onedrive-sdk-python.
8 Using Zapier
Integrations With GMail
and Google Calendar
Zapier is a service for writing integrations with hundreds of cloud
services. Here we will write some demos for writing automatic
integrations with GMail and Google Calendar.
Using the Zapier service is simple. You need to register the services
you want to interact with on the Zapier developer web site and
then you can express how you want to interact with services using
natural language prompts.
8.1 Set Up Development
Environment
You will need a developer key for Zapier Natural Language Actions
API1. Go to this linked web page and look for “Dev App” in the
“Provider Name” column. If a key does not exist, you’ll need to set
up an action to create a key. Click “Set up Actions” and follow the
instructions. Your key will be in the Personal API Key column for
the “Dev App.” Click to reveal and copy your key. You can read the
documentation2.
When I set up my Zapier account I set up three Zapier Natural
Language Actions:
• Gmail: Find Email
1https://nla.zapier.com/get-started/
2https://nla.zapier.com/api/v1/dynamic/docs
Using Zapier Integrations With GMail and Google Calendar
• Gmail: Send Email
• Google Calendar: Find Event
If you do the same then you will see the Zapier registered actions:
8.2 Sending a Test GMail
In the following example replace TEST_EMAIL_ADDRESS with
an email address that you can use for testing.
Using Zapier Integrations With GMail and Google Calendar
from langchain.llms import OpenAI
from langchain.agents import initialize_agent
from langchain.agents.agent_toolkits import ZapierToolkit
from langchain.utilities.zapier import ZapierNLAWrapper
llm = OpenAI(temperature=0)
zapier = ZapierNLAWrapper()
toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)
agent = initialize_agent(toolkit.get_tools(), llm, agent=\
"zero-shot-react-description", verbose=True)
agent.run("Send an Email to TEST_EMAIL_ADDRESS via gmail \
that is a pitch for hiring Mark Watson as a consultant fo
r deep learning and large language models")
Here is the sample output:
$ python send_gmail.py
> Entering new AgentExecutor chain...
I need to use the Gmail: Send Email tool
Action: Gmail: Send Email
Action Input: Send an email to TEST_EMAIL_ADDRESS with th\
e subject "Pitch for Hiring Mark Watson as a Consultant f
or Deep Learning and Large Language Models" and the body
"Dear Mark Watson, I am writing to you to pitch the idea
of hiring you as a consultant for deep learning and large
language models. I believe you have the expertise and ex
perience to help us achieve our goals. Please let me know
if you are interested in discussing further. Thank you f
or your time."
Cc: not enough information provided in the instruction, m\
issing Cc
Observation: {"labelIds": "SENT"}
Using Zapier Integrations With GMail and Google Calendar
Thought: I now know the final answer
Final Answer: An email has been sent to TEST_EMAIL_ADDRES\
S with the subject "Pitch for Hiring Mark Watson as a Con
sultant for Deep Learning and Large Language Models" and
the body "Dear Mark Watson, I am writing to you to pitch
the idea of hiring you as a consultant for deep learning
and large language models. I believe you have the experti
se and experience to help us achieve our goals. Please le
t me know if you are interested in discussing further. Th
ank you for your time."
> Finished chain.
8.3 Google Calendar Integration
Example
Assuming that you configured the Zapier Natural Language Action
“Google Calendar: Find Event” then the same code we used to send
an email in the last section works for checking calendar entries, you
just need to change the natural language prompt:
from langchain.llms import OpenAI
from langchain.agents import initialize_agent
from langchain.agents.agent_toolkits import ZapierToolkit
from langchain.utilities.zapier import ZapierNLAWrapper
llm = OpenAI(temperature=0)
zapier = ZapierNLAWrapper()
toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)
agent = initialize_agent(toolkit.get_tools(), llm,
agent="zero-shot-react-descripti\
on", verbose=True)
Using Zapier Integrations With GMail and Google Calendar
agent.run("Get my Google Calendar entries for tomorrow")
And the output looks like:
$ python get_google_calendar.py
> Entering new AgentExecutor chain...
I need to find events in my Google Calendar
Action: Google Calendar: Find Event
Action Input: Find events in my Google Calendar tomorrow
Observation: {"location": "Greg to call Mark on (928) XXX\
-ZZZZ", "kind": "calendar#event", "end__dateTime": "2023-
03-23T10:00:00-07:00", "status": "confirmed", "end__dateT
ime_pretty": "Mar 23, 2023 10:00AM", "htmlLink": "https:/
/zpr.io/WWWWWWWW"}
Thought: I now know the final answer
Final Answer: I have an event in my Google Calendar tomor\
row at 10:00AM.
> Finished chain.
I edited this output to remove some private information.
9 Natural Language
SQLite Database Queries
With LangChain
The LangChain library support fof SQLite databases uses the
Python library SQLAlchemy for database connections.
abstraction layer allows LangChain to use the same logic and
models for other relational databases.
I have a long work history of writing natural language interfaces
for relational databases that I will review in the chapter wrap up.
For now, I invite you to be amazed at how simple it is to write the
LangChain scripts for querying a database in natural language.
We will use the SQlite sample database from the SQLite Tutorial
web site:
https://www.sqlitetutorial.net/sqlite-sample-database/
This database has 11 tables. The above URI has documentation for
this database so please take a minute to review the table schema
diagram and text description1.
This example is derived from the LangChain documentation2. We
use three classes from the langchain library:
• OpenAI: A class that represents the OpenAI language model,
which is capable of understanding natural language and
generating a response.
1https://www.sqlitetutorial.net/sqlite-sample-database/
2https://langchain.readthedocs.io/en/latest/modules/chains/examples/sqlite.html
Natural Language SQLite Database Queries With LangChain
• SQLDatabase: A class that represents a connection to an SQL
database.
• SQLDatabaseChain: A class that connects the OpenAI lan-
guage model with the SQL database to allow natural language
querying.
The temperature parameter set to 0 in this example. The temper-
ature parameter controls the randomness of the generated output.
A lower value (like 0) makes the model’s output more deterministic
and focused, while a higher value introduces more randomness (or
“creativity”). The run method of the db_chain object translates the
natural language query into an appropriate SQL query, execute it
on the connected database, and then returns the result converting
the output into natural language.
# SQLite NLP Query Demo Script
from langchain import OpenAI, SQLDatabase
from langchain import SQLDatabaseChain
db = SQLDatabase.from_uri("sqlite:///chinook.db")
llm = OpenAI(temperature=0)
db_chain = SQLDatabaseChain(llm=llm, database=db,
verbose=True)
db_chain.run("How many employees are there?")
db_chain.run("What is the name of the first employee?")
db_chain.run("Which customer has the most invoices?")
db_chain.run("List all music genres in the database")
The output (edited for brevity) shows the generated SQL queries
and the query results:
Natural Language SQLite Database Queries With LangChain
$ python sqlite_chat_test.py
> Entering new SQLDatabaseChain chain...
How many employees are there?
SELECT COUNT(*) FROM employees;
SQLResult: [(8,)]
Answer: There are 8 employees.
> Finished chain.
> Entering new SQLDatabaseChain chain...
What is the name of the first employee?
SELECT FirstName, LastName FROM employees WHERE Employee\
Id = 1;
SQLResult: [('Andrew', 'Adams')]
Answer: The first employee is Andrew Adams.
> Finished chain.
> Entering new SQLDatabaseChain chain...
Which customer has the most invoices?
SELECT customers.FirstName, customers.LastName, COUNT(in\
voices.InvoiceId) AS NumberOfInvoices FROM customers INNE
R JOIN invoices ON customers.CustomerId = invoices.Custom
erId GROUP BY customers.CustomerId ORDER BY NumberOfInvoi
ces DESC LIMIT 5;
SQLResult: [('Luis', 'Goncalves', 7), ('Leonie', 'Kohler'\
, 7), ('Francois', 'Tremblay', 7), ('Bjorn', 'Hansen', 7)
, ('Frantisek', 'Wichterlova', 7)]
Answer: Luis Goncalves has the most invoices with 7.
> Finished chain.
> Entering new SQLDatabaseChain chain...
List all music genres in the database
SQLQuery: SELECT Name FROM genres
SQLResult: [('Rock',), ('Jazz',), ('Metal',), ('Alternati\
ve & Punk',), ('Rock And Roll',), ('Blues',), ('Latin',),
Natural Language SQLite Database Queries With LangChain
('Reggae',), ('Pop',), ('Soundtrack',), ('Bossa Nova',),
('Easy Listening',), ('Heavy Metal',), ('R&B/Soul',), ('
Electronica/Dance',), ('World',), ('Hip Hop/Rap',), ('Sci
ence Fiction',), ('TV Shows',), ('Sci Fi & Fantasy',), ('
Drama',), ('Comedy',), ('Alternative',), ('Classical',),
('Opera',)]
Answer: Rock, Jazz, Metal, Alternative & Punk, Rock And R\
oll, Blues, Latin, Reggae, Pop, Soundtrack, Bossa Nova, E
asy Listening, Heavy Metal, R&B/Soul, Electronica/Dance,
World, Hip Hop/Rap, Science Fiction, TV Shows, Sci Fi & F
antasy, Drama, Comedy, Alternative, Classical, Opera
> Finished chain.
9.1 Natural Language Database
Query Wrap Up
I had an example I wrote for the first two editions of my Java
AI book3 (I later removed this example because the code was too
long and too difficult to follow). I later reworked this example in
Common Lisp and used both versions in several consulting projects
in the late 1990s and early 2000s.
The last book I wrote Practical Python Artificial Intelligence
Programming4 used an OpenAI example https://github.com/
openai/openai-cookbook/blob/main/examples/Backtranslation_
of_SQL_queries.py that shows relatively simple code (relative to
my older hand-written Java and Common Lisp code) for a NLP
database interface.
Compared to the elegant support for NLP database queries in
LangChain, the previous examples have limited power and required
a lot more code. As I write this in March 2023, it is a good feeling
3https://leanpub.com/javaai
4https://leanpub.com/pythonai
Natural Language SQLite Database Queries With LangChain
that for the rest of my career, NLP database access is now a solved
problem!
10 Examples Using
Hugging Face Open
Source Models
To start with you will need to create a free account on the Hugging
Face Hub1 and get an API key and install:
pip install --upgrade huggingface_hub
You need to set the foillowing environment variable to your Hug-
ging Face Hub access token:
HUGGINGFACEHUB_API_TOKEN
So far in this book we have been using the OpenAI LLM wrapper:
from langchain.llms import OpenAI
Here we will use the alternative Hugging Face wrapper class:
from langchain import HuggingFaceHub
The LangChain library hides most of the details of using both APIs.
This is a really good thing. I have had a few discussions on social
tech media with people who object to the non open source nature
of OpenAI. While I like the convenience of using OpenAI’s APIs, I
always like to have alternatives for proprietary technology I use.
1https://huggingface.co/docs/huggingface_hub/index
Examples Using Hugging Face Open Source Models
The Hugging Face Hub endpoint in LangChain connects to the
Hugging Face Hub and runs the models via their free inference
endpoints. We need a Hugging Face account and API key to use
these endpoints3. There exists two Hugging Face LLM wrappers,
one for a local pipeline and one for a model hosted on Hugging
Face Hub. Note that these wrappers only work for models that sup-
port the text2text-generation and text-generation tasks. Text2text-
generation refers to the task of generating a text sequence from
another text sequence. For example, generating a summary of a
long article. Text-generation refers to the task of generating a text
sequence from scratch.
10.1 Using LangChain as a Wrapper
for Hugging Face Prediction Model
We will start with a simple example using the prompt text support
in LangChain. The following example is in the script simple_ex-
ample.py:
from langchain import HuggingFaceHub, LLMChain
from langchain.prompts import PromptTemplate
hub_llm = HuggingFaceHub(
repo_id='google/flan-t5-xl',
model_kwargs={'temperature':1e-6}
prompt = PromptTemplate(
input_variables=["name"],
template="What year did {name} get elected as preside\
Examples Using Hugging Face Open Source Models
llm_chain = LLMChain(prompt=prompt, llm=hub_llm)
print(llm_chain.run("George Bush"))
By changing just a few lines of code, you can run many of the
examples in this book using the Hugging Face APIs in place of the
OpenAI APIs.
The LangChain documentation lists the source code for a wrapper
to use local Hugging Face embeddings here2.
10.2 Creating a Custom LlamaIndex
Hugging Face LLM Wrapper Class
That Runs on Your Laptop
We will be downloading the Hugging Face model facebook/opt-
iml-1.3b that is a 2.6 gigabyte file.
This model is downloaded
requested
cached
~/.cache/huggingface/hub for later reuse.
This example is modified from an example for custom LLMs in the
LlamaIndex documentation3. Note that I have used a much smaller
model in this example and reduced the prompt and output text size.
2https://langchain.readthedocs.io/en/latest/_modules/langchain/embeddings/self_hosted_
hugging_face.html
3https://github.com/jerryjliu/llama_index/blob/main/docs/how_to/customization/
custom_llms.md#example-using-a-custom-llm-model
Examples Using Hugging Face Open Source Models
# Derived from example:
https://gpt-index.readthedocs.io/en/latest/how_to/cus\
tom_llms.html
import time
import torch
from langchain.llms.base import LLM
from llama_index import SimpleDirectoryReader, LangchainE\
mbedding
from llama_index import GPTListIndex, PromptHelper
from llama_index import LLMPredictor
from transformers import pipeline
max_input_size = 512
num_output = 64
max_chunk_overlap = 10
prompt_helper = PromptHelper(max_input_size, num_output, \
max_chunk_overlap)
class CustomLLM(LLM):
model_name = "facebook/opt-iml-1.3b"
# I am not using a GPU, but you can add device="cuda:\
# to the pipeline call if you have a local GPU or
# are running this on Google Colab:
pipeline = pipeline("text-generation", model=model_na\
model_kwargs={"torch_dtype":torch\
.bfloat16})
def _call(self, prompt, stop = None):
prompt_length = len(prompt)
response = self.pipeline(prompt, max_new_tokens=n\
um_output)
first_response = response[0]["generated_text"]
Examples Using Hugging Face Open Source Models
# only return newly generated tokens
returned_text = first_response[prompt_length:]
return returned_text
@property
def _identifying_params(self):
return {"name_of_model": self.model_name}
@property
def _llm_type(self):
return "custom"
time1 = time.time()
# define our LLM
llm_predictor = LLMPredictor(llm=CustomLLM())
# Load the your data
documents = SimpleDirectoryReader('../data_small').load_d\
index = GPTListIndex(documents, llm_predictor=llm_predict\
prompt_helper=prompt_helper)
time2 = time.time()
print(f"Time to load model from disk: {time2 - time1} sec\
onds.")
# Query and print response
response = index.query("What is the definition of sport?")
print(response)
time3 = time.time()
print(f"Time for query/prediction: {time3 - time2} second\
Examples Using Hugging Face Open Source Models
When running on my M1 MacBook Pro using only the CPU (no
GPU or Neural Engine configuration) we can read the model from
disk quickly but it takes a while to process queries:
$ python hf_transformer_local.py
INFO:llama_index.token_counter.token_counter:> [build_ind\
ex_from_documents] Total LLM token usage: 0 tokens
INFO:llama_index.token_counter.token_counter:> [build_ind\
ex_from_documents] Total embedding token usage: 0 tokens
Time to load model from disk: 1.5303528308868408 seconds.
INFO:llama_index.token_counter.token_counter:> [query] To\
tal LLM token usage: 182 tokens
INFO:llama_index.token_counter.token_counter:> [query] To\
tal embedding token usage: 0 tokens
"Sport" comes from the Old French desport meaning "leisur\
e", with the oldest definition in English from around 130
0 being "anything humans find amusing or entertaining".[4
Time for query/prediction: 228.8184850215912 seconds.
Even though my M1 MacBook does fairly well when I configure
TensorFlow and PyTorch to use the Apple Silicon GPUs and Neural
Engines, I usually do my model development using Google Colab.
Let’s rerun the last example on Colab:
Examples Using Hugging Face Open Source Models
Using a standard Colab GPU, the query/prediction time is much
faster. Here is a link to my Colab notebook4 if you would prefer to
run this example on Colab instead of on your laptop.
4https://colab.research.google.com/drive/1Ecg-0iid3AD05zM4HgPXTVHcgkGxyi3q?usp=
sharing
11 Using Large Language
Models to Write Recipes
If you ask the ChatGPT web app to write a recipe using a user
supplied ingredient list and a description it does a fairly good job
at generating recipes. For the example in this chapter I am taking
a different approach:
• Use the recipe and ingredient files from my web app http://
cookingspace.com to create context text, given a user prompt
for a recipe.
• Treat this as a text prediction problem.
• Format the response for display.
This approach has an advantage (for me!) that the generated recipes
will be more similar to the recipes I enjoy cooking since the context
data will be derived from my own recipe files.
11.1 Preparing Recipe Data
Recipe
http://cookingspace.com.
The following Python script converts
my JSON data to text descriptions, one per file:
Using Large Language Models to Write Recipes
import json
def process_json(fpath):
with open(fpath, 'r') as f:
data = json.load(f)
for d in data:
with open(f"text_data/{d['name']}.txt", 'w') as f:
f.write("Recipe name: " + d['name'] + '\n\n')
f.write("Number of servings: " +
str(d['num_served']) + '\n\n')
ingrediants = ["
" + str(ii['amount']) +
' ' + ii['units'] + ' ' +
ii['description']
for ii in d['ingredients']]
f.write("Ingredients:\n" +
"\n".join(ingrediants) + '\n\n')
f.write("Directions: " +
' '.join(d['directions']) + '\n')
if __name__ == "__main__":
process_json('data/vegetarian.json')
process_json('data/desert.json')
process_json('data/fish.json')
process_json('data/meat.json')
process_json('data/misc.json')
Here is a listing of one of the shorter generated recipe files (i.e.,
text recipe data converted from raw JSON recipe data from my
CookingSpace.com web site):
Using Large Language Models to Write Recipes
Recipe name: Black Bean Dip
Number of servings: 6
Ingredients:
2 cup Refried Black Beans
1/4 cup Sour cream
1 teaspoon Ground cumin
1/2 cup Salsa
Directions: Use either a food processor or a mixing bowl \
and hand mixer to make this appetizer. Blend the black be
ans and cumin for at least one minute until the mixture i
s fairly smooth. Stir in salsa and sour cream and lightly
mix. Serve immediately or store in the refrigerator.
I have generated 41 individual recipe files that will be used for the
remainder of this chapter.
In the next section when we use a LLM to generate a recipe, the
directions are numbered steps and the formatting is different than
my original recipe document files.
11.2 A Prediction Model Using the
OpenAI text-davinci-002 Model
Here we use the DirectoryLoader class that we have used in
previous examples to load and then create an embedding index.
Here is the listing for the script recipe_generator.py:
Using Large Language Models to Write Recipes
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import DirectoryLoader
from langchain import OpenAI, VectorDBQA
embeddings = OpenAIEmbeddings()
loader = DirectoryLoader('./text_data/', glob="**/*.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=2500,
chunk_overlap=0)
texts = text_splitter.split_documents(documents)
docsearch = Chroma.from_documents(texts, embeddings)
qa = VectorDBQA.from_chain_type(llm=OpenAI(temperature=0,
model_name=
"text-davinci-002"),
chain_type="stuff",
vectorstore=docsearch)
def query(q):
print(f"\n\nRecipe creation request: {q}\n")
print(f"{qa.run(q)}\n\n")
query("Create a new recipe using Broccoli and Chicken")
query("Create a recipe using Beans, Rice, and Chicken")
This generated two recipes. Here is the output for the first request:
Using Large Language Models to Write Recipes
$ python recipe_generator.py
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transie\
Recipe creation request: Create a new recipe using both B\
roccoli and Chicken
Recipe name: Broccoli and Chicken Teriyaki
Number of servings: 4
Ingredients:
1 cup broccoli
1 pound chicken meat
2 tablespoons soy sauce
1 tablespoon honey
1 tablespoon vegetable oil
1 clove garlic, minced
1 teaspoon rice vinegar
Directions:
1. In a large bowl, whisk together soy sauce, honey, vege\
table oil, garlic, and rice vinegar.
2. Cut the broccoli into small florets. Add the broccoli \
and chicken to the bowl and toss to coat.
3. Preheat a grill or grill pan over medium-high heat.
4. Grill the chicken and broccoli for 5-7 minutes per sid\
e, or until the chicken is cooked through and the broccol
i is slightly charred.
5. Serve immediately.
If you examine the text recipe files I indexed you see that the
prediction model merged information from multiple training data
recipes while creating new original text for directions that is loosely
based on the directions that I wrote and information encoded in the
Using Large Language Models to Write Recipes
OpenAI text-davinci-002 model.
Here is the output for the second request:
Recipe creation request: Create a recipe using Beans, Ric\
e, and Chicken
Recipe name: Beans and Rice with Chicken
Number of servings: 4
Ingredients:
1 cup white rice
1 cup black beans
1 chicken breast, cooked and shredded
1/2 teaspoon cumin
1/2 teaspoon chili powder
1/4 teaspoon salt
1/4 teaspoon black pepper
1 tablespoon olive oil
1/2 cup salsa
1/4 cup cilantro, chopped
Directions:
1. Cook rice according to package instructions.
2. In a medium bowl, combine black beans, chicken, cumin,\
chili powder, salt, and black pepper.
3. Heat olive oil in a large skillet over medium heat. Ad\
d the bean mixture and cook until heated through, about 5
minutes.
4. Stir in salsa and cilantro. Serve over cooked rice.
Using Large Language Models to Write Recipes
11.3 Cooking Recipe Generation
Wrap Up
Cooking is one of my favorite activities (in addition to hiking,
kayaking, and playing a variety of musical instruments). I origi-
nally wrote the CookingSpace.com1 web app to scratch a personal
itch: due to a medical issue I had to closely monitor and regulate
my vitamin K intake. I used the US Government’s USDA Nutrition
Database to estimate the amounts of vitamins and nutrients in some
recipes that I use.
When I wanted to experiment with generative models, backed by
my personal recipe data, to create recipes, having available recipe
data from my previous project as well as tools like OpenAI APIs
and LangChain made this experiment simple to set up and run. It
is a common theme in this book that it is now relatively easy to
create personal projects based on our data and our interests.
1http://cookingspace.com
12 Book Wrap Up
This book has been fun to write but it has also somewhat frustrat-
It was fun because I have never been as excited by new technol-
ogy as I have by LLMs and utility software like LangChain and
LlamaIndex for building personalized applications.
This book was frustrating in the sense that it is now so very easy to
build applications that just a few years would have been impossible
to write. Usually when I write books I have two criteria: I only
write about things that I am personally interested in and use, and I
also hope to figure out non-obvious edge cases and make easier for
my readers to use new tech. Here my frustration is writing about
something that it is increasingly simple to do so I feel like my value
is diminished.
All that said I hope, dear reader, that you found this book to be
worth your time reading.
What am I going to do next? Although I am not fond of pro-
gramming in JavaScript (although I find TypeScript to be somewhat
better), I want to explore the possibilities of writing an open source
Persistent Storage Web App Personal Knowledge Base Manage-
ment System. I might get pushback on this but I would probably
make it Apple Safari specific so I can use Apple’s CloudKit JS to
make its use seamless across macOS, iPadOS, and iOS. If I get the
right kind of feedback on social media I might write a book around
this project.
Thank you for reading my book!
Best regards, Mark Watson
Loving Common Lisp, or the Savvy
Programmer’s Secret Weapon
Mark Watson
This book is for sale at http://leanpub.com/lovinglisp
This version was published on 2023-05-01
This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing
process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and
many iterations to get reader feedback, pivot until you have the right book and build traction once
you do.
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0
International License
Contents
Cover Material, Copyright, and License . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Notes on the Eighth Edition Published August 2022 . . . . . . . . . . . . . . . . . . . . . . . .
Notes on the Seventh Edition Published March 2021 . . . . . . . . . . . . . . . . . . . . . . .
Notes on the Sixth Edition Published June 2020 . . . . . . . . . . . . . . . . . . . . . . . . . .
Notes on the Fifth Edition Published September 2019 . . . . . . . . . . . . . . . . . . . . . . .
Why Use Common Lisp? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Request from the Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Older Book Editions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Setting Up Your Common Lisp Development System and Quicklisp . . . . . . . . . . . . . .
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Why Did I Write this Book? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Free Software Tools for Common Lisp Programming . . . . . . . . . . . . . . . . . . . . . . .
Making Book Examples Run Portably on Most Common Lisp Implementations . . . . . . .
How is Lisp Different from Languages like Java and C++? . . . . . . . . . . . . . . . . . . . .
Advantages of Working in a Lisp Environment . . . . . . . . . . . . . . . . . . . . . . . . . . .
Common Lisp Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Getting Started with SBCL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Making the repl Nicer using rlwrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Basics of Lisp Programming
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Operations on Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Arrays and Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Hash Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Eval to Evaluate Lisp Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using a Text Editor to Edit Lisp Source Files . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Recovering from Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Garbage Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Loading your Working Environment Quickly . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CONTENTS
Functional Programming Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Quicklisp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Quicklisp to Find Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Quicklisp to Configure Emacs and Slime
. . . . . . . . . . . . . . . . . . . . . . . . . .
Defining Lisp Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Lambda Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Recursion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Closures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using the Function eval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Defining Common Lisp Macros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example Macro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using the Splicing Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using macroexpand-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Common Lisp Loop Macros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
dolist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
dotimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
do . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using the loop Special Form to Iterate Over Vectors or Arrays . . . . . . . . . . . . . . . . .
Common Lisp Package System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Input and Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Lisp read and read-line Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Lisp Printing Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Plotting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Implementing the Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Packaging as a Quicklisp Project
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Common Lisp Object System - CLOS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example of Using a CLOS Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Implementation of the HTMLstream Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using Defstruct or CLOS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Heuristically Guided Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Network Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
An introduction to Drakma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
An introduction to Hunchentoot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Complete REST Client Server Example Using JSON for Data Serialization . . . . . . . . . .
Network Programming Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CONTENTS
Using the Microsoft Bing Search APIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Getting an Access Key for Microsoft Bing Search APIs . . . . . . . . . . . . . . . . . . . . . .
Example Search Script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Accessing Relational Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Database Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Using MongoDB, Solr NoSQL Data Stores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
MongoDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A Common Lisp Solr Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
NoSQL Wrapup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Loading and Running the NLP Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Part of Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Categorizing Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
Detecting People’s Names and Place Names
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
Summarizing Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
Text Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
Information Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
DBPedia Lookup Service . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Web Spiders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
Using Apache Nutch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
Using The CL Machine-Learning Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Using the CLML Data Loading and Access APIs . . . . . . . . . . . . . . . . . . . . . . . . . . 134
K-Means Clustering of Cancer Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
SVM Classification of Cancer Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
CLML Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
Backpropagation Neural Networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
Hopfield Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
Using Python Deep Learning Models In Common Lisp With a Web Services Interface . . 161
Setting up the Python Web Services Used in this Chapter . . . . . . . . . . . . . . . . . . . . 161
Installing the spaCY NLP Services
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
Installing the Coreference NLP Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Common Lisp Client for the spaCy NLP Web Services . . . . . . . . . . . . . . . . . . . . . . 163
Common Lisp Client for the Coreference NLP Web Services . . . . . . . . . . . . . . . . . . 165
Trouble Shooting Possible Problems - Skip if this Example Works on Your System . . . . . 166
Python Interop Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
CONTENTS
Using the PY4CL Library to Embed Python in Common Lisp . . . . . . . . . . . . . . . . . . . 168
Project Structure, Building the Python Wrapper, and Running an Example . . . . . . . . . . 168
Implementation of spacy-py4cl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
Trouble Shooting Possible Problems - Skip if this Example Works on Your System . . . . . 171
Wrap-up for Using Py4CL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
Semantic Web and Linked Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
Resource Description Framework (RDF) Data Model . . . . . . . . . . . . . . . . . . . . . . . 174
Extending RDF with RDF Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
The SPARQL Query Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
Case Study: Using SPARQL to Find Information about Board of Directors Members of
Corporations and Organizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
Installing the Apache Jena Fuseki RDF Server . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
Common Lisp Client Examples for the Apache Jena Fuseki RDF Server . . . . . . . . . . . . 187
Automatically Generating Data for Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . 190
Implementation Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Generating RDF Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
Generating Data for the Neo4j Graph Database . . . . . . . . . . . . . . . . . . . . . . . . . . 195
Implementing the Top Level Application APIs . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
Implementing The Web Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Creating a Standalone Application Using SBCL . . . . . . . . . . . . . . . . . . . . . . . . . . 203
Augmenting RDF Triples in a Knowledge Graph Using DBPedia . . . . . . . . . . . . . . . . 204
KGCreator Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
Knowledge Graph Sampler for Creating Small Custom Knowledge Graphs . . . . . . . . . 207
Knowledge Graph Navigator Common Library Implementation . . . . . . . . . . . . . . . . . 212
Example Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
Project Configuration and Running the Application . . . . . . . . . . . . . . . . . . . . . . . . 218
Review of NLP Utilities Used in Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
Developing Low-Level SPARQL Utilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
Implementing the Caching Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
Utilities in the Main Library File kgn-common.lisp . . . . . . . . . . . . . . . . . . . . . . . . 226
Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
Knowledge Graph Navigator Text-Based User Interface . . . . . . . . . . . . . . . . . . . . . . 234
Example Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
Text User Interface Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
Knowledge Graph Navigator User Interface Using LispWorks CAPI . . . . . . . . . . . . . . 246
Project Configuration and Running the Application . . . . . . . . . . . . . . . . . . . . . . . . 247
Utilities to Colorize SPARQL and Generated Output . . . . . . . . . . . . . . . . . . . . . . . 249
CONTENTS
Main Implementation File kgn-capi-ui.lisp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
User Interface Utilites File user-interface.lisp . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
User Interface CAPI Options Panes Definition File option-pane.lisp . . . . . . . . . . . . . . 258
Using LispWorks CAPI UI Toolkit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
Wrap-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
Using the OpenAI APIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
Using the Hugging Face Deep Learning Natural Language Processing APIs . . . . . . . . . 274
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for
Semantically Querying Your Own Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
Overview of Local Embeddings Vector Database to Enhance the Use of GPT3 APIs With
Local Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
Implementing a Local Vector Database for Document Embeddings . . . . . . . . . . . . . . 278
Using Local Embeddings Vector Database With OpenAI GPT APIs . . . . . . . . . . . . . . 281
Testing Local Embeddings Vector Database With OpenAI GPT APIs . . . . . . . . . . . . . 282
Adding Chat History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
Wrap Up for Using Local Embeddings Vector Database to Enhance the Use of GPT3 APIs
With Local Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Prompt Engineering for Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
Two Types of LLMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
Prompt Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
Prompt Engineering Wrapup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
Using Common Lisp with Wolfram/One . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Book Wrapup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
Cover Material, Copyright, and
License
Copyright 2011-2023 Mark Watson. All rights reserved. This book may be shared using the Creative
Commons “share and share alike, no modifications, no commercial reuse” license.
This eBook will be updated occasionally so please periodically check the leanpub.com web page for
this book¹ for updates.
This is the eighth edition released August 2022.
Please visit the author’s website².
If you found a copy of this book on the web and find it of value then please consider buying a copy
at leanpub.com/lovinglisp³.
If you would like to support my work please consider purchasing my books on Leanpub⁴ and star
my git repositories that you find useful on GitHub⁵. You can also interact with me on social media
on Mastodon⁶ and Twitter⁷.
¹https://leanpub.com/lovinglisp
²http://markwatson.com
³https://leanpub.com/lovinglisp
⁴https://leanpub.com/u/markwatson
⁵https://github.com/mark-watson?tab=repositories&q=&type=public
⁶https://mastodon.social/@mark_watson
⁷https://twitter.com/mark_l_watson
Preface
Notes on the Eighth Edition Published August 2022
The main change is splitting the Knowledge Graph Navigator (KGN) chapter that features the
LispWorks CAPI UI APIs into three chapters for a library for KGN functionality, a text based
(console) UI, and a CAPI based UI. I added examples using the OpenAI GPT-3 APIs. There are
other small corrections and improvements.
Notes on the Seventh Edition Published March 2021
I added two short chapters to the previous edition: Knowledge Graph Sampler for Creating Small
Custom Knowledge Graphs and Using Common Lisp With Wolfram/One.
Notes on the Sixth Edition Published June 2020
Two examples optionally use the CAPI user interface toolkit provided with LispWorks Common
Lisp⁸ and work with the free personal edition. The first CAPI application is Knowledge Graph
Navigator⁹ and the second CAPI example is Knowledge Graph Creator¹⁰. Both of these examples
build up utilities for working with Knowledge Graphs and the Semantic Web.
I expand the Plot Library chapter to generate either PNG graphics files or if you are using the free
personal edition of LispWorks you can also direct plotting output to a new window in interactive
programs.
I added a new chapter on using the py4cl library to embed Python libraries and application code into
a Common Lisp system. I provide new examples for embedding spaCy and TensorFlow applications
in Common Lisp applications. In earlier editions, I used a web services interface to wrap Python
code using spaCy and TensorFlow. I am leaving that chapter intact, renaming it from “Using Python
Deep Learning Models In Common Lisp” to “Using Python Deep Learning Models In Common Lisp
With a Web Services Interface.” The new chapter for this edition is “Using the PY4CL Library to
Embed Python in Common Lisp.”
⁸https://lispworks.com
⁹http://knowledgegraphnavigator.com
¹⁰http://kgcreator.com
Preface
Notes on the Fifth Edition Published September 2019
There were two chapters added:
• A complete application for processing text to generate data for Knowledge Graphs (targeting
the open source Neo4J graph database and also support RDF semantic web/linked data).
• A library for accessing the state of the art spaCy natural language processing (NLP) library
and also a state of the art deep learning model. These models are implemented in thin Python
wrappers that use Python libraries like spaCy, PyTorch, and TensorFlow. These examples
replace a simple hybrid Java and Common Lisp example in previous editions.
I have added text and explanations as appropriate throughout the book and I removed the CouchDB
examples.
I have made large changes to how the code for this book is packaged. I have reorganized the example
code on GitHub by providing the examples as multiple Quicklisp libraries or applications. I now do
this with all of my Common Lisp code and it makes it easier to write smaller libraries that can be
composed into larger applications. In my own workflow, I also like to use Makefile targets to build
standalone applications that can be run on other computers without installing Lisp development
environments. Please follow the directions at the end of the Preface for configuring Quicklisp for
easy builds and use of the example software for this book.
Why Use Common Lisp?
Why Common Lisp? Isn’t Common Lisp an old language? Do many people still use Common Lisp?
I believe that using Lisp languages like Common Lisp, Clojure, Racket, and Scheme are all secret
weapons useful in agile software development. An interactive development process and live
production updates feel like a breath of fresh air if you have development on heavy weight like
Java Enterprise Edition (JEE).
Yes, Common Lisp is an old language but with age comes stability and extremely good compiler
technology. There is also a little inconsistency between different Common Lisp systems in such
things as handling threads but with a little up front knowledge you can choose which Common Lisp
systems will support your requirements.
A Request from the Author
I spent time writing this book to help you, dear reader. I release this book under the Creative
Commons License and set the minimum purchase price to $0.00 (free!) in order to reach the most
readers. Under this license you can share a PDF version of this book with your friends and coworkers
Preface
and I encourage you to do so. If you found this book on the web (or it was given to you) and if it
provides value to you then please consider doing one of the following to support my future writing
efforts and also to support future updates to this book:
• Purchase a copy of this book leanpub.com/lovinglisp/¹¹ or any other of my leanpub books at
https://leanpub.com/u/markwatson¹²
• Hire me as a consultant¹³
I enjoy writing and your support helps me write new editions and updates for my books and to
develop new book projects. Thank you!
Older Book Editions
The fourth edition of this book was released in May 2017 and the major changes were:
• Added an example application KGCreator that processes text data to automatically generate
data for Knowledge Graphs. This example application supports the Neo4J graph database as
well as semantic web/linked data systems. The major changes were:
• Added a backpropagation neural network example
• Added a deep learning example using the Java based Armed Bear Common Lisp with the
popular DeepLearning4j library
• Added a heuristic search example
• Added two machine learning examples (K-Means clustering and SVM classification) using the
CLML library
• A few edits to the previous text
The third edition was released in October 2014. The major changes made in the 2014 edition are:
• I reworked the chapter Common Lisp Basics.
• I added material to the chapter on using QuickLisp.
The second edition was released in 2013 and was derived from the version that I distributed on my
web site and I moved production of the book to leanpub.com¹⁴.
¹¹https://leanpub.com/lovinglisp/
¹²https://leanpub.com/u/markwatson
¹³https://markwatson.com/
¹⁴https://leanpub.com/u/markwatson
Preface
Acknowledgments
I would like to thank Jans Aasman¹⁵ for contributing as technical editor for the fourth edition of this
book. Jans is CEO of Franz.com¹⁶ which sells Allegro Common Lisp¹⁷ as well as tools for semantic
web and linked data applications.
I would like to thank the following people who made suggestions for improving previous editions
of this book:
Sam Steingold, Andrew Philpot, Kenny Tilton, Mathew Villeneuve, Eli Draluk, Erik Winkels, Adam
Shimali, and Paolo Amoroso.
I would like to also thank several people who pointed out typo errors in this book and for specific
suggestions: Martin Lightheart, Tong-Kiat Tan, Rainer Joswig, Gerold Rupprecht, HN member
rurban, David Cortesi. I would like to thank the following Reddit /r/lisp readers who pointed out
mistakes in the fifth edition of this book: arnulfslayer, rpiirp, and itmuckel. I would like to thank
Ted Briscoe for pointing out a problem with the spacy web client example in the 6th edition.
I would like to thank Paul Graham for coining the phrase “The Secret Weapon” (in his excellent
paper “Beating the Averages”) in discussing the advantages of Lisp and giving me permission to
reuse his phrase.
I would especially like to thank my wife Carol Watson for her fine work in
editing this book.
Setting Up Your Common Lisp Development System
and Quicklisp
These instructions assume the use of SBCL. See comments for LispWorks, Franz Common Lisp,
and Closure Common List at the end of this section. I assume that you have installed SBCL and
Quicklisp by following the instructions at lisp-lang.org/learn/getting-started¹⁸. These instructions
also guide you through installing the Slime extensions for Emacs. I use both Emacs + Slime and
VSCode with Common Lisp plugins for editing Common Lisp. If you like VSCode then I recommend
Yasuhiro Matsumoto’s Lisp plugin for syntax highlighting. For both Emacs and VSCode I usually
run a separate REPL in a terminal window and don’t run an editor-integrated REPL. I think that I
am in the minority in using a separate REPL running in a shell.
I have been using Common Lisp since about 1982 and Quicklisp (developed and maintained by Zach
Beane¹⁹) has been the most revolutionary change in my Common Lisp development (even more so
than getting a hardware Lisp Machine and the availability of Coral Common Lisp on the Macintosh).
¹⁵https://en.wikipedia.org/wiki/Jans_Aasman
¹⁶http://franz.com/
¹⁷http://franz.com/products/allegro-common-lisp/
¹⁸https://lisp-lang.org/learn/getting-started/
¹⁹https://www.xach.com
Preface
You can follow the directions on the main GitHub repository for this book: https://github.com/mark-
watson/loving-common-lisp²⁰ to get the examples set up to run on your computer. Starting with the
8th edition, I have a new scheme for distributing the book examples on GitHub:
• A few short example Common Lisp code snippets are still kept in the main repository for the
book: https://github.com/mark-watson/loving-common-lisp²¹.
• The longer examples are now stored in separate GitHub repositories to facilitate using them as
reusable Quicklisp libraries.
• Clone the main GitHub repository and copy the Makefile²² to the directory ∼/quicklisp/local-
projects/ on your computer.
• Change directory to ∼/quicklisp/local-projects/ and run the Makefile target make fetch to
copy all separate GitHub repositories to subdirectories of ∼/quicklisp/local-projects/.
• You can now load any book example using Quicklisp, for example: (ql:quickload :sparql).
For example, the subdirectory loving-common-lisp/src/spacy-py4cl contains a package named
spacy-py4cl that can now be accessed from any directory on your system using:
$ sbcl
(ql:quickload "spacy-py4cl")
* (spacy-py4cl:nlp "My sister has a dog Henry. She loves him.")
* (defvar x (spacy-py4cl:nlp "President Bill Clinton went to Congress. He gave a spe\
ech on taxes and Mexico."))
This example uses the deep learning NLP models in spaCy which is a Python library - see the chapter
on NLP for details on installing the Python dependencies. Note that only a few examples in this book
require Python dependencies.
I have used the SBCL implementation of Common Lisp in this book. There are many fine Common
Lisp implementations from Franz, LispWorks, Clozure Common Lisp, etc. I usually use LispWorks for
my professional development work. If you have any great difficulty adopting the examples to your
choice of Common Lisp implementations and performing web search does not suggest a solution
then you can reach me through my web site, markwatson.com²³.
²⁰https://github.com/mark-watson/loving-common-lisp
²¹https://github.com/mark-watson/loving-common-lisp
²²https://raw.githubusercontent.com/mark-watson/loving-common-lisp/master/Makefile
²³https://markwatson.com
Introduction
This book is intended to get you, the reader, programming quickly in Common Lisp. Although the
Lisp programming language is often associated with artificial intelligence, this introduction is on
general Common Lisp programming techniques. Later we will look at general example applications
and artificial intelligence examples.
The Common Lisp program examples are distributed on the github repo for this book²⁴.
Why Did I Write this Book?
Why the title “Loving Common Lisp”? Simple! I have been using Lisp for almost 40 years and seldom
do I find a better match between a programming language and the programming job at hand. I am
not a total fanatic on Lisp, however. I often use Python for deep learning. I like Ruby, Java and
Javascript for server side programming, and the few years that I spent working on Nintendo video
games and virtual reality systems for SAIC and Disney, I found C++ to be a good bet because of
stringent runtime performance requirements. For some jobs, I find the logic-programming paradigm
useful: I also enjoy the Prolog language.
In any case, I love programming in Lisp, especially the industry standard Common Lisp. As I
wrote the second edition of this book over a decade ago, I had been using Common Lisp almost
exclusively for an artificial intelligence project for a health care company and for commercial
product development. While working on the third edition of this book, I was not using Common
Lisp professionally but since the release of the Quicklisp Common Lisp package manager I have
found myself enjoying using Common Lisp more for small side projects. I use Quicklisp throughout
in the third edition example code so you can easily install required libraries. For the fourth and fifth
editions of this book I have added more examples using neural networks and deep learning. In this
new sixth edition I have added a complete application that uses CAP for the user interface.
As programmers, we all (hopefully) enjoy applying our experience and brains for tackling interesting
problems. My wife and I recently watched a two-night 7-hour PBS special “Joseph Campbell, and the
Power of Myths.” Campbell, a college professor for almost 40 years, said that he always advised his
students to “follow their bliss” and not to settle for jobs and avocations that are not what they truly
want to do. That said I always feel that when a job calls for using Java, Python or other languages
besides Lisp, that even though I may get a lot of pleasure from the job I am not following my bliss.
My goal in this book is to introduce you to one of my favorite programming languages, Common
Lisp. I assume that you already know how to program in another language but if you are a complete
beginner you can still master the material in this book with some effort. I challenge you to make
this effort.
²⁴https://github.com/mark-watson/loving-common-lisp
Introduction
Free Software Tools for Common Lisp Programming
There are several Common Lisp compilers and runtime tools available for free on the web:
• CLISP – licensed under the GNU GPL and is available for Windows, Macintosh, and Linux/U-
• Clozure Common Lisp (CCL) – open source with good Mac OS X and Linux support
• CMU Common Lisp – open source implementation
• SBCL – derived from CMU Common Lisp
• ECL – compiles using a separate C/C++ compiler
• ABCL – Armed Bear Common Lisp for the JVM
There are also fine commercial Common Lisp products:
• LispWorks – high quality and reasonably priced system for Windows and Linux. No charge
for distributing compiled applications lispworks.com²⁵
• Allegro Common Lisp - high quality, great support and higher cost. franz.com²⁶
• MCL – Macintosh Common Lisp. I used this Lisp environment in the late 1980s. MCL was so
good that I gave away my Xerox 1108 Lisp Machine and switched to a Mac and MCL for my
development work. Now open source but only runs on the old MacOS
I currently (mostly) use SBCL, CCL, and LispWorks. The SBCL compiler produces very fast code
and the compiler warning can be of great value in finding potential problems with your code. Like
CCL because it compiles quickly so is often preferable for development.
For working through this book, I will assume that you are using SBCL or CCL. For the example in
the last chapter you will need LispWorks and the free Personal edition is fine for the purposes of
experimenting with the example application and the CAPI user interface library.
Making Book Examples Run Portably on Most Common
Lisp Implementations
Many of the book examples require making web service calls. In general when I am writing Common
Lisp applications that require making REST calls I prefer using 3rd party Common Lisp libraries like
Drakma or Dexador. However it is sometimes a little tricky to set up Common Lisp on different
operating systems and CPU architectures with libopenssl, libcrypto, etc. Because of this, in book
examples I run the external curl program using uiop:run-program and collect the output as a string
that is then parsed as JSON or CSV data. The overhead of starting an external process is very small
²⁵http://www.lispworks.com
²⁶http://franz.com
Introduction
compared to calling a web service so in your own applications you can either follow my example of
using curl or use the Drakma or Dexador libraries. Using the Apple M1 processor on macOS can be
particularly problematic with OpenSSL issues.
I also use the excellent Common Lisp to Python bridge library py4cl in a few book examples. Usually
py4cl installs without problems.
How is Lisp Different from Languages like Java and
This is a trick question! Lisp is slightly more similar to Java than C++ because of automated memory
management so we will start by comparing Lisp and Java.
In Java, variables are strongly typed while in Common Lisp values are strongly typed. For example,
consider the Java code:
Float x = new Float(3.14f);
String s = "the cat ran" ;
Object any_object = null;
any_object = s;
x = s;
// illegal: generates a
// compilation error
Here, in Java, variables are strongly typed so a variable x of type Float can’t legally be assigned a
string value: the code in line 5 would generate a compilation error. Lisp code can assign a value to
a variable and then reassign another value of a different type.
Java and Lisp both provide automatic memory management. In either language, you can create new
data structures and not worry about freeing memory when the data is no longer used, or to be more
precise, is no longer referenced.
Common Lisp is an ANSI standard language. Portability between different Common Lisp implemen-
tations and on different platforms is very good. I have used Clozure Common Lisp, SBCL, Allegro
Lisp (from Franz Inc), LispWorks, and CLISP that all run well on Windows, Mac OS X, and Linux.
As a Common Lisp developer you will have great flexibility in tools and platforms.
ANSI Common Lisp was the first object oriented language to become an ANSI standard language.
The Common Lisp Object System (CLOS) is probably the best platform for object oriented
programming.
In C++ programs, a common bug that affects a program’s efficiency is forgetting to free memory that
is no longer used. In a virtual memory system, the effect of a program’s increasing memory usage
is usually just poorer system performance but can lead to system crashes or failures if all available
virtual memory is exhausted. A worse type of C++ error is to free memory and then try to use it.
Can you say “program crash”? C programs suffer from the same types of memory related errors.
Introduction
Since computer processing power is usually much less expensive than the costs of software
development, it is almost always worth while to give up a few percent of runtime efficiency and let
the programming environment of runtime libraries manage memory for you. Languages like Lisp,
Ruby, Python, and Java are said to perform automatic garbage collection.
I have written six books on Java, and I have been quoted as saying that for me, programming in Java
is about twice as efficient (in terms of my time) as programming in C++. I base this statement on
approximately ten years of C++ experience on projects for SAIC, PacBell, Angel Studios, Nintendo,
and Disney. I find Common Lisp and other Lisp languages like Clojure and Scheme to be about twice
as efficient (again, in terms of my time) as Java. That is correct: I am claiming a four times increase
in my programming productivity when using Common Lisp vs. C++.
What do I mean by programming productivity? Simple: for a given job, how long does it take me to
design, code, debug, and later maintain the software for a given task.
Advantages of Working in a Lisp Environment
We will soon see that Lisp is not just a language; it is also a programming environment and runtime
environment.
The beginning of this book introduces the basics of Lisp programming. In later chapters, we will
develop interesting and non-trivial programs in Common Lisp that I argue would be more difficult
to implement in other languages and programming environments.
The big win in programming in a Lisp environment is that you can set up an environment and
interactively write new code and test new code in small pieces. We will cover programming with
large amounts of data in the Chapter on Natural Language Processing, but let me share a a general
use case for work that I do that is far more efficient in Lisp:
Much of my Lisp programming used to be writing commercial natural language processing (NLP)
programs for my company www.knowledgebooks.com. My Lisp NLP code uses a large amount of
memory resident data; for example: hash tables for different types of words, hash tables for text
categorization, 200,000 proper nouns for place names (cities, counties, rivers, etc.), and about 40,000
common first and last names of various nationalities.
If I was writing my NLP products in C++, I would probably use a relational database to store this
data because if I read all of this data into memory for each test run of a C++ program, I would
wait 30 seconds every time that I ran a program test. When I start working in any Common
Lisp environment, I do have to load the linguistic data into memory one time, but then can
code/test/code/test… for hours with no startup overhead for reloading the data that my programs
need to run. Because of the interactive nature of Lisp development, I can test small bits of code when
tracking down errors and when writing new code.
It is a personal preference, but I find the combination of the stable Common Lisp language and an
iterative Lisp programming environment to be much more productive than other languages and
programming environments.
Common Lisp Basics
The material in this chapter will serve as an introduction to Common Lisp. I have attempted to
make this book a self contained resource for learning Common Lisp and to provide code examples
to perform common tasks. If you already know Common Lisp and bought this book for the code
examples later in this book then you can probably skip this chapter.
For working through this chapter we will be using the interactive shell, or repl, built into SBCL and
other Common Lisp systems. For this chapter it is sufficient for you to download and install SBCL²⁷.
Please install SBCL right now, if you have not already done so.
Getting Started with SBCL
When we start SBCL, we see an introductory message and then an input prompt. We will start with
a short tutorial, walking you through a session using SBCL repl (other Common LISP systems are
very similar). A repl is an interactive console where you type expressions and see the results of
evaluating these expressions. An expression can be a large block of code pasted into the repl, using
the load function to load Lisp code into the repl, calling functions to test them, etc. Assuming that
SBCL is installed on your system, start SBCL by running the SBCL program:
% sbcl
(running SBCL from: /Users/markw/sbcl)
This is SBCL 2.0.2, an implementation of ANSI Common Lisp.
More information about SBCL is available at <http://www.sbcl.org/>.
SBCL is free software, provided as is, with absolutely no warranty.
It is mostly in the public domain; some portions are provided under
BSD-style licenses.
See the CREDITS and COPYING files in the
distribution for more information.
* (defvar x 1.0)
* (+ x 1)
²⁷http://www.sbcl.org/platform-table.html
Common Lisp Basics
* (setq x (+ x 1))
* (setq x "the dog chased the cat")
"the dog chased the cat"
"the dog chased the cat"
* (quit)
We started by defining a new variable x in line 11. Notice how the value of the defvar macro is the
symbol that is defined. The Lisp reader prints X capitalized because symbols are made upper case
(we will look at the exception later).
In Lisp, a variable can reference any data type. We start by assigning a floating point value to the
variable x, using the + function to add 1 to x in line 17, using the setq function to change the value
of x in lines 23 and 29 first to another floating point value and finally setting x to a string value.
One thing that you will have noticed: function names always occur first, then the arguments to a
function. Also, parenthesis is used to separate expressions.
I learned to program Lisp in 1976 and my professor half-jokingly told us that Lisp was an acronym
for “Lots-of Irritating Superfluous Parenthesis.” There may be some truth in this when you are just
starting with Lisp programming, but you will quickly get used to the parenthesis, especially if you
use an editor like Emacs that automatically indents Lisp code for you and highlights the opening
parenthesis for every closing parenthesis that you type. Many other editors support coding in Lisp
but I personally use Emacs or sometimes VScode (with Common Lisp plugins) to edit Lisp code.
Before you proceed to the next chapter, please take the time to install SBCL on your computer and
try typing some expressions into the Lisp listener. If you get errors, or want to quit, try using the
quit function:
Common Lisp Basics
* (+ 1 2 3 4)
* (quit)
If you get an error you can enter help to get options for handling an error. When I get an error and
have a good idea of what caused the error then I just enter :a: to abort out of the error).
As we discussed in the introduction, there are many different Lisp programming environments that
you can choose from. I recommend a free set of tools: Emacs, Quicklisp, slime, and SBCL. Emacs is
a fine text editor that is extensible to work well with many programming languages and document
types (e.g., HTML and XML). Slime is an Emacs extension package that greatly facilitates Lisp
development. SBCL is a robust Common Lisp compiler and runtime system that is often used in
production.
We will cover the Quicklisp package manager and using Quicklisp to setup Slime and Emacs in a
later chapter.
I will not spend much time covering the use of Emacs as a text editor in this book since you can try
most of the example code snippets in the book text by copying and then pasting them into a SBCL
repl and by loading the book example source files directly into a repl. If you already use Emacs then
I recommend that you do set up Slime sooner rather than later and start using it for development. If
you are not already an Emacs user and do not mind spending the effort to learn Emacs, then search
the web first for an Emacs tutorial. That said, you will easily be able to use the example code from
this book using any text editor you like with a SBCL repl. I don’t use the vi or vim editors but if vi
is your weapon of choice for editing text then a web search for “common lisp vi vim repl” should
get you going for developing Common Lisp code with vi or vim. If you are not already an Emacs or
vi user then using VSCode with a Common Lisp plugin is recommended.
Here, we will assume that under Windows, Unix, Linux, or Mac OS X you will use one command
window to run SBCL and a separate editor that can edit plain text files.
Making the repl Nicer using rlwrap
While reading the last section you (hopefully!) played with the SBCL interactive repl. If you haven’t
played with the repl, I won’t get too judgmental except to say that if you do not play with the
examples as you read you will not get the full benefit from this book.
Did you notice that the backspace key does not work in the SBCL repl? The way to fix this is to install
the GNU rlwrap utility. On OS X, assuming that you have homebrew²⁸ installed, install rlwrap with:
²⁸http://mxcl.github.io/homebrew/
Common Lisp Basics
brew install rlwrap
If you are running Ubuntu Linux, install rlwrap using:
sudo apt-get install rlwrap
You can then create an alias for bash or zsh using something like the following to define a command
rsbcl:
alias rsbcl='rlwrap sbcl'
This is fine, just remember to run sbcl if you don’t need rlwrap command line editing or run rsbcl
when you do need command line editing. That said, I find that I always want to run SBCL with
command line editing, so I redefine sbcl on my computers using:
which sbcl
/Users/markw/sbcl/sbcl
alias sbcl='rlwrap /Users/markw/sbcl/sbcl'
This alias is different on my laptops and servers, since I don’t usually install SBCL in the default
installation directory. For each of my computers, I add an appropriate alias in my .zshrc file (if I am
running zsh) or my .bashrc file (if I am running bash).
The Basics of Lisp Programming
Although we will use SBCL in this book, any Common Lisp environment will do fine. In previous
sections, we saw the top-level Lisp prompt and how we could type any expression that would be
evaluated:
* 3.14159
3.14159
* "the dog bit the cat"
"the dog bit the cat"
* (defun my-add-one (x)
(+ x 1))
MY-ADD-ONE
* (my-add-one -10)
Common Lisp Basics
Notice that when we defined the function my-add-one in lines 7 and 8, we split the definition over
two lines and on line 8 you don’t see the “*” prompt from SBCL – this lets you know that you have
not yet entered a complete expression. The top level Lisp evaluator counts parentheses and considers
a form to be complete when the number of closing parentheses equals the number of opening
parentheses and an expression is complete when the parentheses match. I tend to count in my head,
adding one for every opening parentheses and subtracting one for every closing parentheses – when
I get back down to zero then the expression is complete. When we evaluate a number (or a variable),
there are no parentheses, so evaluation proceeds when we hit a new line (or carriage return).
The Lisp reader by default tries to evaluate any form that you enter. There is a reader macro ‘ that
prevents the evaluation of an expression. You can either use the ‘ character or quote:
* (+ 1 2)
* '(+ 1 2)
(+ 1 2)
* (quote (+ 1 2))
(+ 1 2)
Lisp supports both global and local variables. Global variables can be declared using defvar:
* (defvar *x* "cat")
* (setq *x* "dog")
* (setq *x* 3.14159)
3.14159
3.14159
One thing to be careful of when defining global variables with defvar: the declared global variable
is dynamically scoped. We will discuss dynamic versus lexical scoping later, but for now a warning:
if you define a global variable avoid redefining the same variable name inside functions. Lisp
programmers usually use a global variable naming convention of beginning and ending dynamically
scoped global variables with the * character. If you follow this naming convention and also do not
use the * character in local variable names, you will stay out of trouble. For convenience, I do not
always follow this convention in short examples in this book.
Common Lisp Basics
Lisp variables have no type. Rather, values assigned to variables have a type. In this last example, the
variable x was set to a string, then to a floating-point number. Lisp types support inheritance and
can be thought of as a hierarchical tree with the type t at the top. (Actually, the type hierarchy is a
DAG, but we can ignore that for now.) Common Lisp also has powerful object oriented programming
facilities in the Common Lisp Object System (CLOS) that we will discuss in a later chapter.
Here is a partial list of types (note that indentation denotes being a subtype of the preceding type):
[top level type (all other types are a sub-type)]
sequence
vector
string
number
rational
integer
complex
character
symbol
structure
function
hash-table
We can use the typep function to test the type of value of any variable or expression or use type-of
to get type information of any value):
* (setq x '(1 2 3))
(1 2 3)
* (typep x 'list)
* (typep x 'sequence)
* (typep x 'number)
* (typep (+ 1 2 3) 'number)
* (type-of 3.14159)
single-float
* (type-of "the dog ran quickly")
(simple-array character (19))
Common Lisp Basics
* (type-of 100193)
(integer 0 4611686018427387903)
A useful feature of all ANSI standard Common Lisp implementations’ top-level listener is that it sets
* to the value of the last expression evaluated. For example:
* (+ 1 2 3 4 5)
* (setq x *)
All Common Lisp environments set * to the value of the last expression evaluated. This example
may be slightly confusing because * is also the prompt character in the SBCL repl that indicates that
you can enter a new expression for evaluation. For example in line 3, the first * character is the repl
prompt and the second * we type in to see that value of the previous expression that we typed into
the repl.
Frequently, when you are interactively testing new code, you will call a function that you just wrote
with test arguments; it is useful to save intermediate results for later testing. It is the ability to create
complex data structures and then experiment with code that uses or changes these data structures
that makes Lisp programming environments so effective.
Common Lisp is a lexically scoped language that means that variable declarations and function
definitions can be nested and that the same variable names can be used in nested let forms; when
a variable is used, the current let form is searched for a definition of that variable and if it is not
found, then the next outer let form is searched. Of course, this search for the correct declaration
of a variable is done at compile time so there need not be extra runtime overhead. We can nest
defun special form inside each other and inside let expressions but this defines the nested functions
globally. We use the special forms flet and labels to define functions inside a scoped environment.
Functions defined inside a labels special form can be recursive while functions defined inside a flet
special form cannot be recursive. Consider the following example in the file nested.lisp (all example
files are in the src directory):
Common Lisp Basics
(flet ((add-one (x)
(+ x 1))
(add-two (x)
(+ x 2)))
(format t "redefined variables: ~A
~A~%" (add-one 100) (add-two 100)))
(let ((a 3.14))
(defun test2 (x)
(print x))
(test2 a))
(test2 50)
(let ((x 1)
(y 2))
;; define a test function nested inside a let statement:
(flet ((test (a b)
(let ((z (+ a b)))
;; define a helper function nested inside a let/function/let:
(flet ((nested-function (a)
(+ a a)))
(nested-function z)))))
;; call nested function 'test':
(format t "test result is ~A~%" (test x y))))
(let ((z 10))
(labels ((test-recursion (a)
(format t "test-recursion ~A~%" (+ a z))
(if (> a 0)
(test-recursion (- a 1)))))
(test-recursion 5)))
We define a top level flet special form in lines 1-5 that defines two nested functions add-one and
add-two and then calls each nested function in the body of the flet special form. For many years I
have used nested defun special forms inside let expressions for defining local functions and you will
notice this use in a few later examples. However, functions defined inside defun special forms have
global visibility so they are not hidden in the local context where they are defined. The example
of a nested defun in lines 7-12 shows that the function test2 has global visibility inside the current
package.
Functions defined inside of a flet special form have access to variables defined in the outer scope
containing the flet (also applies to labels). We see this in lines 14-24 where the local variables x and
y defined in the let expression are visible inside the function nested-function defined inside the
Common Lisp Basics
The final example in lines 26-31 shows a recursive function defined inside a labels special form.
Assuming that we started SBCL in the src directory we can then use the Lisp load function to
evaluate the contents of the file nested.lisp in the sub-directory code_snippets_for_book using
the load function:
* (load "./code_snippets_for_book/nested.lisp")
redefined variables: 101
50 test result is 6
test-recursion 15
test-recursion 14
test-recursion 13
test-recursion 12
test-recursion 11
test-recursion 10
The function load returned a value of t (prints in upper case as T) after successfully loading the file.
We will use Common Lisp vectors and arrays frequently in later chapters, but will also briefly
introduce them here. A singly dimensioned array is also called a vector. Although there are often
more efficient functions for handling vectors, we will just look at generic functions that handle any
type of array, including vectors. Common Lisp provides support for functions with the same name
that take different argument types; we will discuss this in some detail when we cover this in the
later chapter on CLOS. We will start by defining three vectors v1, v2, and v3:
* (setq v1 (make-array '(3)))
#(NIL NIL NIL)
* (setq v2 (make-array '(4) :initial-element "lisp is good"))
#("lisp is good" "lisp is good" "lisp is good" "lisp is good")
* (setq v3 #(1 2 3 4 "cat" '(99 100)))
#(1 2 3 4 "cat" '(99 100))
In line 1, we are defining a one-dimensional array, or vector, with three elements. In line 3 we specify
the default value assigned to each element of the array v2. In line 5 I use the form for specifying
array literals using the special character #. The function aref can be used to access any element in
an array:
Common Lisp Basics
* (aref v3 3)
* (aref v3 5)
'(99 100)
Notice how indexing of arrays is zero-based; that is, indices start at zero for the first element of a
sequence. Also notice that array elements can be any Lisp data type. So far, we have used the special
operator setq to set the value of a variable. Common Lisp has a generalized version of setq called
setf that can set any value in a list, array, hash table, etc. You can use setf instead of setq in all
cases, but not vice-versa. Here is a simple example:
#(NIL NIL NIL)
* (setf (aref v1 1) "this is a test")
"this is a test"
#(NIL "this is a test" NIL)
When writing new code or doing quick programming experiments, it is often easiest (i.e., quickest to
program) to use lists to build interesting data structures. However, as programs mature, it is common
to modify them to use more efficient (at runtime) data structures like arrays and hash tables.
Symbols
We will discuss symbols in more detail the Chapter on Common Lisp Packages. For now, it is enough
for you to understand that symbols can be names that refer to variables. For example:
> (defvar *cat* "bowser")
* *cat*
"bowser"
* (defvar *l* (list *cat*))
("bowser")
Note that the first defvar returns the defined symbol as its value. Symbols are almost always
converted to upper case. An exception to this “upper case rule” is when we define symbols that
may contain white space using vertical bar characters:
Common Lisp Basics
* (defvar |a symbol with Space Characters| 3.14159)
|a symbol with Space Characters|
* |a symbol with Space Characters|
3.14159
Operations on Lists
Lists are a fundamental data structure of Common Lisp. In this section, we will look at some of the
more commonly used functions that operate on lists. All of the functions described in this section
have something in common: they do not modify their arguments.
In Lisp, a cons cell is a data structure containing two pointers. Usually, the first pointer in a cons cell
will point to the first element in a list and the second pointer will point to another cons representing
the start of the rest of the original list.
The function cons takes two arguments that it stores in the two pointers of a new cons data structure.
For example:
* (cons 1 2)
(1 . 2)
* (cons 1 '(2 3 4))
(1 2 3 4)
The first form evaluates to a cons data structure while the second evaluates to a cons data structure
that is also a proper list. The difference is that in the second case the second pointer of the freshly
created cons data structure points to another cons cell.
First, we will declare two global variables l1 and l2 that we will use in our examples. The list l1
contains five elements and the list l2 contains four elements:
* (defvar l1 '(1 2 (3) 4 (5 6)))
* (length l1)
* (defvar l2 '(the "dog" calculated 3.14159))
(1 2 (3) 4 (5 6))
(THE "dog" CALCULATED 3.14159)
Common Lisp Basics
You can also use the function list to create a new list; the arguments passed to function list are the
elements of the created list:
* (list 1 2 3 'cat "dog")
(1 2 3 CAT "dog")
The function car returns the first element of a list and the function cdr returns a list with its first
element removed (but does not modify its argument):
* (car l1)
* (cdr l1)
(2 (3) 4 (5 6))
Using combinations of car and cdr calls can be used to extract any element of a list:
* (car (cdr l1))
* (cadr l1)
Notice that we can combine calls to car and cdr into a single function call, in this case the function
cadr. Common Lisp defines all functions of the form cXXr, cXXXr, and cXXXXr where X can be
either a or d.
Suppose that we want to extract the value 5 from the nested list l1. Some experimentation with using
combinations of car and cdr gets the job done:
(1 2 (3) 4 (5 6))
* (cadr l1)
* (caddr l1)
(car (caddr l1))
* (caar (last l1))
* (caar (cddddr l1))
Common Lisp Basics
The function last returns the last cdr of a list (i.e., the last element, in a list):
* (last l1)
((5 6))
Common list supplies alternative functions to car and cdr that you might find more readable: first,
second, third, fourth, and rest. Here are some examples:
* (defvar *x* '(1 2 3 4 5))
* (first *x*)
* (rest *x*)
(2 3 4 5)
* (second *x*)
* (third *x*)
* (fourth *x*)
The function nth takes two arguments: an index of a top-level list element and a list. The first index
argument is zero based:
(1 2 (3) 4 (5 6))
* (nth 0 l1)
* (nth 1 l1)
* (nth 2 l1)
The function cons adds an element to the beginning of a list and returns as its value a new list (it
does not modify its arguments). An element added to the beginning of a list can be any Lisp data
type, including another list:
Common Lisp Basics
* (cons 'first l1)
(FIRST 1 2 (3) 4 (5 6))
* (cons '(1 2 3) '(11 22 33))
((1 2 3) 11 22 33)
The function append takes two lists as arguments and returns as its value the two lists appended
together:
(1 2 (3) 4 (5 6))
('THE "dog" 'CALCULATED 3.14159)
* (append l1 l2)
(1 2 (3) 4 (5 6) THE "dog" CALCULATED 3.14159)
* (append '(first) l1)
(FIRST 1 2 (3) 4 (5 6))
A frequent error that beginning Lisp programmers make is not understanding shared structures in
lists. Consider the following example where we generate a list y by reusing three copies of the list x:
* (setq x '(0 0 0 0))
(0 0 0 0)
* (setq y (list x x x))
((0 0 0 0) (0 0 0 0) (0 0 0 0))
* (setf (nth 2 (nth 1 y)) 'x)
(0 0 X 0)
((0 0 X 0) (0 0 X 0) (0 0 X 0))
* (setq z '((0 0 0 0) (0 0 0 0) (0 0 0 0)))
((0 0 0 0) (0 0 0 0) (0 0 0 0))
* (setf (nth 2 (nth 1 z)) 'x)
((0 0 0 0) (0 0 X 0) (0 0 0 0))
When we change the shared structure referenced by the variable x that change is reflected three times
in the list y. When we create the list stored in the variable z we are not using a shared structure.
Common Lisp Basics
Using Arrays and Vectors
Using lists is easy but the time spent accessing a list element is proportional to the length of the list.
Arrays and vectors are more efficient at runtime than long lists because list elements are kept on
a linked-list that must be searched. Accessing any element of a short list is fast, but for sequences
with thousands of elements, it is faster to use vectors and arrays.
By default, elements of arrays and vectors can be any Lisp data type. There are options when creating
arrays to tell the Common Lisp compiler that a given array or vector will only contain a single data
type (e.g., floating point numbers) but we will not use these options in this book.
Vectors are a specialization of arrays; vectors are arrays that only have one dimension. For efficiency,
there are functions that only operate on vectors, but since array functions also work on vectors,
we will concentrate on arrays. In the next section, we will look at character strings that are a
specialization of vectors.
We could use the generalized make-sequence function to make a singularly dimensioned array (i.e.,
a vector). Restart sbcl and try:
* (defvar x (make-sequence 'vector 5 :initial-element 0))
#(0 0 0 0 0)
In this example, notice the print format for vectors that looks like a list with a proceeding # character.
As seen in the last section, we use the function make-array to create arrays:
* (defvar y (make-array '(2 3) :initial-element 1))
#2A((1 1 1) (1 1 1))
Notice the print format of an array: it looks like a list proceeded by a # character and the integer
number of dimensions.
Instead of using make-sequence to create vectors, we can pass an integer as the first argument of
make-array instead of a list of dimension values. We can also create a vector by using the function
vector and providing the vector contents as arguments:
Common Lisp Basics
* (make-array 10)
#(NIL NIL NIL NIL NIL NIL NIL NIL NIL NIL)
* (vector 1 2 3 'cat)
#(1 2 3 CAT)
The function aref is used to access sequence elements. The first argument is an array and the
remaining argument(s) are array indices. For example:
#(0 0 0 0 0)
* (aref x 2)
* (setf (aref x 2) "parrot")
"parrot"
#(0 0 "parrot" 0 0)
* (aref x 2)
"parrot"
#2A((1 1 1) (1 1 1))
* (setf (aref y 1 2) 3.14159)
3.14159
#2A((1 1 1) (1 1 3.14159))
Using Strings
It is likely that even your first Lisp programs will involve the use of character strings. In this section,
we will cover the basics: creating strings, concatenating strings to create new strings, for substrings
in a string, and extracting substrings from longer strings. The string functions that we will look
at here do not modify their arguments; rather, they return new strings as values. For efficiency,
Common Lisp does include destructive string functions that do modify their arguments but we will
not discuss these destructive functions here.
We saw earlier that a string is a type of vector, which in turn is a type of array (which in turn is a type
of sequence). A full coverage of the Common Lisp type system is outside the scope of this tutorial
introduction to Common Lisp; a very good treatment of Common Lisp types is in Guy Steele’s
“Common Lisp, The Language” which is available both in print and for free on the web. Many of
the built in functions for handling strings are actually more general because they are defined for the
Common Lisp Basics
type sequence. The Common Lisp Hyperspec is another great free resource that you can find on the
web. I suggest that you download an HTML version of Guy Steele’s excellent reference book and
the Common Lisp Hyperspec and keep both on your computer. If you continue using Common Lisp,
eventually you will want to read all of Steele’s book and use the Hyperspec for reference.
The following text was captured from input and output from a Common Lisp repl. First, we will
declare two global variables s1 and space that contain string values:
* (defvar s1 "the cat ran up the tree")
* (defvar space " ")
One of the most common operations on strings is to concatenate two or more strings into a new
string:
* (concatenate 'string s1 space "up the tree")
"the cat ran up the tree up the tree"
Notice that the first argument of the function concatenate is the type of the sequence that the
function should return; in this case, we want a string. Another common string operation is search
for a substring:
* (search "ran" s1)
* (search "zzzz" s1)
If the search string (first argument to function search) is not found, function search returns nil,
otherwise search returns an index into the second argument string. Function search takes several
optional keyword arguments (see the next chapter for a discussion of keyword arguments):
(search search-string a-longer-string :from-end :test
:test-not :key
:start1 :start2
:end1 :end2)
For our discussion, we will just use the keyword argument :start2 for specifying the starting search
index in the second argument string and the :from-end flag to specify that search should start at
the end of the second argument string and proceed backwards to the beginning of the string:
Common Lisp Basics
* (search " " s1)
* (search " " s1 :start2 5)
* (search " " s1 :from-end t)
The sequence function subseq can be used for strings to extract a substring from a longer string:
* (subseq s1 8)
"ran up the tree"
Here, the second argument specifies the starting index; the substring from the starting index to the
end of the string is returned. An optional third index argument specifies one greater than the last
character index that you want to extract:
* (subseq s1 8 11)
It is frequently useful to remove white space (or other) characters from the beginning or end of a
string:
* (string-trim '(#\space #\z #\a) " a boy said pez")
"boy said pe"
The character #\space is the space character. Other common characters that are trimmed are #\tab
and #\newline. There are also utility functions for making strings upper or lower case:
* (string-upcase "The dog bit the cat.")
"THE DOG BIT THE CAT."
* (string-downcase "The boy said WOW!")
"the boy said wow!"
We have not yet discussed equality of variables. The function eq returns true if two variables refer
to the same data in memory. The function eql returns true if the arguments refer to the same data
in memory or if they are equal numbers or characters. The function equal is more lenient: it returns
true if two variables print the same when evaluated. More formally, function equal returns true if
the car and cdr recursively equal to each other. An example will make this clearer:
Common Lisp Basics
* (defvar x '(1 2 3))
* (defvar y '(1 2 3))
* (eql x y)
* (equal x y)
(1 2 3)
(1 2 3)
For strings, the function string= is slightly more efficient than using the function equal:
* (eql "cat" "cat")
* (equal "cat" "cat")
* (string= "cat" "cat")
Common Lisp strings are sequences of characters. The function char is used to extract individual
characters from a string:
"the cat ran up the tree"
* (char s1 0)
* (char s1 1)
Using Hash Tables
Hash tables are an extremely useful data type. While it is true that you can get the same effect by
using lists and the assoc function, hash tables are much more efficient than lists if the lists contain
many elements. For example:
Common Lisp Basics
* (defvar x '((1 2) ("animal" "dog")))
* (assoc 1 x)
* (assoc "animal" x)
* (assoc "animal" x :test #'equal)
("animal" "dog")
The second argument to function assoc is a list of cons cells. Function assoc searches for a sub-list
(in the second argument) that has its car (i.e., first element) equal to the first argument to function
assoc. The perhaps surprising thing about this example is that assoc seems to work with an integer
as the first argument but not with a string. The reason for this is that by default the test for equality
is done with eql that tests two variables to see if they refer to the same memory location or if they
are identical if they are numbers. In the last call to assoc we used “:test #’equal” to make assoc use
the function equal to test for equality.
The problem with using lists and assoc is that they are very inefficient for large lists. We will see
that it is no more difficult to code with hash tables.
A hash table stores associations between key and value pairs, much like our last example using the
assoc function. By default, hash tables use eql to test for equality when looking for a key match. We
will duplicate the previous example using hash tables:
* (defvar h (make-hash-table))
* (setf (gethash 1 h) 2)
* (setf (gethash "animal" h) "dog")
* (gethash 1 h)
* (gethash "animal" h)
Notice that gethash returns multiple values: the first value is the value matching the key passed as
the first argument to function gethash and the second returned value is true if the key was found
and nil otherwise. The second returned value could be useful if hash values are nil.
Common Lisp Basics
Since we have not yet seen how to handle multiple returned values from a function, we will digress
and do so here (there are many ways to handle multiple return values and we are just covering one
of them):
* (multiple-value-setq (a b) (gethash 1 h))
Assuming that variables a and b are already declared, the variable a will be set to the first returned
value from gethash and the variable b will be set to the second returned value.
If we use symbols as hash table keys, then using eql for testing for equality with hash table keys is
* (setf (gethash 'bb h) 'aa)
* (gethash 'bb h)
However, we saw that eql will not match keys with character string values. The function make-
hash-table has optional key arguments and one of them will allow us to use strings as hash key
values:
(make-hash-table &key :test :size :rehash-size :rehash-threshold)
Here, we are only interested in the first optional key argument :test that allows us to use the function
equal to test for equality when matching hash table keys. For example:
Common Lisp Basics
* (defvar h2 (make-hash-table :test #'equal))
* (setf (gethash "animal" h2) "dog")
* (setf (gethash "parrot" h2) "Brady")
"Brady"
* (gethash "parrot" h2)
"Brady" ;
It is often useful to be able to enumerate all the key and value pairs in a hash table. Here is a simple
example of doing this by first defining a function my-print that takes two arguments, a key and a
value. We can then use the maphash function to call our new function my-print with every key
and value pair in a hash table:
* (defun my-print (a-key a-value)
(format t "key: ~A value: ~A~\%" a-key a-value))
MY-PRINT
* (maphash #'my-print h2)
key: parrot value: Brady
key: animal value: dog
The function my-print is applied to each key/value pair in the hash table. There are a few other
useful hash table functions that we demonstrate here:
* (hash-table-count h2)
* (remhash "animal" h2)
* (hash-table-count h2)
* (clrhash h2)
#S(HASH-TABLE EQUAL)
* (hash-table-count h2)
The function hash-table-count returns the number of key and value pairs in a hash table. The
function remhash can be used to remove a single key and value pair from a hash table. The function
clrhash clears out a hash table by removing all key and value pairs in a hash table.
Common Lisp Basics
It is interesting to note that clrhash and remhash are the first Common Lisp functions that we
have seen so far that modify any of its arguments, except for setq and setf that are macros and not
functions.
Using Eval to Evaluate Lisp Forms
We have seen how we can type arbitrary Lisp expressions in the Lisp repl listener and then they are
evaluated. We will see in the Chapter on Input and Output that the Lisp function read evaluates
lists (or forms) and indeed the Lisp repl uses function read.
In this section, we will use the function eval to evaluate arbitrary Lisp expressions inside a program.
As a simple example:
* (defvar x '(+ 1 2 3 4 5))
(+ 1 2 3 4 5)
* (eval x)
Using the function eval, we can build lists containing Lisp code and evaluate generated code inside
our own programs. We get the effect of “data is code”. A classic Lisp program, the OPS5 expert
system tool, stored snippets of Lisp code in a network data structure and used the function eval to
execute Lisp code stored in the network. A warning: the use of eval is likely to be inefficient in
non-compiled code. For efficiency, the OPS5 program contained its own version of eval that only
interpreted a subset of Lisp used in the network.
Using a Text Editor to Edit Lisp Source Files
I usually use Emacs, but we will briefly discuss the editor vi also. If you use vi (e.g., enter “vi
nested.lisp”) the first thing that you should do is to configure vi to indicate matching opening
parentheses whenever a closing parentheses is typed; you do this by typing “:set sm” after vi is
running.
If you choose to learn Emacs, enter the following in your .emacs file (or your _emacs file in your
home directory if you are running Windows):
Common Lisp Basics
(set-default 'auto-mode-alist
(append '(("\\.lisp$" . lisp-mode)
("\\.lsp$" . lisp-mode)
("\\.cl$" . lisp-mode))
auto-mode-alist))
Now, whenever you open a file with the extension of “lisp”, “lsp”, or “cl” (for “Common Lisp”) then
Emacs will automatically use a Lisp editing mode. I recommend searching the web using keywords
“Emacs tutorial” to learn how to use the basic Emacs editing commands - we will not repeat this
information here.
I do my professional Lisp programming using free software tools: Emacs, SBCL, Clozure Common
Lisp, and Clojure. I will show you how to configure Emacs and Slime in the last section of the
Chapter on Quicklisp.
Recovering from Errors
When you enter forms (or expressions) in a Lisp repl listener, you will occasionally make a mistake
and an error will be thrown. Here is an example where I am not showing all of the output when
entering help when an error is thrown:
* (defun my-add-one (x) (+ x 1))
MY-ADD-ONE
* (my-add-one 10)
* (my-add-one 3.14159)
4.14159
* (my-add-one "cat")
debugger invoked on a SIMPLE-TYPE-ERROR: Argument X is not a NUMBER: "cat"
Type HELP for debugger help, or (SB-EXT:EXIT) to exit from SBCL.
restarts (invokable by number or by possibly-abbreviated name):
0: [ABORT] Exit debugger, returning to top level.
(SB-KERNEL:TWO-ARG-+ "cat" 1)
0] help
Common Lisp Basics
The debug prompt is square brackets, with number(s) indicating the current
control stack level and, if you've entered the debugger recursively, how
deeply recursed you are.
Getting in and out of the debugger:
TOPLEVEL, TOP
exits debugger and returns to top level REPL
RESTART
invokes restart numbered as shown (prompt if not given).
prints the error condition and restart cases.
Inspecting frames:
BACKTRACE [n]
shows n frames going down the stack.
LIST-LOCALS, L lists locals in current frame.
PRINT, P
displays function call for current frame.
SOURCE [n]
displays frame's source form with n levels of enclosing forms.
Stepping:
START Selects the CONTINUE restart if one exists and starts
single-stepping. Single stepping affects only code compiled with
under high DEBUG optimization quality. See User Manual for details.
Steps into the current form.
Steps over the current form.
Stops stepping temporarily, but resumes it when the topmost frame that
was stepped into returns.
Stops single-stepping.
0] list-locals
SB-DEBUG::ARG-0
SB-DEBUG::ARG-1
0] backtrace 2
Backtrace for: #<SB-THREAD:THREAD "main thread" RUNNING {1002AC32F3}>
0: (SB-KERNEL:TWO-ARG-+ "cat" 1)
1: (MY-ADD-ONE "cat")
Common Lisp Basics
Here, I first used the backtrace command :bt to print the sequence of function calls that caused the
error. If it is obvious where the error is in the code that I am working on then I do not bother using
the backtrace command. I then used the abort command :a to recover back to the top level Lisp
listener (i.e., back to the greater than prompt). Sometimes, you must type :a more than once to fully
recover to the top level greater than prompt.
Garbage Collection
Like other languages like Java and Python, Common Lisp provides garbage collection (GC) or
automatic memory management.
In simple terms, GC occurs to free memory in a Lisp environment that is no longer accessible by any
global variable (or function closure, which we will cover in the next chapter). If a global variable
*variable-1* is first set to a list and then if we later then set *variable-1* to, for example nil, and if
the data referenced in the original list is not referenced by any other accessible data, then this now
unused data is subject to GC.
In practice, memory for Lisp data is allocated in time ordered batches and ephemeral or generational
garbage collectors garbage collect recent memory allocations far more often than memory that has
been allocated for a longer period of time.
Loading your Working Environment Quickly
When you start using Common Lisp for large projects, you will likely have many files to load
into your Lisp environment when you start working. Most Common Lisp implementations have
a function called defsystem that works somewhat like the Unix make utility. While I strongly
recommend defsystem for large multi-person projects, I usually use a simpler scheme when working
on my own: I place a file loadit.lisp in the top directory of each project that I work on. For any project,
its loadit.lisp file loads all source files and initializes any global data for the project.
The last two chapters of this book provide example applications that are configured to work with
Quicklisp, which we will study in the next chapter.
Another good technique is to create a Lisp image containing all the code and data for all your projects.
There is an example of this in the first section of the Chapter on NLP. In this example, it takes a few
minutes to load the code and data for my NLP (natural language processing) library so when I am
working with it I like to be able to quickly load a SBCL Lisp image.
All Common Lisp implementations have a mechanism for dumping a working image containing
code and data.
Common Lisp Basics
Functional Programming Concepts
There are two main styles for doing Common Lisp development. Object oriented programming is
well supported (see the Chapter on CLOS) as is functional programming. In a nut shell, functional
programming means that we should write functions with no side effects. First let me give you a
non-functional example with side effects:
(defun non-functional-example (car)
(set-color car "red"))
This example using CLOS is non-functional because we modify the value of an argument to
the function. Some functional languages like the Lisp Clojure language and the Haskell language
dissuade you from modifying arguments to functions. With Common Lisp you should make a
decision on which approach you like to use.
Functional programming means that we avoid maintaining state inside of functions and treat data
as immutable (i.e., once an object is created, it is never modified). We could modify the last example
to be function by creating a new car object inside the function, copy the attributes of the car passed
as an object, change the color to “red” of the new car object, and return the new car instance as the
value of the function.
Functional programming prevents many types of programming errors, makes unit testing simpler,
and makes programming for modern multi-core CPUs easier because read-only objects are inher-
ently thread safe. Modern best practices for the Java language also prefer immutable data objects
and a functional approach.
Quicklisp
For several decades managing packages and libraries was a manual process when developing Lisp
systems. I used to package the source code for specific versions of libraries as part of my Common
Lisp projects. Early package management systems mk-defsystem and ASDF were very useful, but I
did not totally give up my practice keeping third party library source code with my projects until
Zach Beane created the Quicklisp package system²⁹. You will need to have Quicklisp installed for
many of the examples later in this book so please take the time to install it now as per the instructions
on the Quicklisp web site.
Using Quicklisp to Find Packages
We will need the Common Lisp Hunchentoot library later in the Chapter on Network Programming
so we will install it now using Quicklisp as an example for getting started with Quicklisp.
We already know the package name we want, but as an example of discovering packages let’s start
by using Quicklisp to search for all packages with “hunchentoot” in the package name:
* (ql:system-apropos "hunchentoot")
#<SYSTEM clack-handler-hunchentoot / clack-20131111-git / quicklisp 2013-11-11>
#<SYSTEM hunchentoot / hunchentoot-1.2.21 / quicklisp 2013-11-11>
#<SYSTEM hunchentoot-auth / hunchentoot-auth-20101107-git / quicklisp 2013-11-11>
#<SYSTEM hunchentoot-cgi / hunchentoot-cgi-20121125-git / quicklisp 2013-11-11>
#<SYSTEM hunchentoot-dev / hunchentoot-1.2.21 / quicklisp 2013-11-11>
#<SYSTEM hunchentoot-single-signon / hunchentoot-single-signon-20131111-git / quickl\
isp 2013-11-11>
#<SYSTEM hunchentoot-test / hunchentoot-1.2.21 / quicklisp 2013-11-11>
#<SYSTEM hunchentoot-vhost / hunchentoot-vhost-20110418-git / quicklisp 2013-11-11>
We want the base package seen in line 3 and we can install the base package as seen in the following
example:
²⁹http://www.quicklisp.org/
Quicklisp
* (ql:quickload :hunchentoot)
To load "hunchentoot":
Load 1 ASDF system:
hunchentoot
; Loading "hunchentoot"
.......
(:HUNCHENTOOT)
In line 1, I refer to the package name using a symbol :hunchentoot but using the string “hunchentoot”
would have worked the same. The first time you ql:quickload a library you may see additional
printout and it takes longer to load because the source code is downloaded from the web and cached
locally in the directory ∼/quicklisp/local-projects. In most of the rest of this book, when I install
or use a package by calling the ql:quickload function I do not show the output from this function
in the repl listings.
Now, we can use the fantastically useful Common Lisp function apropos to see what was just
installed:
* (apropos "hunchentoot")
HUNCHENTOOT::*CLOSE-HUNCHENTOOT-STREAM* (bound)
HUNCHENTOOT:*HUNCHENTOOT-DEFAULT-EXTERNAL-FORMAT* (bound)
HUNCHENTOOT::*HUNCHENTOOT-STREAM*
HUNCHENTOOT:*HUNCHENTOOT-VERSION* (bound)
HUNCHENTOOT:HUNCHENTOOT-CONDITION
HUNCHENTOOT:HUNCHENTOOT-ERROR (fbound)
HUNCHENTOOT::HUNCHENTOOT-OPERATION-NOT-IMPLEMENTED-OPERATION (fbound)
HUNCHENTOOT::HUNCHENTOOT-SIMPLE-ERROR
HUNCHENTOOT::HUNCHENTOOT-SIMPLE-WARNING
HUNCHENTOOT::HUNCHENTOOT-WARN (fbound)
HUNCHENTOOT:HUNCHENTOOT-WARNING
HUNCHENTOOT-ASD:*HUNCHENTOOT-VERSION* (bound)
HUNCHENTOOT-ASD::HUNCHENTOOT
:HUNCHENTOOT (bound)
:HUNCHENTOOT-ASD (bound)
:HUNCHENTOOT-DEV (bound)
:HUNCHENTOOT-NO-SSL (bound)
:HUNCHENTOOT-TEST (bound)
:HUNCHENTOOT-VERSION (bound)
As long as you are thinking about the new tool Quicklisp that is now in your tool chest, you should
install most of the packages and libraries that you will need for working through the rest of this
Quicklisp
book. I will show the statements needed to load more libraries without showing the output printed
in the repl as each package is loaded:
(ql:quickload "clsql")
(ql:quickload "clsql-postgresql")
(ql:quickload "clsql-mysql")
(ql:quickload "clsql-sqlite3")
(ql:quickload :drakma)
(ql:quickload :hunchentoot)
(ql:quickload :cl-json)
(ql:quickload "clouchdb")
;; for CouchDB access
(ql:quickload "sqlite")
You need to have the Postgres and MySQL client developer libraries installed on your system for
the clsql-postgresql and clsql-mysql installations to work. If you are unlikely to use relational
databases with Common Lisp then you might skip the effort of installing Postgres and MySQL. The
example in the Chapter on the Knowledge Graph Navigator uses the SQLite database for caching.
You don’t need any extra dependencies for the sqlite package.
Using Quicklisp to Configure Emacs and Slime
I assume that you have Emacs installed on your system. In a repl you can setup the Slime package
that allows Emacs to connect to a running Lisp environment:
(ql:quickload "quicklisp-slime-helper")
Pay attention to the output in the repl. On my system the output contained the following:
[package quicklisp-slime-helper]
slime-helper.el installed in "/Users/markw/quicklisp/slime-helper.el"
To use, add this to your ~/.emacs:
(load (expand-file-name "~/quicklisp/slime-helper.el"))
;; Replace "sbcl" with the path to your implementation
(setq inferior-lisp-program "sbcl")
If you installed rlwrap and defined an alias for running SBCL, make sure you set the inferior lisp
program to the absolute path of the SBCL executable; on my system I set the following in my .emacs
Quicklisp
(setq inferior-lisp-program "/Users/markw/sbcl/sbcl")
I am not going to cover using Emacs and Slime, there are many good tutorials on the web you can
In later chapters we will write libraries and applications as Quicklisp projects so that you will be
able to load your own libraries, making it easier to write small libraries that you can compose into
larger applications.
Defining Lisp Functions
In the previous chapter, we defined a few simple functions. In this chapter, we will discuss how
to write functions that take a variable number of arguments, optional arguments, and keyword
arguments.
The special form defun is used to define new functions either in Lisp source files or at the top level
Lisp listener prompt. Usually, it is most convenient to place function definitions in a source file and
use the function load to load them into our Lisp working environment.
In general, it is bad form to use global variables inside Lisp functions. Rather, we prefer to pass all
required data into a function via its argument list and to get the results of the function as the value
(or values) returned from a function. Note that if we do require global variables, it is customary to
name them with beginning and ending * characters; for example:
(defvar *lexical-hash-table*
(make-hash-table :test #'equal :size 5000))
Then in this example, if you see the variable *lexical-hash-table* inside a function definition, you
will know that at least by naming convention, that this is a global variable.
In Chapter 1, we saw an example of using lexically scoped local variables inside a function definition
(in the example file nested.lisp).
There are several options for defining the arguments that a function can take. The fastest way to
introduce the various options is with a few examples.
First, we can use the &aux keyword to declare local variables for use in a function definition:
* (defun test (x &aux y)
(setq y (list x x))
* (test 'cat)
(CAT CAT)
* (test 3.14159)
(3.14159 3.14159)
It is considered better coding style to use the let special operator for defining auxiliary local variables;
for example:
Defining Lisp Functions
* (defun test (x)
(let ((y (list x x)))
* (test "the dog bit the cat")
("the dog bit the cat" "the dog bit the cat")
You will probably not use &aux very often, but there are two other options for specifying function
arguments: &optional and &key.
The following code example shows how to use optional function arguments. Note that optional
arguments must occur after required arguments.
* (defun test (a &optional b (c 123))
(format t "a=~A b=~A c=~A~%" a b c))
* (test 1)
a=1 b=NIL c=123
* (test 1 2)
a=1 b=2 c=123
* (test 1 2 3)
a=1 b=2 c=3
* (test 1 2 "Italian Greyhound")
a=1 b=2 c=Italian Greyhound
In this example, the optional argument b was not given a default value so if unspecified it will default
to nil. The optional argument c is given a default value of 123.
We have already seen the use of keyword arguments in built-in Lisp functions. Here is an example
of how to specify key word arguments in your functions:
Defining Lisp Functions
* (defun test (a &key b c)
(format t "a=~A b=~A c=~A~%" a b c))
* (test 1)
a=1 b=NIL c=NIL
* (test 1 :c 3.14159)
a=1 b=NIL c=3.14159
* (test "cat" :b "dog")
a=cat b=dog c=NIL
Using Lambda Forms
It is often useful to define unnamed functions. We can define an unnamed function using lambda;
for example, let’s look at the example file src/lambda1.lisp. But first, we will introduce the Common
Lisp function funcall that takes one or more arguments; the first argument is a function and any
remaining arguments are passed to the function bound to the first argument. For example:
* (funcall 'print 'cat)
* (funcall '+ 1 2)
* (funcall #'- 2 3)
In the first two calls to funcall here, we simply quote the function name that we want to call. In
the third example, we use a better notation by quoting with #’. We use the #’ characters to quote a
function name.
Consider the following repl listing where we will look at a primary difference between quoting a
symbol using ‘ and with #’:
Defining Lisp Functions
Clozure Common Lisp Version 1.12
DarwinX8664
? 'barfoo531
BARFOO531
? (apropos "barfoo")
BARFOO531
? #'bar987
> Error: Undefined function: BAR987
On line three we create a new symbol BARFOO531 that is interned as you can see from looking
at all interned symbols containing the string “barfoo”. Line 7 throws an error because #’ does not
intern a new symbol.
Here is the example file src/lambda1.lisp:
(defun test ()
(let ((my-func
(lambda (x) (+ x 1))))
(funcall my-func 1)))
Here, we define a function using lambda and set the value of the local variable my-func to the
unnamed function’s value. Here is output from the function test:
* (test)
The ability to use functions as data is surprisingly useful. For now, we will look at a simple example:
* (defvar f1 #'(lambda (x) (+ x 1)))
* (funcall f1 100)
* (funcall #'print 100)
Notice that the second call to function testfn prints “100” twice: the first time as a side effect of
calling the function print and the second time as the returned value of testfn (the function print
returns what it is printing as its value).
Defining Lisp Functions
Using Recursion
Later, we will see how to use special Common Lisp macros for programming repetitive loops. In this
section, we will use recursion for both coding simple loops and as an effective way to solve a variety
of problems that can be expressed naturally using recursion.
As usual, the example programs for this section are found in the src directory. In the file
src/recursion1.lisp, we see our first example of recursion:
;; a simple loop using recursion
(defun recursion1 (value)
(format t "entering recursion1(~A)~\%" value)
(if (< value 5)
(recursion1 (1+ value))))
This example is simple, but it is useful for discussing a few points. First, notice how the function
recursion1 calls itself with an argument value of one greater than its own input argument only if
the input argument “value” is less than 5. This test keeps the function from getting in an infinite
loop. Here is some sample output:
* (load "recursion1.lisp")
;; Loading file recursion1.lisp ...
;; Loading of file recursion1.lisp is finished.
* (recursion1 0)
entering recursion1(0)
entering recursion1(1)
entering recursion1(2)
entering recursion1(3)
entering recursion1(4)
entering recursion1(5)
* (recursion1 -3)
entering recursion1(-3)
entering recursion1(-2)
entering recursion1(-1)
entering recursion1(0)
entering recursion1(1)
entering recursion1(2)
entering recursion1(3)
entering recursion1(4)
Defining Lisp Functions
entering recursion1(5)
* (recursion1 20)
entering recursion1(20)
Why did the call on line 24 not loop via recursion? Because the input argument is not less than 5, no
recursion occurs.
Closures
We have seen that functions can take other functions as arguments and return new functions as
values. A function that references an outer lexically scoped variable is called a closure. The example
file src/closure1.lisp contains a simple example:
(let* ((fortunes
'("You will become a great Lisp Programmer"
"The force will not be with you"
"Take time for meditation"))
(len (length fortunes))
(index 0))
(defun fortune ()
(let ((new-fortune (nth index fortunes)))
(setq index (1+ index))
(if (>= index len) (setq index 0))
new-fortune)))
Here the function fortune is defined inside a let form. Because the local variable fortunes is
referenced inside the function fortune, the variable fortunes exists after the let form is evaluated. It
is important to understand that usually a local variable defined inside a let form “goes out of scope”
and can no longer be referenced after the let form is evaluated.
However, in this example, there is no way to access the contents of the variable fortunes except by
calling the function fortune. At a minimum, closures are a great way to hide variables. Here is some
output from loading the src/closure1.lisp file and calling the function fortune several times:
Defining Lisp Functions
* (load "closure1.lisp")
;; Loading file closure1.lisp ...
;; Loading of file closure1.lisp is finished.
* (fortune)
"You will become a great Lisp Programmer"
* (fortune)
"The force will not be with you"
* (fortune)
"Take time for meditation"
* (fortune)
"You will become a great Lisp Programmer"
Using the Function eval
In Lisp languages we often say that code is data. The function eval can be used to execute code that
is stored as Lisp data. Let’s look at an example:
Clozure Common Lisp Version 1.12
DarwinX8664
? '(+ 1 2.2)
(+ 1 2.2)
? (eval '(+ 1 2.2))
? (eval '(defun foo2 (x) (+ x x)))
? (foo2 4)
I leave it up to you, dear reader, how often you are motivated to use eval. In forty years of using
Lisp languages my principle use of eval has been in modifying the standard version of the Ops5
programming language for production systems³⁰ to support things like multiple data worlds and
new actions to spawn off new data worlds and to remove them. Ops5 works by finding common
expressions in a set of production rules (also referred to as “expert systems”) and factoring them into
a network (a Rete network if you want to look it up) with common expressions in rules stored in
just a single place. eval is used a lot in Ops5 and I used it for my extensions to Ops5.
³⁰https://github.com/sharplispers/ops5
Defining Common Lisp Macros
We saw in the last chapter how the Lisp function eval could be used to evaluate arbitrary Lisp code
stored in lists. Because eval is inefficient, a better way to generate Lisp code automatically is to define
macro expressions that are expanded inline when they are used. In most Common Lisp systems,
using eval requires the Lisp compiler to compile a form on-the-fly which is not very efficient. Some
Lisp implementations use an interpreter for eval which is likely to be faster but might lead to obscure
bugs if the interpreter and compiled code do not function identically.
The ability to add functionality and syntax to the Common Lisp language, to in effect extend the
language as needed, is truly a super power of languages like Common Lisp and Scheme.
Example Macro
The file src/macro1.lisp contains both a simple macro and a function that uses the macro. This
macro example is a bit contrived since it could be just a function definition, but it does show the
process of creating and using a macro. We are using the gensym function to define a new unique
symbol to reference a temporary variable:
;; first simple macro example:
(defmacro double-list (a-list)
(let ((ret (gensym)))
`(let ((,ret nil))
(dolist (x ,a-list)
(setq ,ret (append ,ret (list x x))))
,ret)))
;; use the macro:
(defun test (x)
(double-list x))
The backquote character seen at the beginning of line 5 is used to quote a list in a special way:
nothing in the list is evaluated during macro expansion unless it is immediately preceded by a
comma character. In this case, we specify ,a-list because we want the value of the macro’s argument
a-list to be substituted into the specially quoted list. We will look at dolist in some detail in the next
chapter but for now it is sufficient to understand that dolist is used to iterate through the top-level
elements of a list, for example:
Defining Common Lisp Macros
* (dolist (x '("the" "cat" "bit" "the" "rat"))
(print x))
Notice that the example macro double-list itself uses the macro dolist. It is common to nest macros
in the same way functions can be nested.
Returning to our macro example in the file src/macro1.lisp, we will try the function test that uses
the macro double-list:
* (load "macro1.lisp")
;; Loading file macro1.lisp ...
;; Loading of file macro1.lisp is finished.
* (test '(1 2 3))
(1 1 2 2 3 3)
Using the Splicing Operator
Another similar example is in the file src/macro2.lisp:
;; another macro example that uses ,@:
(defmacro double-args (&rest args)
`(let ((ret nil))
(dolist (x ,@args)
(setq ret (append ret (list x x))))
;; use the macro:
(defun test (&rest x)
(double-args x))
Here, the splicing operator ,@ is used to substitute in the list args in the macro double-args.
Defining Common Lisp Macros
Using macroexpand-1
The function macroexpand-1 is used to transform macros with arguments into new Lisp expressions.
For example:
* (defmacro double (a-number)
(list '+ a-number a-number))
DOUBLE
* (macroexpand-1 '(double n))
(+ N N) ;
Writing macros is an effective way to extend the Lisp language because you can control the code
passed to the Common Lisp compiler. In both macro example files, when the function test was
defined, the macro expansion is done before the compiler processes the code. We will see in the next
chapter several useful macros included in Common Lisp.
We have only “scratched the surface” looking at macros; the interested reader is encouraged to
search the web using, for example, “Common Lisp macros.” There are two books in particular that
I recommend that take a deep dive into Common Lisp macros: Paul Graham’s “On Lisp” and Doug
Hoyte’s “Let Over Lambda.” Both are deep books and will change the way you experience software
development. A good plan of study is spending a year absorbing “On Lisp” before tackling “Let Over
Lambda.”
Using Common Lisp Loop Macros
In this chapter, we will discuss several useful macros for performing iteration (we saw how to use
recursion for iteration in Chapter 2):
• dolist – a simple way to process the elements of a list
• dotimes – a simple way to iterate with an integer valued loop variable
• do – the most general looping macro
• loop – a complex looping macro that I almost never use in my own code because it does not
look “Lisp like.” I don’t use the loop macro in this book. Many programmers do like the loop
macro so you are likely to see it when reading other people’s code.
dolist
We saw a quick example of dolist in the last chapter. The arguments of the dolist macro are:
(dolist (a-variable a-list [optional-result-value])
...body... )
Usually, the dolist macro returns nil as its value, but we can add a third optional argument which
will be returned as the generated expression’s value; for example:
* (dolist (a '(1 2) 'done) (print a))
* (dolist (a '(1 2)) (print a))
The first argument to the dolist macro is a local lexically scoped variable. Once the code generated
by the dolist macro finishes executing, this variable is undefined.
dotimes
The dotimes macro is used when you need a loop with an integer loop index. The arguments of the
dotimes macro are:
Using Common Lisp Loop Macros
(dotimes (an-index-variable max-index-plus-one [optional-result-value])
...body... )
Usually, the dotimes macro returns nil as its value, but we can add a third optional argument that
will be returned as the generated expression’s value; for example:
* (dotimes (i 3 "all-done-with-test-dotimes-loop") (print i))
"all-done-with-test-dotimes-loop"
As with the dolist macro, you will often use a let form inside a dotimes macro to declare additional
temporary (lexical) variables.
The do macro is more general purpose than either dotimes or dolist but it is more complicated to
use. Here is the general form for using the do looping macro:
(do ((variable-1 variable-1-init-value variable-1-update-expression)
(variable-2 variable-2-init-value variable-2-update-expression)
(variable-N variable-N-init-value variable-N-update-expression))
(loop-termination-test
loop-return-value)
optional-variable-declarations
expressions-to-be-executed-inside-the-loop)
There is a similar macro do* that is analogous to let* in that loop variable values can depend on the
values or previously declared loop variable values.
As a simple example, here is a loop to print out the integers from 0 to 3. This example is in the file
src/do1.lisp:
;; example do macro use
Using Common Lisp Loop Macros
(do ((i 0 (1+ i)))
((> i 3) "value-of-do-loop")
(print i))
In this example, we only declare one loop variable so we might as well as used the simpler dotimes
macro.
Here we load the file src/do1.lisp:
* (load "do1.lisp")
;; Loading file do1.lisp ...
;; Loading of file do1.lisp is finished.
You will notice that we do not see the return value of the do loop (i.e., the string “value-of-do-loop”)
because the top-level form that we are evaluating is a call to the function load; we do see the return
value of load printed. If we had manually typed this example loop in the Lisp listener, then you
would see the final value value-of-do-loop printed.
Using the loop Special Form to Iterate Over Vectors or
Arrays
We previousely used dolist to iterate over elements in lists. For efficiency we will often use vectors
(one dimensional arrays) and we can use loop to similarly handle vectors:
(loop for td across testdata
(print td))))
where testdata is a one dimensional array (a vector) and inside the do block the local variable td is
assigned to each element in the vector.
Common Lisp Package System
In later chapters we will see two complete applications that are defined as Quicklisp projects: the
chapter on the Knowledge Graph Creator and the chapter on the Knowledge Graph Navigator.
Another example for setting up a Quicklib project can be seen in the chapter Plotting Data.
While these later chapters provide practical examples for bundling up your own projects in packages,
the material here will give you general background information that you should know.
In the simple examples that we have seen so far, all newly created Lisp symbols have been placed in
the default package. You can always check the current package by evaluating the expression package:
> *package*
#<PACKAGE COMMON-LISP-USER>
As we will use in the following example, the package :cl is an alias for :common-lisp-user.
We will define a new package :my-new-package and two functions foo1 and foo2 inside the
package. Externally to this package, assuming that it is loaded, we can access foo2 using my-new-
package:foo2. foo1 is not exported so it cannot be accessed this way. However, we can always start
a symbol name with a package name and two colon characters if we want to use a symbol defined
in another package so we can use my-new-package::foo1. Using :: allows us access to symbols not
explicitly exported.
When I leave package :my-new-package in line 22 and return to package :cl, and try to access
my-new-package:foo1 notice that an error is thrown.
On line 3 we define the alias :p1 for the package :my-new-package and we use this alias in line
44. The main point of the following example is that we define two functions in a package but only
export one of these functions. By default the other function is not visible outside of the new package.
* (defpackage "MY-NEW-PACKAGE"
(:use :cl)
(:nicknames "P1")
(:export :FOO2))
#<PACKAGE "MY-NEW-PACKAGE">
* (in-package my-new-package)
#<PACKAGE "MY-NEW-PACKAGE">
* (defun foo1 () "foo1")
Common Lisp Package System
* (defun foo2 () "foo2")
* (foo1)
"foo1"
* (foo2)
"foo2"
* (in-package :cl)
#<PACKAGE "COMMON-LISP">
* (my-new-package:foo2)
"foo2"
* (my-new-package:foo1)
debugger invoked on a SB-INT:SIMPLE-READER-PACKAGE-ERROR in thread
#<THREAD "main thread" RUNNING {1001F1ECE3}>:
The symbol "FOO1" is not external in the MY-NEW-PACKAGE package.
Stream: #<SYNONYM-STREAM :SYMBOL SB-SYS:*STDIN* {100001C343}>
Type HELP for debugger help, or (SB-EXT:EXIT) to exit from SBCL.
restarts (invokable by number or by possibly-abbreviated name):
0: [CONTINUE] Use symbol anyway.
1: [ABORT
] Exit debugger, returning to top level.
* (p1:foo2)
"foo2"
Since we specified a nickname in the defpackage expression, Common Lisp allows the use of the
nickname (in this case P1) in calling function foo2 that is exported from package :my-new-package.
Near the end of the last example, we switched back to the default package COMMON-LISP-USER
so we had to specify the package name for the function foo2 on line 42.
What about the error on line 28 where my-new-package:foo1 is undefined because the function
Common Lisp Package System
foo1 is not exported (see line 4)? It turns out that you can easily use symbols not exported from a
package by using :: instead of a single :. Here, this would be defined: (my-new-package::foo1).
When you are writing very large Common Lisp programs, it is useful to be able to break up
the program into different modules and place each module and all its required data in different
name spaces by creating new packages. Remember that all symbols, including variables, generated
symbols, CLOS methods, functions, and macros are in some package.
For small packages I sometimes put a defpackage expression at the top of the file immediately
followed by an in-package expression to switch to the new package. In the general case, please
properly use separate project and asdf files as I do in the later chapters Knowledge Graph Creator
and Knowledge Graph Navigator.
Input and Output
We will see that the input and output of Lisp data is handled using streams. Streams are powerful
abstractions that support common libraries of functions for writing to the terminal, files, sockets,
and to strings.
In all cases, if an input or output function is called without specifying a stream, the default for
input stream is *standard-input* and the default for output stream is *standard-output*. These
default streams are connected to the Lisp listener that we discussed in Chapter 2. In the later chapter
Knowledge Graph Navigator that supports a user interface, we will again use output streams bound
to different scrolling output areas of the application window to write color-hilighted text. The stream
formalism is general purpose, covering many common I/O use cases.
The Lisp read and read-line Functions
The function read is used to read one Lisp expression. Function read stops reading after reading
one expression and ignores new line characters. We will look at a simple example of reading a file
test.dat using the example Lisp program in the file read-test-1.lisp. Both of these files can be found
in the directory src/code_snippets_for_book that came bundled with this web book. Start your
Lisp program in the src directory. The contents of the file test.dat is:
4 "the cat bit the rat"
read with-open-file
In the function read-test-1, we use the macro with-open-file to read from a file. To write to a file
(which we will do later), we can use the keyword arguments :direction :output. The first argument
to the macro with-open-file is a symbol that is bound to a newly created input stream (or an output
stream if we are writing a file); this symbol can then be used in calling any function that expects a
stream argument.
Notice that we call the function read with three arguments: an input stream, a flag to indicate if
an error should be thrown if there is an I/O error (e.g., reaching the end of a file), and the third
argument is the value that function read should return if the end of the file (or stream) is reached.
When calling read with these three arguments, either the next expression from the file test.dat will
be returned, or the value nil will be returned when the end of the file is reached. If we do reach the
end of the file, the local variable x will be assigned the value nil and the function return will break
out of the dotimes loop. One big advantage of using the macro with-open-file over using the open
function (which we will not cover) is that the file stream is automatically closed when leaving the
code generated by the with-open-file macro. The contents of file read-test-1.lisp is:
Input and Output
(defun read-test-1 ()
"read a maximum of 1000 expressions from the file 'test.dat'"
(with-open-file
(input-stream "test.dat" :direction :input)
(dotimes (i 1000)
(let ((x (read input-stream nil nil)))
(if (null x) (return)) ;; break out of the 'dotimes' loop
(format t "next expression in file: ~S~%" x)))))
Here is the output that you will see if you load the file read-test-1.lisp and execute the expression
(read-test-1):
* (load "read-test-1.lisp")
;; Loading file read-test-1.lisp ...
;; Loading of file read-test-1.lisp is finished.
* (read-test-1)
next expression in file: 1
next expression in file: 2
next expression in file: 3
next expression in file: 4
next expression in file: "the cat bit the rat"
Note: the string “the cat bit the rat” prints as a string (with quotes) because we used a ∼S instead of
a ∼A in the format string in the call to function format.
In this last example, we passed the file name as a string to the macro with-open-file. This is not
generally portable across all operating systems. Instead, we could have created a pathname object
and passed that instead. The pathname function can take eight different keyword arguments, but
we will use only the two most common in the example in the file read-test-2.lisp in the src directory.
The following listing shows just the differences between this example and the last:
(let ((a-path-name
(make-pathname :directory "testdata"
:name "test.dat")))
(with-open-file
(input-stream a-path-name :direction :input)
Here, we are specifying that we want to use the file test.dat in the subdirectory testdata. Note: I
almost never use pathnames. Instead, I specify files using a string and the character / as a directory
delimiter. I find this to be portable for the Macintosh, Windows, and Linux operating systems using
all Common Lisp implementations.
Input and Output
The file readline-test.lisp is identical to the file read-test-1.lisp except that we call function
readline instead of the function read and we change the output format message to indicate that
an entire line of text has been read
(defun readline-test ()
"read a maximum of 1000 expressions from the file 'test.dat'"
(with-open-file
(input-stream "test.dat" :direction :input)
(dotimes (i 1000)
(let ((x (read-line input-stream nil nil)))
(if (null x) (return)) ;; break out of the 'dotimes' loop
(format t "next line in file: ~S~%" x)))))
When we execute the expression (readline-test), notice that the string contained in the second line
of the input file has the quote characters escaped:
* (load "readline-test.lisp")
;; Loading file readline-test.lisp ...
;; Loading of file readline-test.lisp is finished.
* (readline-test)
next line in file: "1 2 3"
next line in file: "4 \"the cat bit the rat\""
We can also create an input stream from the contents of a string. The file read-from-string-test.lisp
is very similar to the example file read-test-1.lisp except that we use the macro with-input-from-
string (notice how I escaped the quote characters used inside the test string):
(defun read-from-string-test ()
"read a maximum of 1000 expressions from a string"
(let ((str "1 2 \"My parrot is named Brady.\" (11 22)"))
(with-input-from-string
(input-stream str)
(dotimes (i 1000)
(let ((x (read input-stream nil nil)))
(if (null x) (return)) ;; break out of the 'dotimes' loop
(format t "next expression in string: ~S~%" x))))))
We see the following output when we load the file read-from-string-test.lisp:
Input and Output
* (load "read-from-string-test.lisp")
;; Loading file read-from-string-test.lisp ...
;; Loading of file read-from-string-test.lisp is finished.
* (read-from-string-test)
next expression in string: 1
next expression in string: 2
next expression in string: "My parrot is named Brady."
next expression in string: (11 22)
We have seen how the stream abstraction is useful for allowing the same operations on a variety
of stream data. In the next section, we will see that this generality also applies to the Lisp printing
functions.
Lisp Printing Functions
All of the printing functions that we will look at in this section take an optional last argument that
is an output stream. The exception is the format function that can take a stream value as its first
argument (or t to indicate *standard-output*, or a nil value to indicate that format should return
a string value).
Here is an example of specifying the optional stream argument:
* (print "testing")
"testing"
"testing"
* (print "testing" *standard-output*)
"testing"
"testing"
The function print prints Lisp objects so that they can be read back using function read. The
corresponding function princ is used to print for “human consumption”. For example:
Input and Output
* (print "testing")
"testing"
"testing"
* (princ "testing")
testing
"testing"
Both print and princ return their first argument as their return value, which you see in the previous
output. Notice that princ also does not print a new line character, so princ is often used with terpri
(which also takes an optional stream argument).
We have also seen many examples in this book of using the format function. Here is a different use
of format, building a string by specifying the value nil for the first argument:
* (let ((l1 '(1 2))
(x 3.14159))
(format nil "~A~A" l1 x))
"(1 2)3.14159"
We have not yet seen an example of writing to a file. Here, we will use the with-open-file macro
with options to write a file and to delete any existing file with the same name:
(with-open-file (out-stream "test1.dat"
:direction :output
:if-exists :supersede)
(print "the cat ran down the road" out-stream)
(format out-stream "1 + 2 is: ~A~%" (+ 1 2))
(princ "Stoking!!" out-stream)
(terpri out-stream))
Here is the result of evaluating this expression (i.e., the contents of the newly created file test1.dat
in the src directory):
% cat test1.dat
"the cat ran down the road" 1 + 2 is: 3
Stoking!!
Notice that print generates a new line character before printing its argument.
Plotting Data
We will use Zach Beane’s vecto library³¹ for plotting data with the results written to files. Ideally we
would like to have interactive plotting capability but for the purposes of this book I need to support
the combinations of all Common Lisp implementations on multiple operating systems. Interactive
plotting libraries are usually implementation and OS dependent. We will use the plotlib example
we develop in the later chapter Backpropagation Neural Networks.
Implementing the Library
The examples here are all contained in the directory src/plotlib and is packaged as a Quicklisp
loadable library. This library will be used in later chapters.
When I work on my macOS laptop, I leave the output graphics file open in the Preview App and
whenever I rerun a program producing graphics in the REPL, making the preview App window
active refreshes the graphics display.
PNG file generated by running plotlib test
The following listing shows the file plotlib.lisp that is a simple wrapper for the vecto Common Lisp
plotting library. Please note that I only implemented wrappers for vecto functionality that I need
for later examples in this book, so the following code is not particularly general but should be easy
enough for you to extend for the specific needs of your projects.
³¹http://xach.com/lisp/vecto/
Plotting Data
;; Misc. plotting examples using the vecto library
(ql:quickload :vecto) ;; Zach Beane's plotting library
(defpackage #:plotlib
(:use #:cl #:vecto))
(in-package #:plotlib)
;; the coordinate (0,0) is the lower left corner of the plotting area.
;; Increasing the y coordinate is "up page" and increasing x is "to the right"
;; fills a rectangle with a gray-scale value
(defun plot-fill-rect (x y width height gscale) ; 0 < gscale < 1
(set-rgb-fill gscale gscale gscale)
(rectangle x y width height)
(fill-path))
;; plots a frame rectangle
(defun plot-frame-rect (x y width height)
(set-line-width 1)
(set-rgb-fill 1 1 1)
(rectangle x y width height)
(stroke))
(defun plot-line(x1 y1 x2 y2)
(set-line-width 1)
(set-rgb-fill 0 0 0)
(move-to x1 y1)
(line-to x2 y2)
(stroke))
(defun plot-string(x y str)
(let ((font (get-font "OpenSans-Regular.ttf")))
(set-font font 15)
(set-rgb-fill 0 0 0)
(draw-string x y str)))
(defun plot-string-bold(x y str)
(let ((font (get-font "OpenSans-Bold.ttf")))
(set-font font 15)
(set-rgb-fill 0 0 0)
(draw-string x y str)))
Plotting Data
(defun test-plotlib (file)
(with-canvas (:width 90 :height 90)
(plot-fill-rect 5 10 15 30 0.2) ; black
(plot-fill-rect 25 30 30 7 0.7) ; medium gray
(plot-frame-rect 10 50 30 7)
(plot-line 90 5 10 5)
(plot-string 10 65 "test 1 2 3")
(plot-string-bold 10 78 "Hello")
(save-png file)))
;;(test-plotlib "test-plotlib.png")
This plot library is used in later examples in the chapters on search, backpropagation neural networks
and Hopfield neural networks. I prefer using implementation and operating specific plotting libraires
for generating interactive plots, but the advantage of writing plot data to a file using the vecto library
is that the code is portable across operating systems and Common Lisp implementations.
Packaging as a Quicklisp Project
The two files src/plotlib/plotlib.asd src/plotlib/package.lisp configure the library. The file pack-
age.lisp defines the required library vecto and lists the functions that are publicly exported from
the library:
(defpackage #:plotlib
(:use #:cl #:vecto)
(:export save-png plot-fill-rect plot-frame-rect
plot-size-rect plot-line plot-string plot-string-bold
pen-width))
To run the test function provided with this library you load the library and preface exported function
names with the package name plotlib: as in this example:
(ql:quickload "plotlib")
(plotlib::test-plotlib "test-plotlib.png")
In addition to a package.lisp file we also use a file with the extension .asd
Plotting Data
(asdf:defsystem #:plotlib
:description "Describe plotlib here"
:author "mark.watson@gmail.com"
:license "Apache 2"
:depends-on (#:vecto)
:components ((:file "package")
(:file "plotlib")))
If you have specified a dependency that is not already downloaded to your computer, Quicklisp will
install the dependency for you.
Common Lisp Object System - CLOS
CLOS was the first ANSI standardized object oriented programming facility. While I do not use
classes and objects as often in my Common Lisp programs as I do when using Java and Smalltalk,
it is difficult to imagine a Common Lisp program of any size that did not define and use at least a
few CLOS classes.
The example program for this chapter in the file src/loving_snippets/HTMLstream.lisp. I used this
CLOS class about ten years ago in a demo for my commercial natural language processing product
to automatically generate demo web pages.
We are going to start our discussion of CLOS somewhat backwards by first looking at a short test
function that uses the HTMLstream class. Once we see how to use this example CLOS class, we
will introduce a small subset of CLOS by discussing in some detail the implementation of the
HTMLstream class and finally, at the end of the chapter, see a few more CLOS programming
techniques. This book only provides a brief introduction to CLOS; the interested reader is encouraged
to do a web search for “CLOS tutorial”.
The macros and functions defined to implement CLOS are a standard part of Common Lisp. Common
Lisp supports generic functions, that is, different functions with the same name that are distinguished
by different argument types.
Example of Using a CLOS Class
The file src/loving_snippets/HTMLstream.lisp contains a short test program at the end of the file:
(defun test (&aux x)
(setq x (make-instance 'HTMLstream))
(set-header x "test page")
(add-element x "test text - this could be any element")
(add-table
'(("<b>Key phrase</b>" "<b>Ranking value</b>")
("this is a test" 3.3)))
(get-html-string x))
The generic function make-instance takes the following arguments:
Common Lisp Object System - CLOS
make-instance class-name &rest initial-arguments &key ...
There are four generic functions used in the function test:
• set-header - required to initialize class and also defines the page title
• add-element - used to insert a string that defines any type of HTML element
• add-table - takes a list of lists and uses the list data to construct an HTML table
• get-html-string - closes the stream and returns all generated HTML data as a string
The first thing to notice in the function test is that the first argument for calling each of these generic
functions is an instance of the class HTMLstream. You are free to also define a function, for example,
add-element that does not take an instance of the class HTMLstream as the first function argument
and calls to add-element will be routed correctly to the correct function definition.
We will see that the macro defmethod acts similarly to defun except that it also allows us to define
many methods (i.e., functions for a class) with the same function name that are differentiated by
different argument types and possibly different numbers of arguments.
Implementation of the HTMLstream Class
The class HTMLstream is very simple and will serve as a reasonable introduction to CLOS
programming. Later we will see more complicated class examples that use multiple inheritance.
Still, this is a good example because the code is simple and the author uses this class frequently
(some proof that it is useful!). The code fragments listed in this section are all contained in the file
src/loving_snippets/HTMLstream.lisp. We start defining a new class using the macro defclass
that takes the following arguments:
defclass class-name list-of-super-classes
list-of-slot-specifications class-specifications
The class definition for HTMLstream is fairly simple:
(defclass HTMLstream ()
((out :accessor out))
(:documentation "Provide HTML generation services"))
Here, the class name is HTMLstream, the list of super classes is an empty list (), the list of slot
specifications contains only one slot specification for the slot named out and there is only one class
specification: a documentation string. Slots are like instance variables in languages like Java and
Smalltalk. Most CLOS classes inherit from at least one super class but we will wait until the next
section to see examples of inheritance. There is only one slot (or instance variable) and we define
Common Lisp Object System - CLOS
an accessor variable with the same name as the slot name. This is a personal preference of mine to
name read/write accessor variables with the same name as the slot.
The method set-header initializes the string output stream used internally by an instance of this
class. This method uses convenience macro with-accessors that binds a local set of local variable
to one or more class slot accessors. We will list the entire method then discuss it:
(defmethod set-header ((ho HTMLstream) title)
(with-accessors
((out out))
(setf out (make-string-output-stream))
(princ "<HTML><head><title>" out)
(princ title out)
(princ "</title></head><BODY>" out)
(terpri out)))
The first interesting thing to notice about the defmethod is the argument list: there are two
arguments ho and title but we are constraining the argument ho to be either a member of the
class HTMLstream or a subclass of HTMLstream. Now, it makes sense that since we are passing
an instance of the class HTMLstream to this generic function (or method – I use the terms “generic
function” and “method” interchangeably) that we would want access to the slot defined for this class.
The convenience macro with-accessors is exactly what we need to get read and write access to the
slot inside a generic function (or method) for this class. In the term ((out out)), the first out is local
variable bound to the value of the slot named out for this instance ho of class HTMLstream. Inside
the with-accessors macro, we can now use setf to set the slot value to a new string output stream.
Note: we have not covered the Common Lisp type string-output-stream yet in this book, but we
will explain its use on the next page.
By the time a call to the method set-header (with arguments of an HTMLstream instance and a
string title) finishes, the instance has its slot set to a new string-output-stream and HTML header
information is written to the newly created string output stream. Note: this string output stream is
now available for use by any class methods called after set-header.
There are several methods defined in the file src/loving_snippets/HTMLstream.lisp, but we will
look at just four of them: add-H1, add-element, add-table, and get-html-string. The remaining
methods are very similar to add-H1 and the reader can read the code in the source file.
As in the method set-header, the method add-H1 uses the macro with-accessors to access the stream
output stream slot as a local variable out. In add-H1 we use the function princ that we discussed in
Chapter on Input and Output to write HTML text to the string output stream:
Common Lisp Object System - CLOS
(defmethod add-H1 ((ho HTMLstream) some-text)
(with-accessors
((out out))
(princ "<H1>" out)
(princ some-text out)
(princ "</H1>" out)
(terpri out)))
The method add-element is very similar to add-H1 except the string passed as the second argument
element is written directly to the stream output stream slot:
(defmethod add-element ((ho HTMLstream) element)
(with-accessors
((out out))
(princ element out)
(terpri out)))
The method add-table converts a list of lists into an HTML table. The Common Lisp function princ-
to-string is a useful utility function for writing the value of any variable to a string. The functions
string-left-trim and string-right-trim are string utility functions that take two arguments: a list of
characters and a string and respectively remove these characters from either the left or right side of
a string. Note: another similar function that takes the same arguments is string-trim that removes
characters from both the front (left) and end (right) of a string. All three of these functions do not
modify the second string argument; they return a new string value. Here is the definition of the
add-table method:
(defmethod add-table ((ho HTMLstream) table-data)
(with-accessors
((out out))
(princ "<TABLE BORDER=\"1\" WIDTH=\"100\%\">" out)
(dolist (d table-data)
(terpri out)
(princ "
<TR>" out)
(terpri out)
(dolist (w d)
(princ "
<TD>" out)
(let ((str (princ-to-string w)))
(setq str (string-left-trim '(#\() str))
(setq str (string-right-trim '(#\)) str))
Common Lisp Object System - CLOS
(princ str out))
(princ "</TD>" out)
(terpri out))
(princ "
</TR>" out)
(terpri out))
(princ "</TABLE>" out)
(terpri out)))
The method get-html-string gets the string stored in the string output stream slot by using the
function get-output-stream-string:
(defmethod get-html-string ((ho HTMLstream))
(with-accessors
((out out))
(princ "</BODY></HTML>" out)
(terpri out)
(get-output-stream-string out)))
CLOS is a rich framework for object oriented programming, providing a superset of features found
in languages like Java, Ruby, and Smalltalk. I have barely scratched the surface in this short CLOS
example for generating HTML. Later in the book, whenever you see calls to make-instance, that
lets you know we are using CLOS even if I don’t specifically mention CLOS in the examples.
Using Defstruct or CLOS
You might notice from my own code that I use Common Lisp defstruct macros to define data
structures more often than I use CLOS. The defclass macro used to create CLOS classes are much
more flexible but for simple data structures I find that using defstruct is much more concise. In the
simplest case, a defstruct can just be a name of the new type followed by slot names. For each slot
like my-slot-1 accessor functions are generated automatically. Here is a simple example:
Clozure Common Lisp Version 1.12
DarwinX8664
? (defstruct struct1 s1 s2)
STRUCT1
? (make-struct1 :s1 1 :s2 2)
#S(STRUCT1 :S1 1 :S2 2)
? (struct1-s1 (make-struct1 :s1 1 :s2 2))
Common Lisp Object System - CLOS
We defined a struct struct1 on line3 with two slots names s1 and s2, show the use of the automatically
generated constructor make-struct1 on line 5, and one of the two automatically generated accessor
functions struct1-s1 on line 7. The names of accessor functions are formed with the structure name
and the slot name.
Heuristically Guided Search
We represent search space as a graph: nodes and links between the nodes. The following figure
shows the simple graph that we use as an example, finding a route from node n1 to node n11:
Plot of best route using the plotlib utilities
The following example code uses a heuristic for determining which node to try first from any specific
location: move to the node that is closest spatially to the goal node. We see that this heuristic will
not always work to produce the most efficient search but we will still get to the goal node. As an
example in which the heuristic does not work, consider when we start at node n1 in the lower left
corner of the figure. The search algorithm can add nodes n2 and n4 to the nodes to search list and
will search using node n4 first since n4 is closer to the goal node n11 than node n2. In this case,
the search will eventually need to back up trying the path n1 to n2. Despite this example of the
heuristic not working to decrease search time, in general, for large search spaces (i.e., graphs with
many nodes and edges) it can dramatically decrease search time.
The main function A*search starting in line 5 extends to line 151 because all search utility functions
are nested (lexically scoped) inside the mani function. The actual code for the main function
A*search is in lines 150 and 151.
Heuristically Guided Search
The data representing nodes in this implementation is globally scoped (see the definitions on lines
155-165 in the “throw away test code” near the bottom of the file) and we set the property path-list
to store the nodes directy connected to each node (set in function init-path-list in lines 36-52). I
originally wrote this code in 1990 which explains it non-functional style using globally scoped node
variables.
;; Perform a heuristic A* search between the start and goal nodes:
;; Copyright 1990, 2017 by Mark Watson
(defun A*search (nodes paths start goal &aux possible-paths best)
(defun Y-coord (x) (truncate (cadr x)))
(defun X-coord (x) (truncate (car x)))
(defun dist-between-points (point1 point2)
(let ((x-dif (- (X-coord point2) (X-coord point1)))
(y-dif (- (Y-coord point2) (Y-coord point1))))
(sqrt (+ (* x-dif x-dif)
(* y-dif y-dif)))))
(setq possible-paths
(dist-between-points
(eval start)
(eval goal))
(list start))))
(defun init-network ()
(setq paths (init-lengths paths))
(init-path-list nodes paths))
(defun init-lengths (pathlist)
(let (new-path-list pathlength path-with-length)
(dolist (path pathlist)
(setq pathlength (slow-path-length path))
(setq path-with-length (append path (list pathlength)))
(setq new-path-list (cons path-with-length new-path-list)))
new-path-list))
(defun init-path-list (nodes paths)
(dolist (node nodes)
Heuristically Guided Search
(get node 'path-list)
;; let returns all paths connected to node:
(let (path-list)
(dolist (path paths)
(if (equal node (start-node-name path))
(setq path-list
(cons (list (end-node-name path)
(path-length path))
path-list))
(if (equal node (end-node-name path))
(setq path-list (cons (list (start-node-name path)
(path-length path))
path-list)))))
path-list ))))
(defun slow-path-length (path)
(dist-between-points (start-node path) (end-node path)))
(defun path-length (x) (caddr x))
(defun start-node (path) (eval (car path)))
(defun end-node (path) (eval (cadr path)))
(defun start-node-name (x) (car x))
(defun end-node-name (x) (cadr x))
(defun first-on-path (x) (caddr x))
(defun goal-node (x) (car x))
(defun distance-to-that-node (x) (cadr x))
(defun enumerate-children (node goal)
(let* ((start-to-lead-node-dist (cadr node)) ;; distance already calculated
(path (caddr node))
(lead-node (car path)))
(if (get-stored-path lead-node goal)
(consider-best-path lead-node goal path start-to-lead-node-dist)
(consider-all-nodes lead-node goal path start-to-lead-node-dist))))
(defun consider-best-path (lead-node goal path distance-to-here)
(let* (
(first-node (get-first-node-in-path lead-node goal))
(dist-to-first (+ distance-to-here
(get-stored-dist lead-node first-node)))
(total-estimate (+ distance-to-here
Heuristically Guided Search
(get-stored-dist lead-node goal)))
(new-path (cons first-node path)))
(list (list total-estimate dist-to-first new-path))))
(defun get-stored-path (start goal)
(if (equal start goal)
(list start 0)
(assoc goal (get start 'path-list))))
(defun node-not-in-path (node path)
(if (null path)
(if (equal node (car path))
(node-not-in-path node (cdr path)))))
(defun consider-all-nodes (lead-node goal path start-to-lead-node-dist)
(let (dist-to-first total-estimate new-path new-nodes)
(dolist (node (collect-linked-nodes lead-node))
(if (node-not-in-path node path)
(let ()
(setq dist-to-first (+ start-to-lead-node-dist
(get-stored-dist lead-node node)))
(setq total-estimate (+ dist-to-first
(dist-between-points
(eval node)
goal))))
(setq new-path (cons node path))
(setq new-nodes (cons (list total-estimate
dist-to-first
new-path)
new-nodes)))))
new-nodes))
(defun collect-linked-nodes (node)
(let (links)
(dolist (link (get node 'path-list))
(if (null (first-on-path link))
(setq links (cons (goal-node link) links))))
links))
(defun get-stored-dist (node1 node2)
(distance-to-that-node (get-stored-path node1 node2)))
Heuristically Guided Search
(defun collect-ascending-search-list-order (a l)
(if (null l)
(list a)
(if (< (car a) (caar l))
(cons a l)
(cons (car l) (Collect-ascending-search-list-order a (cdr l))))))
(defun get-first-node-in-path (start goal)
(let (first-node)
(setq first-node
(first-on-path (get-stored-path start goal)))
(if first-node first-node goal)))
(defun a*-helper ()
(if possible-paths
(let ()
(setq best (car possible-paths))
(setq possible-paths (cdr possible-paths))
(if (equal (first (caddr best)) goal)
(let ()
(dolist (child (enumerate-children best goal))
(setq possible-paths
(collect-ascending-search-list-order
child possible-paths)))
(a*-helper))))))
(init-network)
(reverse (caddr (a*-helper))))
Throw away test code:
(defvar n1 '(30 201))
(defvar
n2 '(25 140))
(defvar
n3 '(55 30))
(defvar
n4 '(105 190))
(defvar
n5 '(95 110))
(defvar
n6 '(140 22))
(defvar
n7 '(160 150))
(defvar
n8 '(170 202))
(defvar
n9 '(189 130))
(defvar
n10 '(200 55))
(defvar
n11 '(205 201))
Heuristically Guided Search
(print (A*search
'(n1 n2 n3 n4 n5 n6 n7 n8 n9 n10 n11) ;; nodes
'((n1 n2) (n2 n3) (n3 n5) (n3 n6) (n6 n10) ;; paths
(n9 n10) (n7 n9) (n1 n4) (n4 n2) (n5 n8)
(n8 n4) (n7 n11))
'n1 'n11)) ;; starting and goal nodes
The following example in the repl shows the calculation of the path that we saw in the figure of the
graph search space.
$ sbcl
* (load "astar_search.lisp")
(N1 N2 N3 N6 N10 N9 N7 N11)
There are many types of search: breadth first as we used here, depth first, with heuristics to optimize
search dependent on the type of search space.
Network Programming
Distributed computing is pervasive: you need to look no further than the World Wide Web,
Internet chat, etc. Of course, as a Lisp programmer, you will want to do at least some of your
network programming in Lisp! The previous editions of this book provided low level socket network
programming examples. I decided that for this new edition, I would remove those examples and
instead encourage you to “move further up the food chain” and work at a higher level of abstraction
that makes sense for the projects you will likely be developing. Starting in the 1980s, a lot of my
work entailed low level socket programming for distributed networked applications. As I write this,
it is 2013, and there are better ways to structure distributed applications.
Specifically, since many of the examples later in this book fetch information from the web and linked
data sources, we will start be learning how to use Edi Weitz’s Drakma HTTP client library³². In order
to have a complete client server example we will also look briefly at Edi Weitz’s Hunchentoot web
server³³ that uses JSON as a data serialization format. I used to use XML for data serialization but
JSON has many advantages: easier for a human to read and it plays nicely with Javascript code and
some data stores like Postgres (new in versions 9.x), MongoDB, and CouchDB that support JSON as
a native data format.
The code snippets in the first two sections of this chapter are derived from examples in the Drackma
and Hunchentoot documentation.
An introduction to Drakma
Edi Weitz’s Drakma library³⁴ supports fetching data via HTTP requests. As you can see in the
Drakma documentation, you can use this library for authenticated HTTP requests (i.e., allow you
to access web sites that require a login), support HTTP GET and PUT operations, and deal with
cookies. The top level API that we will use is drakma:http-request that returns multiple values. In
the following example, I want only the first three values, and ignore the others like the original URI
that was fetched and an IO stream object. We use the built-in Common Lisp macro multiple-value-
³²http://weitz.de/drakma/
³³http://weitz.de/hunchentoot/
³⁴http://weitz.de/drakma/
Network Programming
* (ql:quickload :drakma)
* (multiple-value-setq
(data http-response-code headers)
(drakma:http-request "http://markwatson.com"))
I manually formatted the last statement I entered in the last repl listing and I will continue to
manually edit the repl listings in the rest of this book to make them more easily readable.
The following shows some of the data bound to the variables data, http-response-code, and
headers:
* data
"<!DOCTYPE html>
<html>
<head>
<title>Mark Watson: Consultant and Author</title>
The value of http-response-code is 200 which means that there were no errors:
* http-response-code
The HTTP response headers will be useful in many applications; for fetching the home page of my
web site the headers are:
* headers
((:SERVER . "nginx/1.1.19")
(:DATE . "Fri, 05 Jul 2013 15:18:27 GMT")
(:CONTENT-TYPE . "text/html; charset=utf-8")
(:TRANSFER-ENCODING . "chunked")
(:CONNECTION . "close")
(:SET-COOKIE
"ring-session=cec5d7ba-e4da-4bf4-b05e-aff670e0dd10;Path=/"))
We will use Drakma later in this book for several examples. In the next section we will write a web
app using Hunchentoot and test it with a Drakma client.
Network Programming
An introduction to Hunchentoot
Edi Weitz’s Hunchentoot project³⁵ is a flexible library for writing web applications and web services.
We will also use Edi’s CL-WHO library in this section for generating HTML from Lisp code.
Hunchentoot will be installed the first time you quick load it in the example code for this section:
(ql:quickload "hunchentoot")
I will use only easy handler framework³⁶ in the Hunchentoot examples in this section. I leave it to
you to read the documentation on using custom acceptors³⁷ after you experiment with the examples
in this section.
The following code will work for both multi-threading installations of SBCL and single thread
installations (e.g., some default installations of SBCL on OS X):
(ql:quickload :hunchentoot)
(ql:quickload :cl-who)
(in-package :cl-user)
(defpackage hdemo
(:use :cl
:cl-who
:hunchentoot))
(in-package :hdemo)
(defvar *h* (make-instance 'easy-acceptor :port 3000))
;; define a handler with the arbitrary name my-greetings:
(define-easy-handler (my-greetings :uri "/hello") (name)
(setf (hunchentoot:content-type*) "text/html")
(with-html-output-to-string (*standard-output* nil :prologue t)
(:html
(:head (:title "hunchentoot test"))
(:body
(:h1 "hunchentoot form demo")
(:form
:method :post
(:input :type :text
:name "name"
³⁵http://weitz.de/hunchentoot/
³⁶http://weitz.de/hunchentoot/#easy-handlers
³⁷http://weitz.de/hunchentoot/#acceptors
Network Programming
:value name)
(:input :type :submit :value "Submit your name"))
(:p "Hello " (str name))))))
(hunchentoot:start *h*)
In lines 5 through 9 we create an use a new package that includes support for generating HTML
in Lisp code (CL-WHO) and the Hunchentoot library). On line 11 we create an instance of an easy
acceptor on port 3000 that provides useful default behaviors for providing HTTP services.
The Hunchentoot macro define-easy-handler is used in lines 15 through 28 to define an HTTP
request handler and add it to the easy acceptor instance. The first argument, my-greetings in this
example, is an arbitrary name and the keyword :uri argument provides a URL pattern that the easy
acceptor server object uses to route requests to this handler. For example, when you run this example
on your computer, this URL routing pattern would handle requests like:
http://localhost:3000/hello
In lines 17 through 28 we are using the CL-WHO library to generate HTML for a web page. As you
might guess, :html generates the outer <html></html> tags for a web page. Line 19 would generate
HTML like:
<head>
<title>hunchentoot test</title>
</head>
Lines 22 through 27 generate an HTML input form and line 28 displays any value generated when
the user entered text in the input filed and clicked the submit button. Notice the definition of the
argument name in line 1 in the definition of the easy handler. If the argument name is not defined,
the nil value will be displayed in line 28 as an empty string.
You should run this example and access the generated web page in a web browser, and enter text,
submit, etc. You can also fetch the generated page HTML using the Drakma library that we saw in
the last section. Here is a code snippet using the Drakma client library to access this last example:
Network Programming
* (drakma:http-request "http://127.0.0.1:3000/hello?name=Mark")
"Hello Mark"
((:CONTENT-LENGTH . "10")
(:DATE . "Fri, 05 Jul 2013 15:57:22 GMT")
(:SERVER . "Hunchentoot 1.2.18")
(:CONNECTION . "Close")
(:CONTENT-TYPE . "text/plain; charset=utf-8"))
#<PURI:URI http://127.0.0.1:3000/hello?name=Mark>
#<FLEXI-STREAMS:FLEXI-IO-STREAM {10095654A3}>
We will use both Drackma and Hunchentoot in the next section.
Complete REST Client Server Example Using JSON for
Data Serialization
A reasonable way to build modern distributed systems is to write REST web services that serve JSON
data to client applications. These client applications might be rich web apps written in Javascript,
other web services, and applications running on smartphones that fetch and save data to a remote
web service.
We will use the cl-json Quicklisp package to encode Lisp data into a string representing JSON
encoded data. Here is a quick example:
* (ql:quickload :cl-json)
* (defvar y (list (list '(cat . "the cat ran") '(dog . 101)) 1 2 3 4 5))
(((CAT . "the cat ran") (DOG . 101)) 1 2 3 4 5)
* (json:encode-json-to-string y)
"[{\"cat\":\"the cat ran\",\"dog\":101},1,2,3,4,5]"
The following list shows the contents of the file src/web-hunchentoot-json.lisp:
Network Programming
(ql:quickload :hunchentoot)
(ql:quickload :cl-json)
(defvar *h* (make-instance 'hunchentoot:easy-acceptor :port 3000))
;; define a handler with the name animal:
(hunchentoot:define-easy-handler (animal :uri "/animal") (name)
(print name)
(setf (hunchentoot:content-type*) "text/plain")
((string-equal name "cat")
(json:encode-json-to-string
'(average_weight . 10)
'(friendly . nil))
"A cat can live indoors or outdoors.")))
((string-equal name "dog")
(json:encode-json-to-string
'(average_weight . 40)
'(friendly . t))
"A dog is a loyal creature, much valued by humans.")))
(json:encode-json-to-string
"unknown type of animal")))))
(hunchentoot:start *h*)
This example is very similar to the web application example in the last section. The difference is
that this application is not intended to be viewed on a web page because it returns JSON data as
HTTP responses. The easy handler definition on line 8 specifies a handler argument name. In lines
12 and 19 we check to see if the value of the argument name is “cat” or “dog” and if it is, we return
the appropriate JSON example data for those animals. If there is no match, the default cond clause
starting on line 26 returns a warning string as a JSON encoded string.
While running this test service, in one repl, you can ue the Drakma library in another repl to test it
(not all output is shown in the next listing):
Network Programming
* (ql:quickload :drakma)
* (drakma:http-request "http://127.0.0.1:3000/animal?name=dog")
"[{\"average_weight\":40,
\"friendly\":true},
\"A dog is a loyal creature, much valued by humans.\"]"
* (drakma:http-request "http://127.0.0.1:3000/animal?name=cat")
"[{\"average_weight\":10,
\"friendly\":null},
\"A cat can live indoors or outdoors.\"]"
You can use the cl-json library to decode a string containing JSON data to Lisp data:
* (ql:quickload :cl-json)
To load "cl-json":
Load 1 ASDF system:
cl-json
; Loading "cl-json"
(:CL-JSON)
* (cl-json:decode-json-from-string
(drakma:http-request "http://127.0.0.1:3000/animal?name=dog"))
(((:AVERAGE--WEIGHT . 40) (:FRIENDLY . T))
"A dog is a loyal creature, much valued by humans.")
For most of my work, REST web services are “read-only” in the sense that clients don’t modify state
on the server. However, there are use cases where a client application might want to; for example,
letting clients add new animals to the last example.
(defparameter *animal-hash* (make-hash-table))
;; handle HTTP POST requests:
(hunchentoot:define-easy-handler (some-handler :uri "/add") (json-data)
(setf (hunchentoot:content-type*) "text/plain")
(let* ((data-string (hunchentoot:raw-post-data :force-text t))
(data (cl-json:decode-json-from-string json-data))
;; assume that the name of the animal is a hashed value:
(animal-name (gethash "name" data)))
Network Programming
(setf (gethash animal-name *animal-hash*) data))
In line 4 we are defining an additional easy handler with a handler argument json-data. This data
is assumed to be a string encoding of JSON data which is decoded into Lisp data in lines 6 and 7. We
save the data to the global variable animal-hash.
In this example, we are storing data sent from a client in an in-memory hash table. In a real
application new data might be stored in a database.
Network Programming Wrap Up
You have learned the basics for writing web services and writing clients to use web services. Later,
we will use web services written in Python by writing Common Lisp clients: we will wrap retrained
deep learning models and access them from Common Lisp.
Using the Microsoft Bing Search APIs
I have used the Bing search APIs for many years. Microsoft Bing supports several commercial search
engine services, including my favorite search engine Duck Duck Go. Bing is now part of the Azure
infrastructure that is branded as “Cognitive Services.” You should find the example code for this
chapter relatively easy to extend to other Azure Cognitive Services that you might need to use.
You will need to register with Microsoft’s Azure search service to use the material in this chapter. It
is likely that you view search as a manual human-centered activity. I hope to expand your thinking
to considering applications that automate search, finding information on the web, and automatically
organizing information.
While the example code uses only the search APIs, with some modification it can be extended to
work with all REST APIs provided by Azure Cognitive Services³⁸ that include: analyzing text to
get user intent, general language understanding, detecting key phrases and entity names, translate
between languages, converting between speech and text, and various computer vision services.
These services are generally free or very low cost for a few thousand API calls a month, with
increased cost for production deployments. Microsoft spends about $1 billion a year in research
and development for Azure Cognitive Services.
Getting an Access Key for Microsoft Bing Search APIs
You will need to set up an Azure account if you don’t already have one. I use the Bing search APIs
fairly often for research but I have never spent more than about a dollar a month and usually I get
no bill at all. For personal use it is a very inexpensive service.
You start by going to the web page https://azure.microsoft.com/en-us/try/cognitive-services/³⁹ and
sign up for an access key. The Search APIs sign up is currently in the fourth tab in this web form.
When you navigate to the Search APIs tab, select the option Bing Search APIs v7. You will get an
API key that you need to store in an environment variable that you will soon need:
export BING_SEARCH_V7_SUBSCRIPTION_KEY=1e97834341d2291191c772b7371ad5b7
That is not my real subscription key!
You also set the Bing search API as an environment variable:
³⁸https://azure.microsoft.com/en-us/services/cognitive-services/
³⁹https://azure.microsoft.com/en-us/try/cognitive-services/
Using the Microsoft Bing Search APIs
export BING_SEARCH_V7_ENDPOINT=https://api.cognitive.microsoft.com/bing/v7.0/search
Example Search Script
Instead of using a pure Common Lisp HTTP client library I often prefer using the curl command run
in a separate process. The curl utility handles all possible authentication modes, handles headers,
response data in several formats, etc. We capture the output from curl in a string that in turn gets
processed by a JSON library.
It takes very little Common Lisp code to access the Bing search APIs. The function websearch makes
a generic web search query. The function get-wikidata-uri uses the websearch function by adding
“site:wikidata.org” to the query and returning only the WikiData URI for the original search term.
We will later see several examples. I will list the entire library with comments to follow:
(in-package #:bing)
(defun get-wikidata-uri (query)
(let ((sr (websearch (concatenate 'string "site:wikidata.org " query))))
(cadar sr)))
(defun websearch (query)
(let* ((key (uiop:getenv "BING_SEARCH_V7_SUBSCRIPTION_KEY"))
(endpoint (uiop:getenv "BING_SEARCH_V7_ENDPOINT"))
(command
(concatenate
'string
"curl -v -X GET \""
endpoint "?q="
(drakma:url-encode query :utf-8)
"&mkt=en-US&limit=4\""
" -H \"Ocp-Apim-Subscription-Key: " key "\""))
(response
(uiop:run-program command :output :string)))
(with-input-from-string
(s response)
(let* ((json-as-list (json:decode-json s))
(values (cdadr (cddr (nth 2 json-as-list)))))
(mapcar #'(lambda (x)
(let ((name (assoc :name x))
(display-uri (assoc :display-url x))
(snippet (assoc :snippet x)))
(list (cdr name) (cdr display-uri) (cdr snippet))))
values)))))
Using the Microsoft Bing Search APIs
We get the Bing access key and the search API endpoint in lines 8-9. Lines 10-16 create a complete
call to the curl* command line utility. We spawn a process to run **curl and capture the string
output in the variable response in lines 17-18. You might want to add a few print statements to see
typical values for the variables command and response. The response data is JSON data encoded
in a string, with straightforward code in lines 19-28 to parse out the values we want.
The following repl listing shows this library in use:
$ sbcl
This is SBCL 2.0.2, an implementation of ANSI Common Lisp.
* (ql:quickload "bing")
To load "bing":
Load 1 ASDF system:
; Loading "bing"
..............
("bing")
* (bing:get-wikidata-uri "Sedona Arizona")
"https://www.wikidata.org/wiki/Q80041"
* (bing:websearch "Berlin")
(("Berlin - Wikipedia" "https://en.wikipedia.org/wiki/Berlin"
"Berlin (/ b￿￿r￿l￿n /; German: [b￿￿￿li￿n] (listen)) is the capital and largest cit\
y of Germany by both area and population. Its 3,769,495 (2019) inhabitants make it t\
he most populous city proper of the European Union. The city is one of Germany's 16 \
federal states.")
("THE 15 BEST Things to Do in Berlin - 2020 (with Photos ..."
"https://www.tripadvisor.com/Attractions-g187323-Activities-Berlin.html"
"Book your tickets online for the top things to do in Berlin, Germany on Tripadvis\
or: See 571,599 traveler reviews and photos of Berlin tourist attractions. Find what\
to do today, this weekend, or in August. We have reviews of the best places to see \
in Berlin. Visit top-rated & must-see attractions.")
("Berlin - Official Website of the City of Berlin, Capital ..."
"https://www.berlin.de/en"
"Official Website of Berlin: Information about the Administration, Events, Culture\
, Tourism, Hotels and Hotel Booking, Entertainment, Tickets, Public Transport, Polit\
ical System, Local Authorities and Business in Berlin.")
("Berlin | History, Map, Population, Attractions, & Facts ..."
"https://www.britannica.com/place/Berlin"
"Berlin is situated about 112 miles (180 km) south of the Baltic Sea, 118 miles (1\
90 km) north of the Czech-German border, 110 miles (177 km) east of the former inner\
-German border, and 55 miles (89 km) west of Poland. It lies in the wide glacial val\
ley of the Spree River, which runs through the centre of the city.")
("Berlin travel | Germany - Lonely Planet"
Using the Microsoft Bing Search APIs
"https://www.lonelyplanet.com/germany/berlin"
"Welcome to Berlin Berlin's combo of glamour and grit is bound to mesmerise all th\
ose keen to explore its vibrant culture, cutting-edge architecture, fabulous food, i\
ntense parties and tangible history.")
("Berlin 2020: Best of Berlin, Germany Tourism - Tripadvisor"
"https://www.tripadvisor.com/Tourism-g187323"
"Berlin is an edgy city, from its fashion to its architecture to its charged polit\
ical history. The Berlin Wall is a sobering reminder of the hyper-charged postwar at\
mosphere, and yet the graffiti art that now covers its remnants has become symbolic \
of social progress.")
("Berlin 2020: Best of Berlin, OH Tourism - Tripadvisor"
"https://www.tripadvisor.com/Tourism-g50087-Berlin_Ohio-Vacations.html"
"Berlin Tourism: Tripadvisor has 11,137 reviews of Berlin Hotels, Attractions, and\
Restaurants making it your best Berlin resource.")
("Berlin (band) - Wikipedia" "https://en.wikipedia.org/wiki/Berlin_(band)"
"Berlin is the alias for vocalist Terri Nunn, as well as the American new wave ban\
d she fronts, having been originally formed in Orange County, California. The band g\
ained mainstream-commercial success with singles including \" Sex (I'm A...) \", \" \
No More Words \" and the chart-topping \" Take My Breath Away \" from the 1986 film \
Top Gun.")
("Berlin's official travel website - visitBerlin.de"
"https://www.visitberlin.de/en"
"Berlin's way to a metropolis 100 Years of Greater Berlin In 1920, modern Berlin w\
as born at one fell swoop. 8 cities, 59 rural communities and 27 manor districts uni\
te to form \"Greater Berlin\""))
I have been using the Bing search APIs for many years. They are a standard part of my application
building toolkit.
Wrap-up
You can check out the wide range of Congitive Services⁴⁰ on the Azure site. Available APIs include:
language detection, speech recognition, vision libraries for object recognition, web search, and
anomaly detection in data.
In addition to using automated web scraping to get data for my personal research, I often use
automated web search. I find the Microsoft’s Azure Bing search APIs are the most convenient to use
and I like paying for services that I use.
⁴⁰https://azure.microsoft.com/en-us/try/cognitive-services/
Accessing Relational Databases
There are good options for accessing relational databases from Common Lisp. Personally I almost
always use Postgres and in the past I used either native foreign client libraries or the socket interface
to Postgres. Recently, I decided to switch to CLSQL⁴¹ which provides a common interface for
accessing Postgres, MySQL, SQLite, and Oracle databases. There are also several recent forks of
CLSQL on github. We will use CLSQL in examples in this book. Hopefully while reading the Chapter
on Quicklisp you installed CLSQL and the back end for one or more databases that you use for your
projects.
For some database applications when I know that I will always use the embedded SQLite database
(i.e., that I will never want to switch to Postgres of another database) I will just use the sqlite library
as I do in chapter Knowledge Graph Navigator.
If you have not installed CLSQL yet, then please install it now:
(ql:quickload "clsql")
You also need to install one or more CLSQL backends, depending on which relational databases you
(ql:quickload "clsql-postgresql")
(ql:quickload "clsql-mysql")
(ql:quickload "clsql-sqlite3")
The directory src/clsql_examples contains the standalone example files for this chapter.
While I often prefer hand crafting SQL queries, there seems to be a general movement in software
development towards the data mapper or active record design patterns. CLSQL provides Object
Relational Mapping (ORM) functionality to CLOS.
You will need to create a new database news in order to follow along with the examples in this
chapter and later in this book. I will use Postgres for examples in this chapter and use the following
to create a new database (my account is “markw” and the following assumes that I have Postgres
configured to not require a password for this account when accessing the database from “localhost”):
⁴¹http://clsql.b9.com/
Accessing Relational Databases
psql (9.1.4)
Type "help" for help.
markw=# create database news;
CREATE DATABASE
We will use three example programs that you can find in the src/clsql_examples directory in the
book repository on github:
• clsql_create_news_schema.lisp to create table “articles” in database “news”
• clsql_write_to_news.lisp to write test data to table “articles”
• clsql_read_from_news.lisp to read from the table “articles”
The following listing shows the file src/clsql_examples/clsql_create_news_schema.lisp:
(ql:quickload :clsql)
(ql:quickload :clsql-postgresql)
;; Postgres connection specification:
(host db user password &optional port options tty).
;; The first argument to **clsql:connect** is a connection
;; specification list:
(clsql:connect '("localhost" "news" "markw" nil)
:database-type :postgresql)
(clsql:def-view-class articles ()
:db-kind :key
:db-constraints :not-null
:type integer
:initarg :id)
:accessor uri
:type (string 60)
:initarg :uri)
(title
:accessor title
:type (string 90)
:initarg :title)
:accessor text
Accessing Relational Databases
:type (string 500)
:nulls-ok t
:initarg :text)))
(defun create-articles-table ()
(clsql:create-view-from-class 'articles))
In this repl listing, we create the database table “articles” using the function create-articles-table
that we just defined:
src git:(master) sbcl
(running SBCL from: /Users/markw/sbcl)
* (load "clsql_create_news_schema.lisp")
* (create-articles-table)
NOTICE:
CREATE TABLE / PRIMARY KEY will create implicit index
"article_pk" for table "articles"
The following listing shows the file src/clsql_examples/clsql_write_to_news.lisp:
(ql:quickload :clsql)
(ql:quickload :clsql-postgresql)
;; Open connection to database and create CLOS class and database view
;; for table 'articles':
(load "clsql_create_news_schema.lisp")
(defvar *a1*
(make-instance
'article
:uri "http://test.com"
:title "Trout Season is Open on Oak Creek"
:text "State Fish and Game announced the opening of trout season"))
(clsql:update-records-from-instance *a1*)
;; modify a slot value and update database:
(setf (slot-value *a1* 'title) "Trout season is open on Oak Creek!!!")
(clsql:update-records-from-instance *a1*)
;; warning: the last statement changes the "id" column in the table
You should load the file clsql_write_to_news.lisp one time in a repl to create the test data. The
following listing shows file clsql_read_from_news.lisp:
Accessing Relational Databases
(ql:quickload :clsql)
(ql:quickload :clsql-postgresql)
;; Open connection to database and create CLOS class and database view
;; for table 'articles':
(load "clsql_create_news_schema.lisp")
(defun pp-article (article)
(format t
"~%URI: ~S ~%Title: ~S ~%Text: ~S ~%"
(slot-value article 'uri)
(slot-value article 'title)
(slot-value article 'text)))
(dolist (a (clsql:select 'article))
(pp-article (car a)))
Loading the file clsql_read_from_news.lisp produces the following output:
URI: "http://test.com"
Title: "Trout season is open on Oak Creek!!!"
Text: "State Fish and Game announced the opening of trout season"
URI: "http://example.com"
Title: "Longest day of year"
Text: "The summer solstice is on Friday."
You can also embed SQL where clauses in queries:
(dolist (a (clsql:select 'article :where "title like '%season%'"))
(pp-article (car a)))
which produces this output:
URI: "http://test.com"
Title: "Trout season is open on Oak Creek!!!"
Text: "State Fish and Game announced the opening of
trout season"
In this example, I am using a SQL like expression to perform partial text matching.
Accessing Relational Databases
Database Wrap Up
You learned the basics for accessing relational databases. When I am designing new systems for
processing data I like to think of my Common Lisp code as being purely functional: my Lisp functions
accept arguments that they do not modify and return results. I like to avoid side effects, that is
changing global state. When I do have to handle mutable state (or data) I prefer storing mutable state
in an external database. I use this same approach when I use the Haskell functional programming
language.
Using MongoDB, Solr NoSQL Data
Stores
Non-relational data stores are commonly used for applications that don’t need either full relational
algebra or must scale.
The MongoDB example code is in the file src/loving_snippets/mongo_news.lisp. The Solr example
code is in the subdirectories src/solr_examples.
Note for the fifth edition: The Common Lisp cl-mongo library is now unsupported for versions of
MongoDB later than 2.6 (released in 2016). You can install an old version of MongoDB for macOS⁴²
or for Linux⁴³. I have left the MongoDB examples in this section but I can’t recommend that you use
cl-mongo and MongoDB for any serious applications.
Brewer’s CAP theorem states that a distributed data storage system comprised of multiple nodes can
be robust to two of three of the following guarantees: all nodes always have a Consistent view of the
state of data, general Availablity of data if not all nodes are functioning, and Partition tolerance so
clients can still communicate with the data storage system when parts of the system are unavailable
because of network failures. The basic idea is that different applications have different requirements
and sometimes it makes sense to reduce system cost or improve scalability by easing back on one of
these requirements.
A good example is that some applications may not need transactions (the first guarantee) because it
is not important if clients sometimes get data that is a few seconds out of date.
MongoDB allows you to choose consistency vs. availability vs. efficiency.
I cover the Solr indexing and search service (based on Lucene) both because a Solr indexed document
store is a type of NoSQL data store and also because I believe that you will find Solr very useful for
building systems, if you don’t already use it.
MongoDB
The following discussion of MongoDB is based on just my personal experience, so I am not covering
all use cases. I have used MongoDB for:
• Small clusters of MongoDB nodes to analyze social media data, mostly text mining and
sentiment analysis. In all cases for each application I ran MongoDB with one write master
⁴²https://www.mongodb.org/dl/osx
⁴³https://www.mongodb.org/dl/linux
Using MongoDB, Solr NoSQL Data Stores
(i.e., I wrote data to this one node but did not use it for reads) and multiple read-only slave
nodes. Each slave node would run on the same server that was usually performing a single bit
of analytics.
• Multiple very large independent clusters for web advertising. Problems faced included trying
to have some level of consistency across data centers. Replica sets were used within each data
center.
• Running a single node MongoDB instance for low volume data collection and analytics.
One of the advantages of MongoDB is that it is very “developer friendly” because it supports ad-
hoc document schemas and interactive queries. I mentioned that MongoDB allows you to choose
consistency vs. availability vs. efficiency. When you perform MongoDB writes you can specify some
granularity of what constitutes a “successful write” by requiring that a write is performed at a
specific number of nodes before the client gets acknowledgement that the write was successful. This
requirement adds overhead to each write operation and can cause writes to fail if some nodes are
not available.
The MongoDB online documentation⁴⁴ is very good. You don’t have to read it in order to have fun
playing with the following Common Lisp and MongoDB examples, but if you find that MongoDB is
a good fit for your needs after playing with these examples then you should read the documentation.
I usually install MongoDB myself but it is sometimes convenient to use a hosting service. There are
several well regarded services and I have used MongoHQ⁴⁵.
At this time there is no official Common Lisp support for accessing MongoDB but there is a useful
project by Alfons Haffmans’ cl-mongo⁴⁶ that will allow us to write Common Lisp client applications
and have access to most of the capabilities of MongoDB.
The file src/mongo_news.lisp contains the example code used in the next three sessions.
Adding Documents
The following repl listing shows the cl-mongo APIs for creating a new document, adding elements
(attributes) to it, and inserting it in a MongoDB data store:
⁴⁴http://docs.mongodb.org/manual/
⁴⁵https://www.mongohq.com/
⁴⁶https://github.com/fons/cl-mongo
Using MongoDB, Solr NoSQL Data Stores
(ql:quickload "cl-mongo")
(cl-mongo:db.use "news")
(defun add-article (uri title text)
(let ((doc (cl-mongo:make-document)))
(cl-mongo:add-element "uri" uri doc)
(cl-mongo:add-element "title" title doc)
(cl-mongo:add-element "text" text doc)
(cl-mongo:db.insert "article" doc)))
;; add a test document:
(add-article "http://test.com" "article title 1" "article text 1")
In this example, three string attributes were added to a new document before it was saved.
Fetching Documents by Attribute
We will start by fetchng and pretty-printing all documents in the collection articles and fetching all
articles a list of nested lists where the inner nested lists are document URI, title, and text:
(defun print-articles ()
(cl-mongo:pp (cl-mongo:iter (cl-mongo:db.find "article" :all))))
;; for each document, use the cl-mongo:get-element on
;; each element we want to save:
(defun article-results->lisp-data (mdata)
(let ((ret '()))
;;(print (list "size of result=" (length mdata)))
(dolist (a mdata)
;;(print a)
(cl-mongo:get-element "uri" a)
(cl-mongo:get-element "title" a)
(cl-mongo:get-element "text" a))
ret)))
(defun get-articles ()
(article-results->lisp-data
(cadr (cl-mongo:db.find "article" :all))))
Output for these two functions looks like:
Using MongoDB, Solr NoSQL Data Stores
* (print-articles)
"_id" -> objectid(99778A792EBB4F76B82F75C6)
http://test.com/3
"title"
article title 3
"text"
article text 3
"_id" -> objectid(D47DEF3CFDB44DEA92FD9E56)
http://test.com/2
"title"
article title 2
"text"
article text 2
* (get-articles)
(("http://test.com/2" "article title 2" "article text 2")
("http://test.com/3" "article title 3" "article text 3"))
Fetching Documents by Regular Expression Text Search
By reusing the function article-results->lisp-data defined in the last section, we can also search for
JSON documents using regular expressions matching attribute values:
;; find documents where substring 'str' is in the title:
(defun search-articles-title (str)
(article-results->lisp-data
(cl-mongo:iter
(cl-mongo:db.find
"article"
(cl-mongo:kv
"title"
// TITLE ATTRIBUTE
(cl-mongo:kv "$regex" str)) :limit 10)))))
;; find documents where substring 'str' is in the text element:
(defun search-articles-text (str)
(article-results->lisp-data
(cl-mongo:db.find
Using MongoDB, Solr NoSQL Data Stores
"article"
(cl-mongo:kv
"text"
// TEXT ATTRIBUTE
(cl-mongo:kv "$regex" str)) :limit 10))))
I set the limit to return a maximum of ten documents. If you do not set the limit, this example code
only returns one search result. The following repl listing shows the results from calling function
search-articles-text:
* (SEARCH-ARTICLES-TEXT "text")
(("http://test.com/2" "article title 2" "article text 2")
("http://test.com/3" "article title 3" "article text 3"))
* (SEARCH-ARTICLES-TEXT "3")
(("http://test.com/3" "article title 3" "article text 3"))
I find using MongoDB to be especially effective when experimenting with data and code. The schema
free JSON document format, using interactive queries using the mongo shell⁴⁷, and easy to use client
libraries like clouchdb for Common Lisp will let you experiment with a lot of ideas in a short period
of time. The following listing shows the use of the interactive mongo shell. The database news is
the database used in the MongoDB examples in this chapter; you will notice that I also have other
databases for other projects on my laptop:
src git:(master) mongo
MongoDB shell version: 2.4.5
connecting to: test
> show dbs
kbsportal
0.03125GB
knowledgespace
0.03125GB
(empty)
mark_twitter
0.0625GB
myfocus
0.03125GB
0.03125GB
0.125GB
twitter
0.125GB
> use news
switched to db news
> show collections
article
system.indexes
⁴⁷http://docs.mongodb.org/manual/mongo/
Using MongoDB, Solr NoSQL Data Stores
> db.article.find()
{ "uri" : "http://test.com/3",
"title" : "article title 3",
"text" : "article text 3",
"_id" : ObjectId("99778a792ebb4f76b82f75c6") }
{ "uri" : "http://test.com/2",
"title" : "article title 2",
"text" : "article text 2",
"_id" : ObjectId("d47def3cfdb44dea92fd9e56") }
Line 1 of this listing shows starting the mongo shell. Line 4 shows how to list all databases in the
data store. In line 13 I select the database “news” to use. Line 15 prints out the names of all collections
in the current database “news”. Line 18 prints out all documents in the “articles” collection. You can
read the documentation for the mongo shell⁴⁸ for more options like selective queries, adding indices,
When you run a MongoDB service on your laptop, also try the admin interface on http://localhost:28017/⁴⁹.
A Common Lisp Solr Client
The Lucene project is one of the most widely used Apache Foundation projects. Lucene is a flexible
library for preprocessing and indexing text, and searching text. I have personally used Lucene on
so many projects that it would be difficult to count them. The Apache Solr Project⁵⁰ adds a network
interface to the Lucene text indexer and search engine. Solr also adds other utility features to Lucene:
• While Lucene is a library to embed in your programs, Solr is a complete system.
• Solr provides good defaults for preprocessing and indexing text and also provides rich support
for managing structured data.
• Provides both XML and JSON APIs using HTTP and REST.
• Supports faceted search, geospatial search, and provides utilities for highlighting search terms
in surrounding text of search results.
• If your system ever grows to a very large number of users, Solr supports scaling via replication.
I hope that you will find the Common Lisp example Solr client code in the following sections helps
you make Solr part of large systems that you write using Common Lisp.
Installing Solr
Download a binary Solr distribution⁵¹ and un-tar or un-zip this Solr distribution, cd to the
distribution directory, then cd to the example directory and run:
⁴⁸http://docs.mongodb.org/manual/mongo/
⁴⁹http://localhost:28017/
⁵⁰https://lucene.apache.org/solr/
⁵¹https://lucene.apache.org/solr/downloads.html
Using MongoDB, Solr NoSQL Data Stores
~/solr/example>
java -jar start.jar
You can access the Solr Admin Web App at http://localhost:8983/solr/#/⁵². This web app can be seen
in the following screen shot:
Solr Admin Web App
There is no data in the Solr example index yet, so following the Solr tutorial instructions:
⁵²http://localhost:8983/solr/#/
Using MongoDB, Solr NoSQL Data Stores
~/> cd ~/solr/example/exampledocs
~/solr/example/exampledocs> java -jar post.jar *.xml
SimplePostTool version 1.5
Posting files to base url http://localhost:8983/solr/update
using content-type application/xml..
POSTing file gb18030-example.xml
POSTing file hd.xml
POSTing file ipod_other.xml
POSTing file ipod_video.xml
POSTing file manufacturers.xml
POSTing file mem.xml
POSTing file money.xml
POSTing file monitor.xml
POSTing file monitor2.xml
POSTing file mp500.xml
POSTing file sd500.xml
POSTing file solr.xml
POSTing file utf8-example.xml
POSTing file vidcard.xml
14 files indexed.
COMMITting Solr index changes
to http://localhost:8983/solr/update..
Time spent: 0:00:00.480
You will learn how to add documents to Solr directly in your Common Lisp programs in a later
section.
Assuming that you have a fast Internet connection so that downloading Solr was quick, you have
hopefully spent less than five or six minutes getting Solr installed and running with enough example
search data for the Common Lisp client examples we will play with. Solr is a great tool for storing,
indexing, and searching data. I recommend that you put off reading the official Solr documentation
for now and instead work through the Common Lisp examples in the next two sections. Later, if
you want to use Solr then you will need to carefully read the Solr documentation.
Solr’s REST Interface
The Solr REST Interface Documentation⁵³ documents how to perform search using HTTP GET
requests. All we need to do is implement this in Common Lisp which you will see is easy.
Assuming that you have Solr running and the example data loaded, we can try searching for docu-
ments with, for example, the word “British” using the URL http://localhost:8983/solr/select?q=British⁵⁴.
This is a REST request URL and you can use utilities like curl or wget to fetch the XML data. I fetched
⁵³https://wiki.apache.org/solr/SolJSON
⁵⁴http://localhost:8983/solr/select?q=British
Using MongoDB, Solr NoSQL Data Stores
the data in a web browser, as seen in the following screen shot of a Firefox web browser (I like the
way Firefox formats and displays XML data):
Solr Search Results as XML Data
The attributes in the returned search results need some explanation. We indexed several example
XML data files, one of which contained the following XML element that we just saw as a search
result:
<field name="id">GBP</field>
<field name="name">One British Pound</field>
<field name="manu">U.K.</field>
<field name="manu_id_s">uk</field>
<field name="cat">currency</field>
<field name="features">Coins and notes</field>
<field name="price_c">1,GBP</field>
<field name="inStock">true</field>
</doc>
Using MongoDB, Solr NoSQL Data Stores
So, the search result has the same attributes as the structured XML data that was added to the Solr
search index. Solr’s capability for indexing structured data is a superset of just indexing plain text.
If for example we were indexing news stories, then example input data might look like:
<field name="id">new_story_0001</field>
<field name="title">Fishing Season Opens</field>
<field name="text">Fishing season opens on Friday in Oak Creek.</field>
</doc>
With this example, a search result that returned this document as a result would return attributes
id, title, and text, and the values of these three attributes.
By default the Solr web service returns XML data as seen in the last screen shot. For our examples, I
prefer using JSON so we are going to always add a request parameter wt=json to all REST calls. The
following screen shot shows the same data returned in JSON serialization format instead of XML
format of a Chrome web browser (I like the way Chrome formats and displays JSON data with the
JSONView Chrome Browser extension):
Solr Search Results as JSON Data
Using MongoDB, Solr NoSQL Data Stores
You can read the full JSON REST Solr documentation later, but for our use here we will use the
following search patterns:
• http://localhost:8983/solr/select?q=British+One&wt=json - search for documents with either of
the words “British” or “one” in them. Note that in URIs that the “+” character is used to encode
a space character. If you wanted a “+” character you would encode it with “%2B” and a space
character is encoded as “%20”. The default Solr search option is an OR of the search terms,
unlike, for example, Google Search.
• http://localhost:8983/solr/select?q=British+AND+one&wt=json - search for documents that
contain both of the words “British” and “one” in them. The search term in plain text is “British
AND one”.
Common Lisp Solr Client for Search
As we sawearlier in Network Programming it is fairly simple to use the drakma and cl-json
Common Lisp libraries to call REST services that return JSON data. The function do-search defined
in the next listing (all the Solr example code is in the file src/solr-client.lisp) constructs a query
URI as we saw in the last section and uses the Drackma library to perform an HTTP GET operation
and the cl-json library to parse the returned string containing JSON data into Lisp data structures:
(ql:quickload :drakma)
(ql:quickload :cl-json)
(defun do-search (&rest terms)
(let ((query-string (format nil "~{~A~^+AND+~}" terms)))
(cl-json:decode-json-from-string
(drakma:http-request
(concatenate
'string
"http://localhost:8983/solr/select?q="
query-string
"&wt=json")))))
This example code does return the search results as Lisp list data; for example:
Using MongoDB, Solr NoSQL Data Stores
* (do-search "British" "one")
((:RESPONSE-HEADER (:STATUS . 0) (:*Q-TIME . 1)
(:PARAMS (:Q . "British+AND+one") (:WT . "json")))
(:RESPONSE (:NUM-FOUND . 6) (:START . 0)
(:DOCS
((:ID . "GBP") (:NAME . "One British Pound") (:MANU . "U.K.")
(:MANU--ID--S . "uk") (:CAT "currency")
(:FEATURES "Coins and notes")
(:PRICE--C . "1,GBP") (:IN-STOCK . T)
(:--VERSION-- . 1440194917628379136))
((:ID . "USD") (:NAME . "One Dollar")
(:MANU . "Bank of America")
(:MANU--ID--S . "boa") (:CAT "currency")
(:FEATURES "Coins and notes")
(:PRICE--C . "1,USD") (:IN-STOCK . T)
(:--VERSION-- . 1440194917624184832))
((:ID . "EUR") (:NAME . "One Euro")
(:MANU . "European Union")
(:MANU--ID--S . "eu") (:CAT "currency")
(:FEATURES "Coins and notes")
(:PRICE--C . "1,EUR") (:IN-STOCK . T)
(:--VERSION-- . 1440194917626281984))
((:ID . "NOK") (:NAME . "One Krone")
(:MANU . "Bank of Norway")
(:MANU--ID--S . "nor") (:CAT "currency")
(:FEATURES "Coins and notes")
(:PRICE--C . "1,NOK") (:IN-STOCK . T)
(:--VERSION-- . 1440194917631524864))
((:ID . "0579B002")
(:NAME . "Canon PIXMA MP500 All-In-One Photo Printer")
(:MANU . "Canon Inc.")
(:MANU--ID--S . "canon")
(:CAT "electronics" "multifunction printer"
"printer" "scanner" "copier")
(:FEATURES "Multifunction ink-jet color photo printer"
"Flatbed scanner, optical scan resolution of 1,200 x 2,400 dpi"
"2.5\" color LCD preview screen" "Duplex Copying"
"Printing speed up to 29ppm black, 19ppm color" "Hi-Speed USB"
"memory card: CompactFlash, Micro Drive, SmartMedia,
Memory Stick, Memory Stick Pro, SD Card, and MultiMediaCard")
(:WEIGHT . 352.0) (:PRICE . 179.99)
(:PRICE--C . "179.99,USD")
Using MongoDB, Solr NoSQL Data Stores
(:POPULARITY . 6) (:IN-STOCK . T)
(:STORE . "45.19214,-93.89941")
(:--VERSION-- . 1440194917651447808))
((:ID . "SOLR1000")
(:NAME . "Solr, the Enterprise Search Server")
(:MANU . "Apache Software Foundation")
(:CAT "software" "search")
(:FEATURES "Advanced Full-Text Search Capabilities using Lucene"
"Optimized for High Volume Web Traffic"
"Standards Based Open Interfaces - XML and HTTP"
"Comprehensive HTML Administration Interfaces"
"Scalability - Efficient Replication to other Solr Search Servers"
"Flexible and Adaptable with XML configuration and Schema"
"Good unicode support: hÃ©llo (hello with an accent over the e)")
(:PRICE . 0.0) (:PRICE--C . "0,USD") (:POPULARITY . 10) (:IN-STOCK . T)
(:INCUBATIONDATE--DT . "2006-01-17T00:00:00Z")
(:--VERSION-- . 1440194917671370752)))))
I might modify the search function to return just the fetched documents as a list, discarding the
returned Solr meta data:
* (cdr (cadddr (cadr (do-search "British" "one"))))
(((:ID . "GBP") (:NAME . "One British Pound") (:MANU . "U.K.")
(:MANU--ID--S . "uk") (:CAT "currency") (:FEATURES "Coins and notes")
(:PRICE--C . "1,GBP") (:IN-STOCK . T)
(:--VERSION-- . 1440194917628379136))
((:ID . "USD") (:NAME . "One Dollar") (:MANU . "Bank of America")
(:MANU--ID--S . "boa") (:CAT "currency") (:FEATURES "Coins and notes")
(:PRICE--C . "1,USD") (:IN-STOCK . T)
(:--VERSION-- . 1440194917624184832))
((:ID . "EUR") (:NAME . "One Euro") (:MANU . "European Union")
(:MANU--ID--S . "eu") (:CAT "currency") (:FEATURES "Coins and notes")
(:PRICE--C . "1,EUR") (:IN-STOCK . T)
(:--VERSION-- . 1440194917626281984))
((:ID . "NOK") (:NAME . "One Krone") (:MANU . "Bank of Norway")
(:MANU--ID--S . "nor") (:CAT "currency")
(:FEATURES "Coins and notes")
(:PRICE--C . "1,NOK") (:IN-STOCK . T)
(:--VERSION-- . 1440194917631524864))
((:ID . "0579B002")
(:NAME . "Canon PIXMA MP500 All-In-One Photo Printer")
(:MANU . "Canon Inc.") (:MANU--ID--S . "canon")
Using MongoDB, Solr NoSQL Data Stores
(:CAT "electronics" "multifunction printer" "printer"
"scanner" "copier")
(:FEATURES "Multifunction ink-jet color photo printer"
"Flatbed scanner, optical scan resolution of 1,200 x 2,400 dpi"
"2.5\" color LCD preview screen" "Duplex Copying"
"Printing speed up to 29ppm black, 19ppm color" "Hi-Speed USB"
"memory card: CompactFlash, Micro Drive, SmartMedia, Memory Stick,
Memory Stick Pro, SD Card, and MultiMediaCard")
(:WEIGHT . 352.0) (:PRICE . 179.99) (:PRICE--C . "179.99,USD")
(:POPULARITY . 6) (:IN-STOCK . T) (:STORE . "45.19214,-93.89941")
(:--VERSION-- . 1440194917651447808))
((:ID . "SOLR1000") (:NAME . "Solr, the Enterprise Search Server")
(:MANU . "Apache Software Foundation") (:CAT "software" "search")
(:FEATURES "Advanced Full-Text Search Capabilities using Lucene"
"Optimized for High Volume Web Traffic"
"Standards Based Open Interfaces - XML and HTTP"
"Comprehensive HTML Administration Interfaces"
"Scalability - Efficient Replication to other Solr Search Servers"
"Flexible and Adaptable with XML configuration and Schema"
"Good unicode support: hÃ©llo (hello with an accent over the e)")
(:PRICE . 0.0) (:PRICE--C . "0,USD") (:POPULARITY . 10) (:IN-STOCK . T)
(:INCUBATIONDATE--DT . "2006-01-17T00:00:00Z")
(:--VERSION-- . 1440194917671370752)))
There are a few more important details if you want to add Solr search to your Common Lisp
applications. When there are many search results you might want to fetch a limited number of
results and then “page” through them. The following strings can be added to the end of a search
query:
• &rows=2 this example returns a maximum of two “rows” or two query results.
• &start=4 this example skips the first 4 available results
A query that combines skipping results and limiting the number of returned results looks like this:
http://localhost:8983/solr/select?q=British+One&wt=json&start=2&rows=2
Common Lisp Solr Client for Adding Documents
In the last example we relied on adding example documents to the Solr search index using the
directions for setting up a new Solr installation. In a real application, in addition to performing search
requests for indexed documents you will need to add new documents from your Lisp applications.
Using the Drakma we will see that it is very easy to add documents.
We need to construct a bit of XML containing new documents in the form:
Using MongoDB, Solr NoSQL Data Stores
<field name="id">123456</field>
<field name="title">Fishing Season</field>
</doc>
</add>
You can specify whatever field names (attributes) that are required for your application. You can
also pass multiple <doc></doc> elements in one add request. We will want to specify documents in
a Lisp-like way: a list of cons values where each cons value is a field name and a value. For the last
XML document example we would like an API that lets us just deal with Lisp data like:
(do-add '(("id" . "12345")
("title" . "Fishing Season")))
One thing to note: the attribute names and values must be passed as strings. Other data types like
integers, floating point numbers, structs, etc. will not work.
This is nicer than having to use XML, right? The first thing we need is a function to convert a list
of cons values to XML. I could have used the XML Builder functionality in the cxml library that is
available via Quicklisp, but for something this simple I just wrote it in pure Common Lisp with no
other dependencies (also in the example file src/solr-client.lisp) :
(defun keys-values-to-xml-string
(keys-values-list)
(with-output-to-string (stream)
(format stream "<add><doc>")
(dolist (kv keys-values-list)
(format stream "<field name=\"")
(format stream (car kv))
(format stream "\">")
(format stream (cdr kv))
(format stream "\"</field>"))
(format stream "</doc></add>")))
The macro with-output-to-string on line 2 of the listing is my favorite way to generate strings.
Everything written to the variable stream inside the macro call is appended to a string; this string
is the return value of the macro.
The following function adds documents to the Solr document input queue but does not actually
index them:
Using MongoDB, Solr NoSQL Data Stores
(defun do-add (keys-values-list)
(drakma:http-request
"http://localhost:8983/solr/update"
:method :post
:content-type "application/xml"
:content ( keys-values-to-xml-string
keys-values-list)))
You have noticed in line 3 that I am accessing a Solr server running on localhost and not a remote
server. In an application using a remote Solr server you would need to modify this to reference your
server; for example:
"http://solr.knowledgebooks.com:8983/solr/update"
For efficiency Solr does not immediately add new documents to the index until you commit the
additions. The following function should be called after you are done adding documents to actually
add them to the index:
(defun commit-adds ()
(drakma:http-request
"http://localhost:8983/solr/update"
:method :post
:content-type "application/xml"
:content "<commit></commit>"))
Notice that all we need is an empty element <commit></commit> that signals the Solr server that
it should index all recently added documents. The following repl listing shows everything working
together (I am assuming that the contents of the file src/solr-client.lisp has been loaded); not all of
the output is shown in this listing:
* (do-add '(("id" . "12345") ("title" . "Fishing Season")))
((:CONTENT-TYPE . "application/xml; charset=UTF-8")
(:CONNECTION . "close"))
#<PURI:URI http://localhost:8983/solr/update>
#<FLEXI-STREAMS:FLEXI-IO-STREAM {1009193133}>
* (commit-adds)
((:CONTENT-TYPE . "application/xml; charset=UTF-8")
Using MongoDB, Solr NoSQL Data Stores
(:CONNECTION . "close"))
#<PURI:URI http://localhost:8983/solr/update>
#<FLEXI-STREAMS:FLEXI-IO-STREAM {10031F20B3}>
* (do-search "fishing")
((:RESPONSE-HEADER (:STATUS . 0) (:*Q-TIME . 2)
(:PARAMS (:Q . "fishing") (:WT . "json")))
(:RESPONSE (:NUM-FOUND . 1) (:START . 0)
(:DOCS
((:ID . "12345\"") (:TITLE "Fishing Season\"")
(:--VERSION-- . 1440293991717273600)))))
Common Lisp Solr Client Wrap Up
Solr has a lot of useful features that we have not used here like supporting faceted search (drilling
down in previous search results), geolocation search, and looking up indexed documents by attribute.
In the examples I have shown you, all text fields are indexed but Solr optionally allows you fine
control over indexing, spelling correction, word stemming, etc.
Solr is a very capable tool for storing, indexing, and searching data. I have seen Solr used effectively
on projects as a replacement for a relational database or other NoSQL data stores like CouchDB or
MongoDB. There is a higher overhead for modifying or removing data in Solr so for applications
that involve frequent modifications to stored data Solr might not be a good choice.
NoSQL Wrapup
There are more convenient languages than Common Lisp to use for accessing MongoDB. To be
honest, my favorites are Ruby and Clojure. That said, for applications where the advantages of
Common Lisp are compelling, it is good to know that your Common Lisp applications can play
nicely with MongoDB.
I am a polyglot programmer: I like to use the best programming language for any specific job. When
we design and build systems with more than one programming language, there are several options
to share data:
• Use foreign function interfaces to call one language from another from inside one process.
• Use a service architecture and send requests using REST or SOAP.
• Use shared data stores, like relational databases, MongoDB, CouchDB and Solr.
Hopefully this chapter and the last chapter will provide most of what you need for the last option.
Natural Language Processing
Natural Language Processing (NLP) is the automated processing of natural language text with
several goals:
• Determine the parts of speech (POS tagging) of words based on the surrounding words.
• Detect if two text documents are similar.
• Categorize text (e.g., is it about the economy, politics, sports, etc.)
• Summarize text
• Determine the sentiment of text
• Detect names (e.g., place names, people’s names, product names, etc.)
We will use a library that I wrote that performs POS tagging, categorization (classification),
summarization, and detects proper names.
My example code for this chapter is contained in separate Quicklisp projects located in the
subdirectories:
• src/fasttag: performs part of speech tagging and tokenizes text
• src/categorize_summarize: performs categorization (e.g., detects the topic of text is news,
politics, economy, etc.) and text summarization
• src/kbnlp: the top level APIs for my pure Common Lisp natural language processing (NLP)
code. In later chapters we will take a different approach by using Python deep learning models
for NLP that we call as a web service. I use both approaches in my own work.
I worked on this Lisp code, and also similar code in Java, from about 2001 to 2011, and again in 2019
for my application for generating knowledge graph data automatically (this is an example in a later
chapter). I am going to begin the next section with a quick explanation of how to run the example
code. If you find the examples interesting then you can also read the rest of this chapter where I
explain how the code works.
The approach that I used in my library for categorization (word counts) is now dated. I recommend
that you consider taking Andrew Ng’s course on Machine Learning on the free online Coursera
system and then take one of the Coursera NLP classes for a more modern treatment of NLP.
In addition to the code for my library you might also find the linguistic data in src/linguistic_data
useful.
Loading and Running the NLP Library
I repackaged the NLP example code into one long file. The code used to be split over 18 source files.
The code should be loaded from the src/kbnlp directory:
Natural Language Processing
loving-common-lisp git:(master) > cd src/kbnlp
src/kbnlp git:(master) > sbcl
* (ql:quickload "kbnlp")
"Startng to load data...."
"....done loading data."
This also loads the projects in src/fasttag and src/categorize_summarize.
Unfortunately, it takes about a minute using SBCL to load the required linguistic data so I
recommend creating a Lisp image that can be reloaded to avoid the time required to load the data:
* (sb-ext:save-lisp-and-die "nlp-image" :purify t)
[undoing binding stack and other enclosing state... done]
[saving current Lisp image into nlp-image:
writing 5280 bytes from the read-only space at 0x0x20000000
writing 3088 bytes from the static space at 0x0x20100000
writing 80052224 bytes from the dynamic space at 0x0x1000000000
src git:(master) > ls -lh nlp-image
-rw-r--r--
1 markw
76M Jul 13 12:49 nlp-image
In line 1 in this repl listing, I use the SBCL built-in function save-lisp-and-die to create the Lisp
image file. Using save-lisp-and-die is a great technique to use whenever it takes a while to set up
your work environment. Saving a Lisp image for use the next time you work on a Common Lisp
project is reminiscent of working in Smalltalk where your work is saved between sessions in an
image file.
Note: I often use Clozure-CL (CCL) instead of SBCL for developing my NLP libraries because CCL
loads my data files much faster than SBCL.
You can now start SBCL with the NLP library and data preloaded using the Lisp image that you just
created:
src git:(master) > sbcl --core nlp-image
* (in-package :kbnlp)
#<PACKAGE "KBNLP">
* (defvar
(make-text-object
"President Bob Smith talked to Congress about the economy and taxes"))
Natural Language Processing
#S(TEXT
:URL ""
:TITLE ""
:SUMMARY "<no summary>"
:CATEGORY-TAGS (("news_politics.txt" 0.01648)
("news_economy.txt" 0.01601))
:KEY-WORDS NIL
:KEY-PHRASES NIL
:HUMAN-NAMES ("President Bob Smith")
:PLACE-NAMES NIL
:TEXT #("President" "Bob" "Smith" "talked" "to" "Congress" "about" "the"
"economy" "and" "taxes")
:TAGS #("NNP" "NNP" "NNP" "VBD" "TO" "NNP" "IN" "DT" "NN" "CC" "NNS")
:STEMS #("presid" "bob" "smith" "talk" "to" "congress" "about" "the"
"economi" "and" "tax"))
At the end of the file src/knowledgebooks_nlp.lisp in comments is some test code that processes
much more text so that a summary is also generated; here is a bit of the output you will see if you
load the test code into your repl:
(:SUMMARY
"Often those amendments are an effort to change government policy
by adding or subtracting money for carrying it out. The initial
surge in foreclosures in 2007 and 2008 was tied to subprime
mortgages issued during the housing boom to people with shaky
credit. 2 trillion in annual appropriations bills for funding
most government programs — usually low profile legislation that
typically dominates the work of the House in June and July.
Bill Clinton said that banking in Europe is a good business.
These days homeowners who got fixed rate prime mortgages because
they had good credit cannot make their payments because they are
out of work. The question is whether or not the US dollar remains
the world s reserve currency if not the US economy will face
a depression."
:CATEGORY-TAGS (("news_politics.txt" 0.38268)
("news_economy.txt" 0.31182)
("news_war.txt" 0.20174))
Natural Language Processing
:HUMAN-NAMES ("President Bill Clinton")
:PLACE-NAMES ("Florida"))
The top-level function make-text-object takes one required argument that can be either a string
containing text or an array of strings where each string is a word or punctuation. Function make-
text-object has two optional keyword parameters: the URL where the text was found and a title.
(defun make-text-object (words &key (url "") (title ""))
(if (typep words 'string) (setq words (words-from-string words)))
(let* ((txt-obj (make-text :text words :url url :title title)))
(setf (text-tags txt-obj) (part-of-speech-tagger words))
(setf (text-stems txt-obj) (stem-text txt-obj))
;; note: we must find human and place names before calling
;; pronoun-resolution:
(let ((names-places (find-names-places txt-obj)))
(setf (text-human-names txt-obj) (car names-places))
(setf (text-place-names txt-obj) (cadr names-places)))
(setf (text-category-tags txt-obj)
(mapcar
#'(lambda (x)
(car x)
(/ (cadr x) 1000000.0)))
(get-word-list-category (text-text txt-obj))))
(setf (text-summary txt-obj) (summarize txt-obj))
txt-obj))
In line 2, we check if this function was called with a string containing text in which case the function
words-from-string is used to tokenize the text into an array of string tokens. Line two defines the
local variable txt-obj with the value of a new text object with only three slots (attributes) defined:
text, url, and title. Line 4 sets the slot text-tags to the part of speech tokens using the function part-
of-speech-tagger. We use the function find-names-places in line 8 to get person and place names
and store these values in the text object. In lines 11 through 17 we use the function get-word-list-
category to set the categories in the text object. In line 18 we similarly use the function summarize
to calculate a summary of the text and also store it in the text object. We will discuss these NLP
helper functions throughout the rest of this chapter.
The function make-text-object returns a struct that is defined as:
Natural Language Processing
(defstruct text
summary
category-tags
key-words
key-phrases
human-names
place-names
stems)
Part of Speech Tagging
This tagger is the Common Lisp implementation of my FastTag open source project. I based this
project on Eric Brill’s PhD thesis (1995). He used machine learning on annotated text to learn tagging
rules. I used a subset of the tagging rules that he generated that were most often used when he tested
his tagger. I hand coded his rules in Lisp (and Ruby, Java, and Pascal). My tagger is less accurate, but
it is fast - thus the name FastTag.
If you just need part of speech tagging (and not summarization, categorization, and top level APIs
used in the last section) you can load:
(ql:quickload "fasttag")
You can find the tagger implementation in the function part-of-speech-tagger. We already saw
sample output from the tagger in the last section:
:TEXT #("President" "Bob" "Smith" "talked" "to" "Congress" "about" "the"
"economy" "and" "taxes")
:TAGS #("NNP" "NNP" "NNP" "VBD" "TO" "NNP" "IN" "DT" "NN" "CC" "NNS")
The following table shows the meanings of the tags and a few example words:
Natural Language Processing
Definition
Example words
Coord Conjuncn
and, but, or
Noun, sing. or mass
Cardinal number
one, two
Noun, plural
dogs, cats
Determiner
the, some
Proper noun, sing.
Edinburgh
Existential there
Proper noun, plural
Smiths
Foreign Word
mon dieu
Predeterminer
all, both
Preposition
of, in, by
Possessive ending
Adjective
Personal pronoun
I, you, she
Adj., comparative
bigger
Possessive pronoun
my, one’s
Adj., superlative
biggest
Adverb
quickly
List item marker
1, One
Adverb, comparative
faster
can, should
Adverb, superlative
fastest
Particle
up, off
Possessive-Wh
Symbol
+, %, &
Wh-adverb
how, where
Dollar sign
Interjection
oh, oops
Pound sign
verb, base form
eat, run
verb, past tense
verb, gerund
eating
Left paren
verb, past part
Right paren
Verb, present
Verb, present
Sent-final punct
Wh-determiner
which, that
Mid-sent punct.
Wh pronoun
who, what
The function part-of-speech-tagger loops through all input words and initially assigns the most
Natural Language Processing
likely part of speech as specified in the lexicon. Then a subset of Brill’s rules are applied. Rules
operate on the current word and the previous word.
As an example Common Lisp implementation of a rule, look for words that are tagged as common
nouns, but end in “ing” so they should be a gerand (verb form):
; rule 8: convert a common noun to a present
participle verb (i.e., a gerand)
(if (equal (search "NN" r) 0)
(let ((i (search "ing" w :from-end t)))
(if (equal i (- (length w) 3))
(setq r "VBG"))))
You can find the lexicon data in the file src/linguistic_data/FastTagData.lisp. This file is List code
instead of plain data (that in retrospect would be better because it would load faster) and looks like:
(defvar lex-hash (make-hash-table :test #'equal :size 110000))
(setf (gethash "shakeup" lex-hash) (list "NN"))
(setf (gethash "Laurance" lex-hash) (list "NNP"))
(setf (gethash "expressing" lex-hash) (list "VBG"))
(setf (gethash "citybred" lex-hash) (list "JJ"))
(setf (gethash "negative" lex-hash) (list "JJ" "NN"))
(setf (gethash "investors" lex-hash) (list "NNS" "NNPS"))
(setf (gethash "founding" lex-hash) (list "NN" "VBG" "JJ"))
I generated this file automatically from lexicon data using a small Ruby script. Notice that words
can have more than one possible part of speech. The most common part of speech for a word is the
first entry in the lexicon.
Categorizing Text
The code to categorize text is fairly simple using a technique often called “bag of words.” I collected
sample text in several different categories and for each category (like politics, sports, etc.) I calculated
the evidence or weight that words contribute to supporting a category. For example, the word
“president” has a strong weight for the category “politics” but not for the category “sports.” The
reason is that the word “president” occurs frequently in articles and books about politics. The data
file that contains the word weightings for each category is src/data/cat-data-tables.lisp. You can
look at this file; here is a very small part of it:
If you only need categorization and not the other libraries developed in this chapter, you can just
load this library and run the example in the comment at the bottom of the file categorize_summa-
rize.lisp:
Natural Language Processing
({lang=”lisp”,linenos=off} (ql:quickload “categorize_summarize”) (defvar x “President Bill Clinton
<<2 pages text no shown>> “) (defvar words1 (myutils:words-from-string x)) (print words1) (setq
cats1 (categorize_summarize:categorize words1)) (print cats1) (defvar sum1 (categorize_summa-
rize:summarize words1 cats1)) (print sum1)
Let’s look at the implementation, starting with creating hash tables for storing word count data for
each category or topic:
Starting topic: news_economy.txt
(setf *h* (make-hash-table :test #'equal :size 1000))
(setf (gethash "news" *h*) 3915)
(setf (gethash "debt" *h*) 3826)
(setf (gethash "money" *h*) 1809)
(setf (gethash "work" *h*) 1779)
(setf (gethash "business" *h*) 1631)
(setf (gethash "tax" *h*) 1572)
(setf (gethash "poverty" *h*) 1512)
This file was created by a simple Ruby script (not included with the book’s example code) that
processes a list of sub-directories, one sub-directory per category. The following listing shows the
implementation of function get-word-list-category that calculates category tags for input text:
(defun get-word-list-category (words)
(let ((x nil)
(ss nil)
(cat-hash nil)
(word nil)
(len (length words))
(num-categories (length categoryHashtables))
(category-score-accumulation-array
(make-array num-categories :initial-element 0)))
(defun list-sort (list-to-sort)
;;(pprint list-to-sort)
(sort list-to-sort
#'(lambda (list-element-1 list-element-2)
(> (cadr list-element-1) (cadr list-element-2)))))
(do ((k 0 (+ k 1)))
((equal k len))
(setf word (string-downcase (aref words k)))
Natural Language Processing
(do ((i 0 (+ i 1)))
((equal i num-categories))
(setf cat-hash (nth i categoryHashtables))
(setf x (gethash word cat-hash))
(aref category-score-accumulation-array i)
(+ x (aref category-score-accumulation-array i))))))
(setf ss '())
(do ((i 0 (+ i 1)))
((equal i num-categories))
(if (> (aref category-score-accumulation-array i) 0.01)
(nth i categoryNames)
(round (* (aref category-score-accumulation-array i) 10)))
ss))))
(setf ss (list-sort ss))
(let ((cutoff (/ (cadar ss) 2))
(results-array '()))
(dolist (hit ss)
(if (> (cadr hit) cutoff)
(setf results-array (cons hit results-array))))
(reverse results-array))))
On thing to notice in this listing is lines 11 through 15 where I define a nested function list-sort that
takes a list of sub-lists and sorts the sublists based on the second value (which is a number) in the
sublists. I often nest functions when the “inner” functions are only used in the “outer” function.
Lines 2 through 9 define several local variables used in the outer function. The global variable
categoryHashtables is a list of word weighting score hash tables, one for each category. The local
variable category-score-accumulation-array is initialized to an array containing the number zero
in each element and will be used to “keep score” of each category. The highest scored categories will
be the return value for the outer function.
Lines 17 through 27 are two nested loops. The outer loop is over each word in the input word array.
The inner loop is over the number of categories. The logic is simple: for each word check to see if it
has a weighting score in each category’s word weighting score hash table and if it is, increment the
matching category’s score.
The local variable ss is set to an empty list on line 28 and in the loop in lines 29 through 38 I am
copying over categories and their scores when the score is over a threshold value of 0.01. We sort
Natural Language Processing
the list in ss on line 39 using the inner function and then return the categories with a score greater
than the median category score.
Detecting People’s Names and Place Names
The code for detecting people and place names is in the top level API code in the package defined
in src/kbnlp. This package is loaded using:
(ql:quickload "kbnlp")
(kbnlp:make-text-object "President Bill Clinton ran for president of the USA")
The functions that support identifying people’s names and place names in text are in the Common
Lisp package kb nlp::
• find-names (words tags exclusion-list) – words is an array of strings for the words in text, tags
are the parts of speech tags (from FastTag), and the exclusion list is a an array of words that
you want to exclude from being considered as parts of people’s names. The list of found names
records starting and stopping indices for names in the array words.
• not-in-list-find-names-helper (a-list start end) – returns true if a found name is not already
been added to a list for saving people’s names in text
• find-places (words exclusion-list) – this is similar to find-names, but it finds place names. The
list of found place names records starting and stopping indices for place names in the array
words.
• not-in-list-find-places-helper (a-list start end) – returns true if a found place name is not already
been added to a list for saving place names in text
• build-list-find-name-helper (v indices) – This converts lists of start/stop word indices to strings
containing the names
• find-names-places (txt-object) – this is the top level function that your application will call. It
takes a defstruct text object as input and modifies the defstruct text by adding people’s and
place names it finds in the text. You saw an example of this earlier in this chapter.
I will let you read the code and just list the top level function:
Natural Language Processing
(defun find-names-places (txt-object)
(let* ((words (text-text txt-object))
(tags (text-tags txt-object))
(place-indices (find-places words nil))
(name-indices (find-names words tags place-indices))
(name-list
(remove-duplicates
(build-list-find-name-helper words name-indices) :test #'equal))
(place-list
(remove-duplicates
(build-list-find-name-helper words place-indices) :test #'equal)))
(let ((ret '()))
(dolist (x name-list)
(if (search " " x)
(setq ret (cons x ret))))
(setq name-list (reverse ret)))
(remove-shorter-names name-list)
(remove-shorter-names place-list))))
In line 2 we are using the slot accessor text-text to fetch the array of word tokens from the text
object. In lines 3, 4, and 5 we are doing the same for part of speech tags, place name indices in the
words array, and person names indices in the words array.
In lines 6 through 11 we are using the function build-list-find-name-helper twice to construct the
person names and place names as strings given the indices in the words array. We are also using the
Common Lisp built-in function remove-duplicates to get rid of duplicate names.
In lines 12 through 16 we are discarding any persons names that do not contain a space, that is, only
keep names that are at least two word tokens. Lines 17 through 19 define the return value for the
function: a list of lists of people and place names using the function remove-shorter-names twice
to remove shorter versions of the same names from the lists. For example, if we had two names “Mr.
John Smith” and “John Smith” then we would want to drop the shorter name “John Smith” from the
return list.
Summarizing Text
The code for summarizing text is located in the directory src/categorize_summarize and can be
loaded using:
({lang=”lisp”,linenos=off} (ql:quickload “categorize_summarize”)
The code for summarization depends on the categorization code we saw earlier.
Natural Language Processing
There are many applications for summarizing text. As an example, if you are writing a document
management system you will certainly want to use something like Solr to provide search func-
tionality. Solr will return highlighted matches in snippets of indexed document field values. Using
summarization, when you add documents to a Solr (or other) search index you could create a new
unindexed field that contains a document summary. Then when the users of your system see search
results they will see the type of highlighted matches in snippets they are used to seeing in Google,
Bing, or DuckDuckGo search results, and, they will see a summary of the document.
Sounds good? The problem to solve is getting good summaries of text and the technique used may
have to be modified depending on the type of text you are trying to summarize. There are two basic
techniques for summarization: a practical way that almost everyone uses, and an area of research
that I believe has so far seen little practical application. The techniques are sentence extraction and
abstraction of text into a shorter form by combining and altering sentences. We will use sentence
extraction.
How do we choose which sentences in text to extract for the summary? The idea I had in 1999 was
simple. Since I usually categorize text in my NLP processing pipeline why not use the words that
gave the strongest evidence for categorizing text, and find the sentences with the largest number of
these words. As a concrete example, if I categorize text as being “politics”, I identify the words in the
text like “president”, “congress”, “election”, etc. that triggered the “politics” classification, and find
the sentences with the largest concentrations of these words.
Summarization is something that you will probably need to experiment with depending on your
application. My old summarization code contained a lot of special cases, blocks of commented out
code, etc. I have attempted to shorten and simplify my old summarization code for the purposes of
this book as much as possible and still maintain useful functionality.
The function for summarizing text is fairly simple because when the function summarize is called
by the top level NLP library function make-text-object, the input text has already been categorized.
Remember from the example at the beginning of the chapter that the category data looks like this:
:CATEGORY-TAGS (("news_politics.txt" 0.38268)
("news_economy.txt" 0.31182)
("news_war.txt" 0.20174))
This category data is saved in the local variable cats on line 4 of the following listing.
Natural Language Processing
(defun summarize (txt-obj)
(let* ((words (text-text txt-obj))
(num-words (length words))
(cats (text-category-tags txt-obj))
(sentence-count 0)
best-sentences sentence (score 0))
;; loop over sentences:
(dotimes (i num-words)
(let ((word (svref words i)))
(dolist (cat cats)
(let* ((hash (gethash (car cat) categoryToHash))
(value (gethash word hash)))
(if value
(setq score (+ score (* 0.01 value (cadr cat)))))))
(push word sentence)
(if (or (equal word ".") (equal word "!") (equal word ";"))
(let ()
(setq sentence (reverse sentence))
(setq score (/ score (1+ (length sentence))))
(setq sentence-count (1+ sentence-count))
(format t "~%~A : ~A~%" sentence score)
;; process this sentence:
(if (and
(> score 0.4)
(> (length sentence) 4)
(< (length sentence) 30))
(progn
(setq sentence
(reduce
#'(lambda (x y) (concatenate 'string x " " y))
(coerce sentence 'list)))
(push (list sentence score) best-sentences)))
(setf sentence nil score 0)))))
best-sentences
best-sentences
#'(lambda (x y) (> (cadr x) (cadr y)))))
(if best-sentences
(replace-all
(reduce #'(lambda (x y) (concatenate 'string x " " y))
(mapcar #'(lambda (x) (car x)) best-sentences))
" ." ".")
Natural Language Processing
"<no summary>")))
The nested loops in lines 8 through 33 look a little complicated, so let’s walk through it. Our goal
is to calculate an importance score for each word token in the input text and to then select a few
sentences containing highly scored words. The outer loop is over the word tokens in the input text.
For each word token we loop over the list of categories, looking up the current word in each category
hash and incrementing the score for the current word token. As we increment the word token scores
we also look for sentence breaks and save sentences.
The complicated bit of code in lines 16 through 32 where I construct sentences and their scores, and
store sentences with a score above a threshold value in the list best-sentences. After the two nested
loops, in lines 34 through 44 we simply sort the sentences by score and select the “best” sentences for
the summary. The extracted sentences are no longer in their original order, which can have strange
effects, but I like seeing the most relevant sentences first.
Text Mining
Text mining in general refers to finding data in unstructured text. We have covered several text
mining techniques in this chapter:
• Named entity recognition - the NLP library covered in this chapter recognizes person
and place entity names. I leave it as an exercise for you to extend this library to handle
company and product names. You can start by collecting company and product names in
the files src/kbnlp/linguistic_data/names/names.companies and src/kbnlp/data/names/-
names.products and extend the library code.
• Categorizing text - you can increase the accuracy of categorization by adding more weighted
words/terms that support categories. If you are already using Java in the systems you build, I
recommend the Apache OpenNLP library that is more accurate than the simpler “bag of words”
approach I used in my Common Lisp NLP library. If you use Python, then I recommend that
you also try the NLTK library.
• Summarizing text.
In the next chapter I am going to cover another “data centric” topic: performing information
gathering on the web. You will likely find some synergy between being able to use NLP to create
structured data from unstructured text.
Information Gathering
This chapter covers information gathering on the web using data sources and general techniques that
I have found useful. When I was planning this new book edition I had intended to also cover some
basics for using the Semantic Web from Common Lisp, basically distilling some of the data from
my previous book “Practical Semantic Web and Linked Data Applications, Common Lisp Edition”
published in 2011. However since a free PDF is now available for that book⁵⁵ I decided to just refer
you to my previous work if you are interested in the Semantic Web and Linked Data. You can also
find the Java edition of this previous book on my web site.
Gathering information from the web in realtime has some real advantages:
• You don’t need to worry about storing data locally.
• Information is up to date (depending on which web data resources you choose to use).
There are also a few things to consider:
• Data on the web may have legal restrictions on its use so be sure to read the terms and
conditions on web sites that you would like to use.
• Authorship and validity of data may be questionable.
DBPedia Lookup Service
Wikipedia is a great source of information. As you may know, you can download a data dump of all
Wikipedia data⁵⁶ with or without version information and comments. When I want fast access to the
entire Wikipedia set of English language articles I choose the second option and just get the current
pages with no comments of versioning information. This is the direct download link for current
Wikipedia articles.⁵⁷ There are no comments or user pages in this GZIP file. This is not as much data
as you might think, only about 9 gigabytes compressed or about 42 gigabytes uncompressed.
To load and run an example, try:
(ql:quickload "dbpedia")
(dbpedia:dbpedia-lookup "berlin")
Wikipedia is a great resource to have on hand but I am going to show you in this section how to
access the Semantic Web version or Wikipedia, DBPedia⁵⁸ using the DBPedia Lookup Service in the
next code listing that shows the contents of the example file dbpedia-lookup.lisp in the directory
src/dbpedia:
⁵⁵http://markwatson.com/#books/
⁵⁶https://en.wikipedia.org/wiki/Wikipedia:Database_download
⁵⁷http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
⁵⁸http://dbpedia.org/
Information Gathering
(ql:quickload :drakma)
(ql:quickload :babel)
(ql:quickload :s-xml)
;; utility from http://cl-cookbook.sourceforge.net/strings.html#manip:
(defun replace-all (string part replacement &key (test #'char=))
"Returns a new string in which all the occurrences of the part
is replaced with replacement."
(with-output-to-string (out)
(loop with part-length = (length part)
for old-pos = 0 then (+ pos part-length)
for pos = (search part string
:start2 old-pos
:test test)
do (write-string string out
:start old-pos
:end (or pos (length string)))
when pos do (write-string replacement out)
while pos)))
(defstruct dbpedia-data uri label description)
(defun dbpedia-lookup (search-string)
(let* ((s-str (replace-all search-string " " "+"))
(s-uri
(concatenate
'string
"http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?QueryString="
s-str))
(response-body nil)
(response-status nil)
(response-headers nil)
(xml nil)
(multiple-value-setq
(response-body response-status response-headers)
(drakma:http-request
:method :get
:accept "application/xml"))
;; (print (list "raw response body as XML:" response-body))
;;(print (list ("status:" response-status "headers:" response-headers)))
(setf xml
Information Gathering
(s-xml:parse-xml-string
(babel:octets-to-string response-body)))
(dolist (r (cdr xml))
;; assumption: data is returned in the order:
1. label
2. DBPedia URI for more information
3. description
(make-dbpedia-data
:uri (cadr (nth 2 r))
:label (cadr (nth 1 r))
:description
(string-trim
'(#\Space #\NewLine #\Tab)
(cadr (nth 3 r))))
(reverse ret)))
;; (dbpedia-lookup "berlin")
I am only capturing the attributes for DBPedia URI, label and description in this example code. If
you uncomment line 41 and look at the entire response body from the call to DBPedia Lookup, you
can see other attributes that you might want to capture in your applications.
Here is a sample call to the function dbpedia:dbpedia-lookup (only some of the returned data is
shown):
* (ql:quickload "dbpedia")
* (dbpedia:dbpedia-lookup "berlin")
(#S(DBPEDIA-DATA
:URI "http://dbpedia.org/resource/Berlin"
:LABEL "Berlin"
:DESCRIPTION
"Berlin is the capital city of Germany and one of the 16 states of Germany.
With a population of 3.5 million people, Berlin is Germany's largest city
and is the second most populous city proper and the eighth most populous
urban area in the European Union. Located in northeastern Germany, it is
the center of the Berlin-Brandenburg Metropolitan Region, which has 5.9
million residents from over 190 nations. Located in the European Plains,
Berlin is influenced by a temperate seasonal climate.")
Wikipedia, and the DBPedia linked data for of Wikipedia are great sources of online data. If you
Information Gathering
get creative, you will be able to think of ways to modify the systems you build to pull data from
DPPedia. One warning: Semantic Web/Linked Data sources on the web are not available 100% of
the time. If your business applications depend on having the DBPedia always available then you can
follow the instructions on the DBPedia web site⁵⁹ to install the service on one of your own servers.
Web Spiders
When you write web spiders to collect data from the web there are two things to consider:
• Make sure you read the terms of service for web sites whose data you want to use. I have found
that calling or emailing web site owners explaining how I want to use the data on their site
usually works to get permission.
• Make sure you don’t access a site too quickly. It is polite to wait a second or two between
fetching pages and other assets from a web site.
We have already used the Drakma web client library in this book. See the files src/dbpedia/dbpedia-
lookup.lisp (covered in the last section) and src/solr_examples/solr-client.lisp (covered in the
Chapter on NoSQL). Paul Nathan has written library using Drakma to crawl a web site with an
example to print out links as they are found. His code is available under the AGPL license at
articulate-lisp.com/src/web-trotter.lisp⁶⁰ and I recommend that as a starting point.
I find it is sometimes easier during development to make local copies of a web site so that I don’t
have to use excess resources from web site hosts. Assuming that you have the wget utility installed,
you can mirror a site like this:
wget -m -w 2 http://knowledgebooks.com/
wget -mk -w 2 http://knowledgebooks.com/
Both of these examples have a two-second delay between HTTP requests for resources. The option
-m indicates to recursively follow all links on the web site. The -w 2 option delays for two seconds
between requests. The option -mk converts URI references to local file references on your local
mirror. The second example on line 2 is more convenient.
We covered reading from local files in the Chapter on Input and Output. One trick I use is to simply
concatenate all web pages into one file. Assuming that you created a local mirror of a web site, cd
to the top level directory and use something like this:
cd knowledgebooks.com
cat *.html */*.html > ../web_site.html
You can then open the file, search for text in in p, div, h1, etc. HTML elements to process an entire
web site as one file.
⁵⁹http://dbpedia.org
⁶⁰http://articulate-lisp.com/examples/trotter.html
Information Gathering
Using Apache Nutch
Apache Nutch⁶¹, like Solr, is built on Lucene search technology. I use Nutch as a “search engine in a
box” when I need to spider web sites and I want a local copy with a good search index.
Nutch handles a different developer’s use case over Solr which we covered in the Chapter on NoSQL.
As we saw, Solr is an effective tool for indexing and searching structured data as documents. With
very little setup, Nutch can be set up to automatically keep an up to date index of a list of web sites,
and optionally follow links to some desired depth from these “seed” web sites.
You can use the same Common Lisp client code that we used for Solr with one exception; you will
need to change the root URI for the search service to:
http://localhost:8080/opensearch?query=
So the modified client code src/solr_examples/solr-client.lisp needs one line changed:
(defun do-search (&rest terms)
(let ((query-string (format nil "~{~A~^+AND+~}" terms)))
(cl-json:decode-json-from-string
(drakma:http-request
(concatenate
'string
"http://localhost:8080/opensearch?query="
query-string
"&wt=json")))))
Early versions of Nutch were very simple to install and configure. Later versions of Nutch have been
more complex, more performant, and have more services, but it will take you longer to get set up
than earlier versions. If you just want to experiment with Nutch, you might want to start with an
earlier version.
The OpenSearch.org⁶² web site contains many public OpenSearch services that you might want to
try. If you want to modify the example client code in src/solr-client.lisp a good start is OpenSearch
services that return JSON data and OpenSearch Community JSON formats web page⁶³ is a good
place to start. Some of the services on this web page like the New York Times service require that
you sign up for a developer’s API key.
When I start writing an application that requires web data (no matter which programming language
I am using) I start by finding services that may provide the type of data I need and do my initial
development with a web browser with plugin support to nicely format XML and JSON data. I do a
lot of exploring and take a lot of notes before I write any code.
⁶¹https://nutch.apache.org/
⁶²http://www.opensearch.org/Home
⁶³http://www.opensearch.org/Community/JSON_Formats
Information Gathering
Wrap Up
I tried to provide some examples and advice in this short chapter to show you that even though
other languages like Ruby and Python have more libraries and tools for gathering information from
the web, Common Lisp has good libraries for information gathering also and they are easily used
via Quicklisp.
Using The CL Machine-Learning
Library
The CL Machine-Learning (CLML) library was originally developed by MSI (NTT DATA Mathemat-
ical Systems Inc. in Japan) and is supported by many developers. You should visit the CLML web
page⁶⁴ for project documentation and follow the installation directions and read about the project
before using the examples in this chapter. However if you just want to quickly try the following
CLML examples then you can install CLML using Quicklisp:
mkdir -p ~/quicklisp/local-projects
cd ~/quicklisp/local-projects
git clone https://github.com/mmaul/clml.git
sbcl --dynamic-space-size 2560
> (ql:quickload :clml :verbose t)
The installation will take a while to run but after installation using the libraries via quickload is fast.
You can now run the example Quicklisp project src/clml_examples:
$ sbcl --dynamic-space-size 2560
* (ql:quickload "clmltest")
* (clmltest:clml-tests-example)
Please be patient the first time you run this because the first time you load the example project, the
one time installation of CLML will take a while to run but after installation then the example project
loads quickly. CLML installation involves downloading and installing BLAS, LAPACK, and other
libraries.
Other resources for CLML are the tutorials⁶⁵ and contributed extensions⁶⁶ that include support for
plotting (using several libraries) and for fetching data sets.
Although CLML is fairly portable we will be using SBCL and we need to increase the heap space
when starting SBCL when we want to use the CLML library:
sbcl --dynamic-space-size 5000
⁶⁴https://github.com/mmaul/clml
⁶⁵https://github.com/mmaul/clml.tutorials
⁶⁶https://github.com/mmaul/clml.extras
Using The CL Machine-Learning Library
You can refer to the documentation at https://github.com/mmaul/clml⁶⁷. This documentation lists
the packages with some information for each package but realistically I keep the source code for
CLML in an editor or IDE and read source code while writing code that uses CLML. I will show you
with short examples how to use the KNN (K nearest neighbors) and SVM (support vector machines)
APIs. We will not cover other useful CLML APIs like time series processing, Naive Bayes, PCA
(principle component analysis) and general matrix and tensor operations.
Even though the learning curve is a bit steep, CLML provides a lot of functionality for machine
learning, dealing with time series data, and general matrix and tensor operations.
Using the CLML Data Loading and Access APIs
The CLML project uses several data sets and since the few that we will use are small files, they are
included in the book’s repository in directory machine_learning_data under the src directory. The
first few lines of labeled_cancer_training_data.csv are:
Cl.thickness,Cell.size,Cell.shape,Marg.adhesion,Epith.c.size,Bare.nuclei,Bl.cromatin\
,Normal.nucleoli,Mitoses,Class
5,4,4,5,7,10,3,2,1,benign
6,8,8,1,3,4,3,7,1,benign
8,10,10,8,7,10,9,7,1,malignant
2,1,2,1,2,1,3,1,1,benign
The first line in the CSV data files specifies names for each attribute with the name of the last
column being “Class” which here takes on values benign or malignant. Later, the goal will be to
create models that are constructed from training data and then make predictions of the “Class” of
new input data. We will look at how to build and use machine learning models later but here we
concentrate on reading and using input data.
The example file clml_data_apis.lisp shows how to open a file and loop over the values for each
;; note; run SBCL using: sbcl --dynamic-space-size 2560
(ql:quickload '(:clml
:clml.hjs)) ; read data sets
(defpackage #:clml-data-test
(:use #:cl #:clml.hjs.read-data))
(in-package #:clml-data-test)
⁶⁷https://github.com/mmaul/clml
Using The CL Machine-Learning Library
(defun read-data ()
(let ((train1
(clml.hjs.read-data:read-data-from-file
"./machine_learning_data/labeled_cancer_training_data.csv"
:type :csv
:csv-type-spec (append
(make-list 9 :initial-element 'double-float)
'(symbol)))))
(loop-over-and-print-data train1)))
(defun loop-over-and-print-data (clml-data-set)
(print "Loop over and print a CLML data set:")
(let ((testdata (clml.hjs.read-data:dataset-points clml-data-set)))
(loop for td across testdata
(print td))))
(read-data)
The function read-data defined in lines 11-19 uses the utility function clml.hjs.read-data:read-
data-from-file to read a CSV (comma separated value) spreadsheet file from disk. The CSV file is
expected to contain 10 columns (set in lines 17-18) with the first nine columns containing floating
point values and the last column text data.
The function loop-over-and-print-data defined in lines 21-26 reads the CLML data set object,
looping over each data sample (i.e., each row in the original spreadsheet file) and printing it.
Here is some output from loading this file:
$ sbcl --dynamic-space-size 2560
This is SBCL 1.3.16, an implementation of ANSI Common Lisp.
More information about SBCL is available at <http://www.sbcl.org/>.
SBCL is free software, provided as is, with absolutely no warranty.
It is mostly in the public domain; some portions are provided under
BSD-style licenses.
See the CREDITS and COPYING files in the
distribution for more information.
* (load "clml_data_apis.lisp")
"Loop over and print a CLML data set:"
#(5.0d0 4.0d0 4.0d0 5.0d0 7.0d0 10.0d0 3.0d0 2.0d0 1.0d0 |benign|)
#(6.0d0 8.0d0 8.0d0 1.0d0 3.0d0 4.0d0 3.0d0 7.0d0 1.0d0 |benign|)
Using The CL Machine-Learning Library
#(8.0d0 10.0d0 10.0d0 8.0d0 7.0d0 10.0d0 9.0d0 7.0d0 1.0d0 |malignant|)
#(2.0d0 1.0d0 2.0d0 1.0d0 2.0d0 1.0d0 3.0d0 1.0d0 1.0d0 |benign|)
In the next section we will use the same cancer data training file, and another test data in the same
format to cluster this cancer data into similar sets, one set for non-malignant and one for malignant
samples.
K-Means Clustering of Cancer Data Set
We will now read the same University of Wisconsin cancer data set and cluster the input samples
(one sample per row of the spreadsheet file) into similar classes. We will find after training a model
that the data is separated into two clusters, representing non-malignant and malignant samples.
The function cancer-data-cluster-example-read-data defined in lines 33-47 is very similar to the
function read-data in the last section except here we read in two data files: one for training and one
for testing.
The function cluster-using-k-nn defined in lines 13-30 uses the training and test data objects to first
train a model and then to test it with test data that was previously used for training. Notice how
we call this function in line 47: the first two arguments are the two data set objects, the third is the
string “Class” that is the label for the 10th column of the original spreadsheet CSV files, and the last
argument is the type of distance measurement used to compare two data samples (i.e., comparing
any two rows of the training CSV data file).
;; note; run SBCL using: sbcl --dynamic-space-size 2560
(ql:quickload '(:clml
:clml.hjs ; utilities
:clml.clustering))
(defpackage #:clml-knn-cluster-example1
(:use #:cl #:clml.hjs.read-data))
(in-package #:clml-knn-cluster-example1)
;; folowing is derived from test code in CLML:
(defun cluster-using-k-nn (test train objective-param-name
manhattan)
(let (original-data-column-length)
(setq original-data-column-length
(length (aref (clml.hjs.read-data:dataset-points train) 0)))
(let* ((k 5)
(k-nn-estimator
(clml.nearest-search.k-nn:k-nn-analyze train
Using The CL Machine-Learning Library
objective-param-name :all
:distance manhattan :normalize t)))
(loop for data across
(dataset-points
(clml.nearest-search.k-nn:k-nn-estimate k-nn-estimator test))
if (equal (aref data 0) (aref data original-data-column-length))
(format t "Correct: ~a~%" data)
else do
(format t "Wrong:
~a~%" data)))))
;; folowing is derived from test code in CLML:
(defun cancer-data-cluster-example-read-data ()
(let ((train1
(clml.hjs.read-data:read-data-from-file
"./machine_learning_data/labeled_cancer_training_data.csv"
:type :csv
:csv-type-spec (append (make-list 9 :initial-element 'double-float)
'(symbol))))
(test1
(clml.hjs.read-data:read-data-from-file
"./machine_learning_data/labeled_cancer_test_data.csv"
:type :csv
:csv-type-spec (append (make-list 9 :initial-element 'double-float)
'(symbol)))))
;;(print test1)
(print (cluster-using-k-nn test1 train1 "Class" :double-manhattan))))
(cancer-data-cluster-example-read-data)
The following listing shows the output from running the last code example:
Number of self-misjudgement : 13
Correct: #(benign 5.0d0 1.0d0 1.0d0 1.0d0 2.0d0 1.0d0 3.0d0 1.0d0 1.0d0 benign)
Correct: #(benign 3.0d0 1.0d0 1.0d0 1.0d0 2.0d0 2.0d0 3.0d0 1.0d0 1.0d0 benign)
Correct: #(benign 4.0d0 1.0d0 1.0d0 3.0d0 2.0d0 1.0d0 3.0d0 1.0d0 1.0d0 benign)
Correct: #(benign 1.0d0 1.0d0 1.0d0 1.0d0 2.0d0 10.0d0 3.0d0 1.0d0 1.0d0 benign)
Correct: #(benign 2.0d0 1.0d0 1.0d0 1.0d0 2.0d0 1.0d0 1.0d0 1.0d0 5.0d0 benign)
Correct: #(benign 1.0d0 1.0d0 1.0d0 1.0d0 1.0d0 1.0d0 3.0d0 1.0d0 1.0d0 benign)
Wrong:
#(benign 5.0d0 3.0d0 3.0d0 3.0d0 2.0d0 3.0d0 4.0d0 4.0d0 1.0d0
malignant)
Correct: #(malignant 8.0d0 7.0d0 5.0d0 10.0d0 7.0d0 9.0d0 5.0d0 5.0d0 4.0d0
Using The CL Machine-Learning Library
malignant)
Correct: #(benign 4.0d0 1.0d0 1.0d0 1.0d0 2.0d0 1.0d0 2.0d0 1.0d0 1.0d0 benign)
Correct: #(malignant 10.0d0 7.0d0 7.0d0 6.0d0 4.0d0 10.0d0 4.0d0 1.0d0 2.0d0
malignant)
SVM Classification of Cancer Data Set
We will now reuse the same cancer data set but use a different way to classify data into non-
malignant and malignant categories: Support Vector Machines (SVM). SVMs are linear classifiers
which means that they work best when data is linearly separable. In the case of the cancer data, there
are nine dimensions of values that (hopefully) predict one of the two output classes (or categories).
If we think of the first 9 columns of data as defining a 9-dimensional space, then SVM will work well
when a 8-dimensional hyperplane separates the samples into the two output classes (categories).
To make this simpler to visualize, if we just had two input columns, that defines a two-dimensional
space, and if a straight line can separate most of the examples into the two output categories, then
the data is linearly separable so SVM is a good technique to use. The SVM algorithm is effectively
determining the parameters defining this one-dimensional line (or in the cancer data case, the 9-
dimensional hyperspace).
What if data is not linearly separable? Then use the backpropagation neural network code in the
chapter “Backpropagation Neural Networks” or the deep learning code in the chapter “Using Armed
Bear Common Lisp With DeepLearning4j” to create a model.
SVM is very efficient so it often makes sense to first try SVM and if trained models are not accurate
enough then use neural networks, including deep learning.
The following listing of file clml_svm_classifier.lisp shows how to read data, build a model and
evaluate the model with different test data. In line 15 we use the function clml.svm.mu:svm that
requires the type of kernel function to use, the training data, and testing data. Just for reference, we
usually use Gaussian kernel functions for processing numeric data and linear kernel functions for
handling text in natural language processing applications. Here we use a Gaussian kernel.
The function cancer-data-svm-example-read-data defined on line 40 differs from how we read and
processed data earlier because we need to separate out the positive and negative training examples.
The data is split in the lexically scoped function in lines 42-52. The last block of code in lines 54-82
is just top-level test code that gets executed when the file clml_svm_classifier.lisp is loaded.
Using The CL Machine-Learning Library
;; note; run SBCL using: sbcl --dynamic-space-size 2560
(ql:quickload '(:clml
:clml.hjs ; utilities
:clml.svm))
(defpackage #:clml-svm-classifier-example1
(:use #:cl #:clml.hjs.read-data))
(in-package #:clml-svm-classifier-example1)
(defun svm-classifier-test (kernel train test)
"train and test are lists of lists, with first elements being negative
samples and the second elements being positive samples"
(let ((decision-function (clml.svm.mu:svm kernel (cadr train) (car train)))
(correct-positives 0)
(wrong-positives 0)
(correct-negatives 0)
(wrong-negatives 0))
;; type: #<CLOSURE (LAMBDA (CLML.SVM.MU::Z) :IN CLML.SVM.MU::DECISION)>
(print decision-function)
(princ "***** NEGATIVE TESTS: calling decision function:")
(terpri)
(dolist (neg (car test))
;; negative test examples
(let ((prediction (funcall decision-function neg)))
(print prediction)
(if prediction (incf wrong-negatives) (incf correct-negatives))))
(princ "***** POSITIVE TESTS: calling decision function:")
(terpri)
(dolist (pos (cadr test)) ;; positive test examples
(let ((prediction (funcall decision-function pos)))
(print prediction)
(if prediction (incf correct-positives) (incf wrong-positives))))
(format t "Number of correct negatives ~a~%" correct-negatives)
(format t "Number of wrong negatives ~a~%" wrong-negatives)
(format t "Number of correct positives ~a~%" correct-positives)
(format t "Number of wrong positives ~a~%" wrong-positives)))
(defun cancer-data-svm-example-read-data ()
(defun split-positive-negative-cases (data)
(let ((negative-cases '())
Using The CL Machine-Learning Library
(positive-cases '()))
(dolist (d data)
;;(print (list "*
d=" d))
(if (equal (symbol-name (first (last d))) "benign")
(setf negative-cases
(cons (reverse (cdr (reverse d))) negative-cases))
(setf positive-cases
(cons (reverse (cdr (reverse d))) positive-cases))))
(list negative-cases positive-cases)))
(let* ((train1
(clml.hjs.read-data:read-data-from-file
"./machine_learning_data/labeled_cancer_training_data.csv"
:type :csv
:csv-type-spec (append (make-list 9 :initial-element 'double-float)
'(symbol))))
(train-as-list
(split-positive-negative-cases
(coerce
(map 'list
#'(lambda (x) (coerce x 'list))
(coerce (clml.hjs.read-data:dataset-points train1) 'list))
'list)))
(test1
(clml.hjs.read-data:read-data-from-file
"./machine_learning_data/labeled_cancer_test_data.csv"
:type :csv
:csv-type-spec (append (make-list 9 :initial-element 'double-float)
'(symbol))))
(test-as-list
(split-positive-negative-cases
(coerce
(map 'list
#'(lambda (x) (coerce x 'list))
(coerce (clml.hjs.read-data:dataset-points test1) 'list))
'list))))
;; we will use a gaussian kernel for numeric data.
;; note: for text classification, use a clml.svm.mu:+linear-kernel+
(svm-classifier-test
(clml.svm.mu:gaussian-kernel 2.0d0)
train-as-list test-as-list)))
Using The CL Machine-Learning Library
(cancer-data-svm-example-read-data)
The sample code prints the prediction values for the test data which I will not show here. Here are
the last four lines of output showing the cumulative statistics for the test data:
Number of correct negatives 219
Number of wrong negatives 4
Number of correct positives 116
Number of wrong positives 6
CLML Wrap Up
The CLML machine learning library is under fairly active development and I showed you enough to
get started: understanding the data APIs and examples for KNN clustering and SVM classification.
A good alternative to CLML is MGL⁶⁸ that supports backpropagation neural networks, boltzmann
machines, and gaussian processes.
In the next two chapters we continue with the topic of machine learning with backpropagation andf
Hopfield neural networks.
⁶⁸https://github.com/melisgl/mgl
Backpropagation Neural Networks
Let’s start with an overview of how these networks work and then fill in more detail later.
Backpropagation networks are trained by applying training inputs to the network input layer,
propagate values through the network to the output neurons, compare the errors (or differences)
between these propagated output values and the training data output values. These output errors
are backpropagated though the network and the magnitude of backpropagated errors are used to
adjust the weights in the network.
The example we look at here uses the plotlib package from an earlier chapter and the source code
for the example is the file loving_snippet/backprop_neural_network.lisp.
We will use the following diagram to make this process more clear. There are four weights in this
very simple network:
• W¹,¹ is the floating point number representing the connection strength between input_neuron¹
and output_neuron¹
• W²,¹ connects input_neuron² to output_neuron¹
• W¹,² connects input_neuron¹ to output_neuron²
• W²,² connects input_neuron² to output_neuron²
Understanding how connection weights connect neurons in adjacent layers
Backpropagation Neural Networks
Before any training the weight values are all small random numbers.
Consider a training data element where the input neurons have values [0.1, 0.9] and the desired
output neuron values are [0.9 and 0.1], that is flipping the input values. If the propagated output
values for the current weights are [0.85, 0.5] then the value of the first output neuron has a small
error abs(0.85 - 0.9) which is 0.05. However the propagated error of the second output neuron is high:
abs(0.5 - 0.1) which is 0.4. Informally we see that the weights feeding input output neuron 1 (W¹,¹
and W²,¹) don’t need to be changed much but the neuron that feeding input neuron 2 (W¹,² and W²,²)
needs modification (the value of W²,² is too large).
Of course, we would never try to manually train a network like this but it is important to have at least
an informal understanding of how weights connect the flow of value (we will call this activation
value later) between neurons.
In this neural network see in the first figure we have four weights connecting the input and output
neurons. Think of these four weights forming a four-dimensional space where the range in each
dimension is constrained to small positive and negative floating point values. At any point in this
“weight space”, the numeric values of the weights defines a model that maps the inputs to the outputs.
The error seen at the output neurons is accumulated for each training example (applied to the input
neurons). The training process is finding a point in this four-dimensional space that has low errors
summed across the training data. We will use gradient descent to start with a random point in the
four-dimensional space (i.e., an initial random set of weights) and move the point towards a local
minimum that represents the weights in a model that is (hopefully) “good enough” at representing
the training data.
This process is simple enough but there are a few practical considerations:
• Sometimes the accumulated error at a local minimum is too large even after many training
cycles and it is best to just restart the training process with new random weights.
• If we don’t have enough training data then the network may have enough memory capacity
to memorize the training examples. This is not what we want: we want a model with just
enough memory capacity (as represented by the number of weights) to form a generalized
predictive model, but not so specific that it just memorizes the training examples. The solution
is to start with small networks (few hidden neurons) and increase the number of neurons until
the training data can be learned. In general, having a lot of training data is good and it is also
good to use as small a network as possible.
In practice using backpropagation networks is an iterative process of experimenting with the size of
a network.
In the example program (in the file backprop_neural_network.lisp) we use the plotting library
developed earlier to visualize neuron activation and connecting weight values while the network
trains.
The following three screen shots from running the function test3 defined at the bottom of the
file backprop_neural_network.lisp illustrate the process of starting with random weights, getting
Backpropagation Neural Networks
random outputs during initial training, and as delta weights are used to adjust the weights in a
network, then the training examples are learned:
At the start of the training run with random weights and large delta weights
In the last figure the initial weights are random so we get random mid-range values at the output
neurons.
The trained weights start to produce non-random output
As we start to train the network, adjusting the weights, we start to see variation in the output neurons
as a function of what the inputs are.
Backpropagation Neural Networks
After training many cycles the training examples are learned, with only small output errors
In the last figure the network is trained sufficiently well to map inputs [0, 0, 0, 1] to output values
that are approximately [0.8, 0.2, 0.2, 0.3] which is close to the expected value [1, 0, 0, 0].
The example source file backprop_neural_network.lisp is long so we will only look at the more
interesting parts here. Specifically we will not look at the code to plot neural networks using plotlib.
The activation values of individual neurons are limited to the range [0, 1] by first calculating their
values based on the sum activation values of neurons in the previous layer times the values of the
connecting weights and then using the Sigmoid function to map the sums to the desired range. The
Sigmoid function and the derivative of the Sigmoid function (dSigmoid) look like:
Sigmoid and Derivative of the Sigmid Functions
Here are the definitions of these functions:
Backpropagation Neural Networks
(defun Sigmoid (x)
(/ 1.0 (+ 1.0 (exp (- x)))))
(defun dSigmoid (x)
(let ((temp (Sigmoid x)))
(* temp (- 1.0 temp)))
The function NewDeltaNetwork creates a new neual network object. This code allocates storage
for input, hidden, output layers (I sometimes refer to neuron layers as “slabs”), and the connection
weights. Connection weights are initialized to small random values.
; (NewDeltaNetwork sizeList)
sizeList = list of sizes of slabs. This also defines
the number of slabs in the network.
(e.g.,
'(10 5 4) ==> a 3-slab network with 10
input neurons, 5 hidden neurons, and 4 output
neurons).
Returned value = a list describing the network:
(nLayers sizeList
(activation-array[1] .. activation-array[nLayers])
(weight-array[2] .. weight-array[nLayers])
(sum-of-products[2] .. sum-of-products[nLayers[nLayers])
(back-prop-error[2] .. back-prop-error[nLayers]))
(old-delta-weights[2] .. for momentum term
:initial-element 0.0))
(reverse old-dw-list)))
Initialize values for all activations:
(lambda (x)
(let ((num (array-dimension x 0)))
(dotimes (n num)
(setf (aref x n) (frandom 0.01 0.1)))))
a-list)
Initialize values for all weights:
Backpropagation Neural Networks
(lambda (x)
(let ((numI (array-dimension x 0))
(numJ (array-dimension x 1)))
(dotimes (j numJ)
(dotimes (i numI)
(setf (aref x i j) (frandom -0.5 0.5))))))
w-list)
(list numLayers sizeList a-list s-list w-list dw-list
d-list old-dw-list alpha beta)))
In the following listing the function DeltaLearn processes one pass through all of the training data.
Function DeltaLearn is called repeatedly until the return value is below a desired error threshold.
The main loop over each training example is implemented in lines 69-187. Inside this outer loop there
are two phases of training for each training example: a forward pass propagating activation from
the input neurons to the output neurons via any hidden layers (lines 87-143) and then the weight
correcting backpropagation of output errors while making small adjustments to weights (lines 148-
Utility function for training a delta rule neural network.
The first argument is the name of an output PNG plot file
and a nil value turns off plotting the network during training.
The second argument is a network definition (as returned from
NewDeltaNetwork), the third argument is a list of training
data cases (see the example test functions at the end of this
file for examples.
(defun DeltaLearn (plot-output-file-name
netList trainList)
(let ((nLayers (car netList))
(sizeList (cadr netList))
(activationList (caddr netList))
(sumOfProductsList (car (cdddr netList)))
(weightList (cadr (cdddr netList)))
(deltaWeightList (caddr (cdddr netList)))
(deltaList (cadddr (cdddr netList)))
(oldDeltaWeightList (cadddr (cdddr (cdr netList))))
(alpha (cadddr (cdddr (cddr netList))))
(beta (cadddr (cdddr (cdddr netList))))
(inputs nil)
(targetOutputs nil)
(iDimension nil)
Backpropagation Neural Networks
(jDimension nil)
(iActivationVector nil)
(jActivationVector nil)
(n nil)
(weightArray nil)
(sumOfProductsArray nil)
(iDeltaVector nil)
(jDeltaVector nil)
(deltaWeightArray nil)
(oldDeltaWeightArray nil)
(sum nil)
(iSumOfProductsArray nil)
(error nil)
(outputError 0)
(delta nil)
(eida nil)
(inputNoise 0))
; Zero out deltas:
(dotimes (n (- nLayers 1))
(let* ((dw (nth n deltaList))
(len1 (array-dimension dw 0)))
(dotimes (i len1)
(setf (aref dw i) 0.0))))
; Zero out delta weights:
(dotimes (n (- nLayers 1))
(let* ((dw (nth n deltaWeightList))
(len1 (array-dimension dw 0))
(len2 (array-dimension dw 1)))
(dotimes (i len1)
(dotimes (j len2)
(setf (aref dw i j) 0.0)))))
(setq inputNoise *delta-default-input-noise-value*)
Main loop on training examples:
Backpropagation Neural Networks
(dolist (tl trainList)
(setq inputs (car tl))
(setq targetOutputs (cadr tl))
(if *delta-rule-debug-flag*
(print (list "Current targets:" targetOutputs)))
(setq iDimension (car sizeList)) ; get the size of the input slab
(setq iActivationVector (car activationList)) ; input activations
(dotimes (i iDimension) ; copy training inputs to input slab
(aref iActivationVector i)
(+ (nth i inputs) (frandom (- inputNoise) inputNoise))))
; Propagate activation through all of the slabs:
(dotimes (n-1 (- nLayers 1))
; update layer i to layer flowing to layer j
(setq n (+ n-1 1))
(setq jDimension (nth n sizeList)) ; get the size of the j'th layer
(setq jActivationVector (nth n activationList)) ; activation
for slab j
(setq weightArray (nth n-1 weightList))
(setq sumOfProductsArray (nth n-1 sumOfProductsList))
(dotimes (j jDimension) ; process each neuron in slab j
(setq sum 0.0) ; init sum of products to zero
(dotimes (i iDimension) ; activation from neurons in previous slab
(+ sum (* (aref weightArray i j) (aref iActivationVector i)))))
(setf (aref sumOfProductsArray j) sum) ; save sum of products
(setf (aref jActivationVector j) (Sigmoid sum)))
(setq iDimension jDimension)
; reset index for next slab pair
(setq iActivationVector jActivationVector))
; Activation is
spread through the network and sum of products
; calculated. Now modify the weights in the network using back
; error propagation. Start by calculating the error signal for
; each neuron in the output layer:
(setq jDimension (nth (- nLayers 1) sizeList)) ; size of last layer
(setq jActivationVector (nth (- nLayers 1) activationList))
(setq jDeltaVector (nth (- nLayers 2) deltaList))
(setq sumOfProductsArray (nth (- nLayers 2) sumOfProductsList))
Backpropagation Neural Networks
(setq outputError 0)
(dotimes (j jDimension)
(setq delta (- (nth j targetOutputs) (aref jActivationVector j)))
(setq outputError (+ outputError (abs delta)))
(aref jDeltaVector j)
(aref jDeltaVector j)
(* delta (dSigmoid (aref sumOfProductsArray j))))))
; Now calculate the backpropagated error signal for all hidden slabs:
(dotimes (nn (- nLayers 2))
(setq n (- nLayers 3 nn))
(setq iDimension (nth (+ n 1) sizeList))
(setq iSumOfProductsArray (nth n sumOfProductsList))
(setq iDeltaVector (nth n deltaList))
(dotimes (i iDimension)
(setf (aref iDeltaVector i) 0.0))
(setq weightArray (nth (+ n 1) weightList))
(dotimes (i iDimension)
(setq error 0.0)
(dotimes (j jDimension)
(setq error
(+ error (* (aref jDeltaVector j) (aref weightArray i j)))))
(aref iDeltaVector i)
(aref iDeltaVector i)
(* error (dSigmoid (aref iSumOfProductsArray i))))))
(setq jDimension iDimension)
(setq jDeltaVector iDeltaVector))
; Update all delta weights in the network:
(setq iDimension (car sizeList))
(dotimes (n (- nLayers 1))
(setq iActivationVector (nth n activationList))
(setq jDimension (nth (+ n 1) sizeList))
(setq jDeltaVector (nth n deltaList))
(setq deltaWeightArray (nth n deltaWeightList))
(setq weightArray (nth n weightList))
Backpropagation Neural Networks
(setq eida (nth n eidaList))
(dotimes (j jDimension)
(dotimes (i iDimension)
(setq delta (* eida (aref jDeltaVector j) (aref iActivationVector i)))
(aref DeltaWeightArray i j)
(+ (aref DeltaWeightArray i j) delta)))) ; delta weight changes
(setq iDimension jDimension))
; Update all weights in the network:
(setq iDimension (car sizeList))
(dotimes (n (- nLayers 1))
(setq iActivationVector (nth n activationList))
(setq jDimension (nth (+ n 1) sizeList))
(setq jDeltaVector (nth n deltaList))
(setq deltaWeightArray (nth n deltaWeightList))
(setq oldDeltaWeightArray (nth n oldDeltaWeightList))
(setq weightArray (nth n weightList))
(dotimes (j jDimension)
(dotimes (i iDimension)
(aref weightArray i j)
(+ (aref weightArray i j)
(* alpha (aref deltaWeightArray i j))
(* beta
(aref oldDeltaWeightArray i j))))
(setf (aref oldDeltaWeightArray i j) ; save current delta weights
(aref deltaWeightArray i j)))) ; ...for next momentum term.
(setq iDimension jDimension))
(if plot-output-file-name
(DeltaPlot netList plot-output-file-name)))
(/ outputError jDimension)))
The function DeltaRecall in the next listing can be used with a trained network to calculate outputs
for new input values:
Backpropagation Neural Networks
Utility for using a trained neural network in the recall mode.
The first argument to this function is a network definition (as
returned from NewDeltaNetwork) and the second argument is a list
of input neuron activation values to drive through the network.
The output is a list of the calculated activation energy for
each output neuron.
(defun DeltaRecall (netList inputs)
(let ((nLayers (car netList))
(sizeList (cadr netList))
(activationList (caddr netList))
(weightList (cadr (cdddr netList)))
(iDimension nil)
(jDimension nil)
(iActivationVector nil)
(jActivationVector nil)
(n nil)
(weightArray nil)
(returnList nil)
(sum nil))
(setq iDimension (car sizeList)) ; get the size of the input slab
(setq iActivationVector (car activationList)) ; get input activations
(dotimes (i iDimension) ; copy training inputs to input slab
(setf (aref iActivationVector i) (nth i inputs)))
(dotimes (n-1 (- nLayers 1))
; update layer j to layer i
(setq n (+ n-1 1))
(setq jDimension (nth n sizeList)) ; get the size of the j'th layer
(setq jActivationVector (nth n activationList)) ; activation for slab j
(setq weightArray (nth n-1 weightList))
(dotimes (j jDimension) ; process each neuron in slab j
(setq sum 0.0) ; init sum of products to zero
(dotimes (i iDimension) ; get activation from each neuron in last slab
(+ sum (* (aref weightArray i j) (aref iActivationVector i)))))
(if *delta-rule-debug-flag*
(print (list "sum=" sum)))
(setf (aref jActivationVector j) (Sigmoid sum)))
(setq iDimension jDimension) ; get ready for next slab pair
(setq iActivationVector jActivationVector))
(dotimes (j jDimension)
(setq returnList (append returnList (list (aref jActivationVector j)))))
Backpropagation Neural Networks
returnList))
We saw three output plots earlier that were produced during a training run using the following code:
(defun test3 (&optional (restart 'yes) &aux RMSerror) ; three layer network
(equal restart 'yes)
(setq temp (newdeltanetwork '(5 4 5))))
(dotimes (ii 3000)
(let ((file-name
(if (equal (mod ii 400) 0)
(concatenate 'string "output_plot_" (format nil "~12,'0d" ii) ".png")
nil)))
RMSerror
(deltalearn
file-name temp
'(((1 0 0 0 0) (0 1 0 0 0))
((0 1 0 0 0) (0 0 1 0 0))
((0 0 1 0 0) (0 0 0 1 0))
((0 0 0 1 0) (0 0 0 0 1))
((0 0 0 0 1) (1 0 0 0 0)))))
(if (equal (mod ii 50) 0) ;; print error out every 50 cycles
(progn
(princ "....training cycle \#")
(princ ii)
(princ " RMS error = ")
(princ RMSerror)
(terpri))))))
Here the function test3 defines training data for a very small test network for a moderately difficult
function to learn: to rotate the values in the input neurons to the right, wrapping around to the first
neuron. The start of the main loop in line calls the training function 3000 times, creating a plot of
the network every 400 times through the main loop.
Backpropagation networks have been used sucessfully in production for about 25 years. In the next
chapter we will look at a less practical type of network, Hopfield networks, that are still interesting
because the in some sense Hopfield networks model how our brains work. In the final chapter we
will look at deep learning neural networks.
Hopfield Neural Networks
A Hopfield network⁶⁹ (named after John Hopfield) is a recurrent network since the flow of activation
through the network has loops. These networks are trained by applying input patterns and letting
the network settle in a state that stores the input patterns.
The example code is in the file src/loving_snippets/Hopfield_neural_network.lisp.
The example we look at recognizes patterns that are similar to the patterns seen in training examples
and maps input patterns to a similar training input pattern. The following figure shows output from
the example program showing an original training pattern, a similar pattern with one cell turned
on and other off, and the reconstructed pattern:
To be clear, we have taken one of the original input patterns the network has learned, slightly altered
it, and applied it as input to the network. After cycling the network, the slightly scrambled input
pattern we just applied will be used as an associative memory key, look up the original pattern,
and rewrite to input values with the original learned pattern. These Hopfield networks are very
different than backpropagation networks: neuron activation are forced to values of -1 or +1 and not
be differentiable and there are no separate output neurons.
The next example has the values of three cells modified from the original and the original pattern is
still reconstructed correctly:
⁶⁹https://en.wikipedia.org/wiki/Hopfield_network
Hopfield Neural Networks
This last example has four of the original cells modified:
The following example program shows a type of content-addressable memory. After a Hopfield
network learns a set of input patterns then it can reconstruct the original paterns when shown
similar patterns. This reconstruction is not always perfecrt.
The following function Hopfield-Init (in file Hopfield_neural_network.lisp) is passed a list of lists of
training examples that will be remembered in the network. This function returns a list containing the
data defining a Hopfield neural network. All data for the network is encapsulated in the list returned
by this function, so multiple Hopfield neural networks can be used in an application program.
In lines 9-12 we allocate global arrays for data storage and in lines 14-18 the training data is copied.
The inner function adjustInput on lines 20-29 adjusts data values to values of -1.0 or +1.0. In lines
31-33 we are initializing all of the weights in the Hopfield network to zero.
The last nested loop, on lines 35-52, calculates the autocorrelation weight matrix from the input test
patterns.
On lines 54-56, the function returns a representation of the Hopfield network that will be used later in
the function HopfieldNetRecall to find the most similar “remembered” pattern given a new (fresh)
input pattern.
Hopfield Neural Networks
(defun Hopfield-Init (training-data
&aux temp *num-inputs* *num-training-examples*
*training-list* *inputCells* *tempStorage*
*HopfieldWeights*)
(setq *num-inputs* (length (car training-data)))
(setq *num-training-examples* (length training-data))
(setq *training-list* (make-array (list *num-training-examples* *num-inputs*)))
(setq *inputCells* (make-array (list *num-inputs*)))
(setq *tempStorage* (make-array (list *num-inputs*)))
(setq *HopfieldWeights* (make-array (list *num-inputs* *num-inputs*)))
(dotimes (j *num-training-examples*) ;; copy training data
(dotimes (i *num-inputs*)
(aref *training-list* j i)
(nth i (nth j training-data)))))
(defun adjustInput (value)
;; this function is lexically scoped
(if (< value 0.1)
+1.0))
(dotimes (i *num-inputs*) ;; adjust training data
(dotimes (n *num-training-examples*)
(aref *training-list* n i)
(adjustInput (aref *training-list* n i)))))
(dotimes (i *num-inputs*) ;; zero weights
(dotimes (j *num-inputs*)
(setf (aref *HopfieldWeights* i j) 0)))
(dotimes (j-1 (- *num-inputs* 1)) ;; autocorrelation weight matrix
(let ((j (+ j-1 1)))
(dotimes (i j)
(dotimes (s *num-training-examples*)
(setq temp
(truncate
;; 2 if's truncate values to -1 or 1:
(adjustInput (aref *training-list* s i))
Hopfield Neural Networks
(adjustInput (aref *training-list* s j)))
(aref *HopfieldWeights* i j))))
(setf (aref *HopfieldWeights* i j) temp)
(setf (aref *HopfieldWeights* j i) temp)))))
(dotimes (i *num-inputs*)
(setf (aref *tempStorage* i) 0)
(dotimes (j i)
(setf (aref *tempStorage* i)
(+ (aref *tempStorage* i) (aref *HopfieldWeights* i j)))))
(list ;; return the value of the Hopfield network data object
*num-inputs* *num-training-examples* *training-list*
*inputCells* *tempStorage* *HopfieldWeights*))
The following function HopfieldNetRecall iterates the network to let it settle in a stable pattern
which we hope will be the original training pattern most closely resembling the noisy test pattern.
The inner (lexically scoped) function deltaEnergy defined on lines 9-12 calculates a change in energy
from old input values and the autocorrelation weight matrix. The main code uses the inner functions
to iterate over the input cells, possibly modifying the cell at index i delta energy is greater than zero.
Remember that the lexically scoped inner functions have access to the variables for the number of
inputs, the number of training examples, the list of training examples, the input cell values, tempoary
storage, and the Hopfield network weights.
(defun HopfieldNetRecall (aHopfieldNetwork numberOfIterations)
(let ((*num-inputs* (nth 0 aHopfieldNetwork))
(*num-training-examples*
(nth 1 aHopfieldNetwork))
(*training-list* (nth 2 aHopfieldNetwork))
(*inputCells* (nth 3 aHopfieldNetwork))
(*tempStorage* (nth 4 aHopfieldNetwork))
(*HopfieldWeights* (nth 5 aHopfieldNetwork)))
(defun deltaEnergy (row-index y &aux (temp 0.0))
;; lexically scoped
(dotimes (j *num-inputs*)
(setq temp (+ temp (* (aref *HopfieldWeights* row-index j) (aref y j)))))
(- (* 2.0 temp) (aref *tempStorage* row-index)))
(dotimes (ii numberOfIterations) ;; main code
(dotimes (i *num-inputs*)
(setf (aref *inputCells* i)
(if (> (deltaEnergy i *inputCells*) 0)
0))))))
Hopfield Neural Networks
Function test in the next listing uses three different patterns for each test. Note that only the last
pattern gets plotted to the output graphics PNG file for the purpose of producing figures for this
chapter. If you want to produce plots of other patterns, edit just the third pattern defined on line
AAAAA. The following plotting functions are inner lexically scoped so they have access to the data
defined in the enclosing let expression in lines 16-21:
• plotExemplar - plots a vector of data
• plot-original-inputCells - plots the original input cells from training data
• plot-inputCells - plots the modified input cells (a few cells randomly flipped in value)
• modifyInput - scrambles training inputs
(defun test (&aux aHopfieldNetwork)
(let ((tdata '(
;; sample sine wave data with different periods:
(1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0)
(0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0)
(0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1)))
(width 300)
(height 180))
(vecto::with-canvas (:width width :height height)
(plotlib:plot-string-bold 10 (- height 14) "Hopfield pattern classifier")
;; Set up network:
(print tdata)
(setq aHopfieldNetwork (Hopfield-Init tdata))
;; lexically scoped variables are accesible by inner functions:
(let ((*num-inputs* (nth 0 aHopfieldNetwork))
(*num-training-examples*
(nth 1 aHopfieldNetwork))
(*training-list* (nth 2 aHopfieldNetwork))
(*inputCells* (nth 3 aHopfieldNetwork))
(*tempStorage* (nth 4 aHopfieldNetwork))
(*HopfieldWeights* (nth 5 aHopfieldNetwork)))
(defun plotExemplar (row &aux (dmin 0.0) (dmax 1.0) (x 20) (y 40))
(let ((YSize (array-dimension *training-list* 1)))
(plotlib:plot-string (+ x 20) (- height (- y 10))
"Original Training Exemplar")
(dotimes (j Ysize)
(plotlib:plot-fill-rect
(+ x (* j plot-size+1)) (- height y) plot-size plot-size
(truncate (*
(- (aref *training-list* row j) dmin)
Hopfield Neural Networks
(- dmax dmin))
(plotlib:plot-frame-rect (+ x (* j plot-size+1))
(- height y) plot-size plot-size))))
(defun plot-original-inputCells (&aux (dmin 0.0) (dmax 1.0) (x 20) (y 80))
(let ((Xsize (array-dimension *inputCells* 0)))
(plotlib:plot-string (+ x 20) (- height (- y 10)) "Scrambled Inputs")
(dotimes (j Xsize)
(plotlib:plot-fill-rect
(+ x (* j plot-size+1)) (- height y) plot-size plot-size
(truncate (*
(- (aref *inputCells* j) dmin) (- dmax dmin))
(plotlib:plot-frame-rect (+ x (* j plot-size+1))
(- height y) plot-size plot-size))))
(defun plot-inputCells (&aux (dmin 0.0) (dmax 1.0) (x 20) (y 120))
(let ((Xsize (array-dimension *inputCells* 0)))
(plotlib:plot-string (+ x 20) (- height (- y 10))
"Reconstructed Inputs")
(dotimes (j Xsize)
(plotlib:plot-fill-rect
(+ x (* j plot-size+1)) (- height y) plot-size plot-size
(truncate (* (/
(- (aref *inputCells* j) dmin)
(- dmax dmin))
(plotlib:plot-frame-rect
(+ x (* j plot-size+1)) (- height y) plot-size plot-size))))
(defun modifyInput (arrSize arr)
;; modify input array for testing
(dotimes (i arrSize)
(if (< (random 50) 5)
(if (> (aref arr i) 0)
(setf (aref arr i) -1)
(setf (aref arr i) 1)))))
;; Test network on training data that is randomly modified:
(dotimes (iter 10) ;; cycle 10 times and make 10 plots
(dotimes (s *num-training-examples*)
(dotimes (i *num-inputs*)
Hopfield Neural Networks
(setf (aref *inputCells* i) (aref *training-list* s i)))
(plotExemplar s)
(modifyInput *num-inputs* *inputCells*)
(plot-original-inputCells)
(dotimes (call-net 5) ;; iterate Hopfield net 5 times
(HopfieldNetRecall aHopfieldNetwork 1)
;; calling with 1 iteration
(plot-inputCells)))
(vecto::save-png
(concatenate
'string
"output_plot_hopfield_nn_" (format nil "~5,'0d" iter) ".png")))))))
The plotting functions in lines 23-62 use the plotlib library to make the plots you saw earlier. The
function modifyInput in lines 64-69 randomly flips the values of the input cells, taking an original
pattern and slightly modifying it.
Hopfield neural networks, at least to some extent, seem to model some aspects of human brains in
the sense that they can function as content-addressable (also called associative) memories. Ideally
a partial input pattern from a remembered input can reconstruct the complete original pattern.
Another interesting feature of Hopfield networks is that these memories really are stored in a
distributed fashion: some of the weights can be randomly altered and patterns are still remembered,
but with more recall errors.
Using Python Deep Learning Models
In Common Lisp With a Web Services
Interface
In older editions of this book I had an example of using the Java DeepLearning4J deep learning
library using Armed Bear Common Lisp, implemented in Java. I no longer use hybrid Java and
Common Lisp applications in my own work and I decided to remove this example and replace it
with two projects that use simple Python web services that act as wrappers for state of the art deep
learning models with Common Lisp clients in the subdirectories:
• src/spacy_web_client: use the spaCy deep learning models for general NLP. I sometimes use
my own pure Common Lisp NLP libraries we saw in earlier chapters and sometimes I use a
Common Lisp client calling deep learning libraries like spaCy and TensorFlow.
• src/coref_web_client: coreference or anaphora resolution is the act of replacing pronouns in
text with the original nouns that they refer to. This has traditionally been a very difficult and
only partially solved problem until recent advances in deep learning models like BERT.
Note: in the next chapter we will cover similar functionality but we will use the py4cl library to
more directly use Python and libraries like spaCy by starting another Python process and using
streams for communication.
Setting up the Python Web Services Used in this
Chapter
You will need python and pip installed on your system. The source e code for the Python web services
is found in the directory loving-common-lisp/python.
Installing the spaCY NLP Services
I assume that you have some familiarity with using Python. If not, you will still be able to follow these
directions assuming that you have the utilities pip, and python installed. I recommend installing
Python and Pip using Anaconda⁷⁰.
⁷⁰https://anaconda.org/anaconda/conda
Using Python Deep Learning Models In Common Lisp With a Web Services Interface
The server code is in the subdirectory python/python_spacy_nlp_server where you will work
when performing a one time initialization. After the server is installed you can then run it from the
command line from any directory on your laptop.
I recommend that you use virtual Python environments when using Python applications to separate
the dependencies required for each application or development project. Here I assume that you are
running in a Python version 3.6 or higher environment. First you must install the dependencies:
pip install -U spacy
python -m spacy download en
pip install falcon
Then change directory to the subdirectory python/python_spacy_nlp_server in the git repo for
this book and install the NLP server:
cd python/python_spacy_nlp_server
python setup.py install
Once you install the server, you can run it from any directory on your laptop or server using:
spacynlpserver
I use deep learning models written in Python using TensorFlow or PyTorch and provide Python
web services that can be used in applications I write in Haskell or Common Lisp using web client
interfaces for the services written in Python. While it is possible to directly embed models in Haskell
and Common Lisp, I find it much easier and developer friendly to wrap deep learning models I use
a REST services as I have done here. Often deep learning models only require about a gigabyte of
memory and using pre-trained models has lightweight CPU resource needs so while I am developing
on my laptop I might have two or three models running and available as wrapped REST services.
For production, I configure both the Python services and my Haskell and Common Lisp applications
to start automatically on system startup.
This is not a Python programming book and I will not discuss the simple Python wrapping code but
if you are also a Python developer you can easily read and understand the code.
Installing the Coreference NLP Services
I recommend that you use virtual Python environments when using Python applications to separate
the dependencies required for each application or development project. Here I assume that you are
running in a Python version 3.6 environment. First you should install the dependencies:
Using Python Deep Learning Models In Common Lisp With a Web Services Interface
pip install spacy==2.1.0
pip install neuralcoref
pip install falcon
As I write this chapter the neuralcoref model and library require a slightly older version of SpaCy
(the current latest version is 2.1.4).
Then change directory to the subdirectory python/python_coreference_anaphora_resolution_-
server in the git repo for this book and install the coref server:
cd python_coreference_anaphora_resolution_server
python setup.py install
Once you install the server, you can run it from any directory on your laptop or server using:
corefserver
While. as we saw in the last example, it is possible to directly embed models in Haskell and Common
Lisp, I find it much easier and developer friendly to wrap deep learning models I use a REST services
as I have done here. Often deep learning models only require about a gigabyte of memory and
using pre-trained models has lightweight CPU resource needs so while I am developing on my
laptop I might have two or three models running and available as wrapped REST services. For
production, I configure both the Python services and my Haskell and Common Lisp applications
to start automatically on system startup.
This is not a Python programming book and I will not discuss the simple Python wrapping code but
if you are also a Python developer you can easily read and understand the code.
Common Lisp Client for the spaCy NLP Web Services
Before looking at the code, I will show you typical output from running this example:
$ sbcl
This is SBCL 1.3.16, an implementation of ANSI Common Lisp.
* (ql:quickload "spacy-web-client")
To load "spacy":
Load 1 ASDF system:
spacy-web-client
; Loading "spacy-web-client"
.........
("spacy-web-client")
* (defvar x
Using Python Deep Learning Models In Common Lisp With a Web Services Interface
(spacy-web-client:spacy-client
"President Bill Clinton went to Congress. He gave a speech on taxes and Mexico."))
* (spacy-web-client:spacy-data-entities x)
"Bill Clinton/PERSON"
* (spacy-web-client:spacy-data-tokens x)
("President" "Bill" "Clinton" "went" "to" "Congress" "." "He" "gave" "a"
"speech" "on" "taxes" "and" "Mexico" ".")
The client library is implemented in the file src/spacy_web_client/spacy-web-client.lisp:
(in-package spacy-web-client)
(defvar base-url "http://127.0.0.1:8008?text=")
(defstruct spacy-data entities tokens)
(defun spacy-client (query)
(let* ((the-bytes
(drakma:http-request
(concatenate 'string
base-url
(do-urlencode:urlencode
query))
:content-type "application/text"))
(fetched-data
(flexi-streams:octets-to-string the-bytes :external-format :utf-8))
(lists (with-input-from-string (s fetched-data)
(json:decode-json s))))
(print lists)
(make-spacy-data :entities (cadar lists) :tokens (cdadr lists))))
On line 3 we define base URL for accessing the spaCy web service, assuming that it is running on
your laptop and not a remote server. On line 5 we define a defstruct named spacy-data that has
two fields: a list of entities in the input text and a list of word tokens in the input text.
The function spacy-client builds a query string on lines 10-12 that consists of the base-url and the
input query text URL encoded. The drakma library, that we used before, is used to make a HTTP
request from the Python spaCy server. Lines 14-15 uses the flexi-streams package to convert raw byte
data to UTF8 characters. Lines 16-17 use the json package to parse the UTF8 encoded string, getting
two lists of strings. I left the debug printout expression in line 18 so that you can see the results
of parsing the JSON data. The function make-spacy-data was generated for us by the defstruct
statement on line 5.
Using Python Deep Learning Models In Common Lisp With a Web Services Interface
Common Lisp Client for the Coreference NLP Web
Services
Let’s look at some typical output from this example, then we will look at the code:
$ sbcl
This is SBCL 1.3.16, an implementation of ANSI Common Lisp.
More information about SBCL is available at <http://www.sbcl.org/>.
SBCL is free software, provided as is, with absolutely no warranty.
It is mostly in the public domain; some portions are provided under
BSD-style licenses.
See the CREDITS and COPYING files in the
distribution for more information.
#P"/Users/markw/quicklisp/setup.lisp"
"starting up quicklisp"
* (ql:quickload "coref")
To load "coref":
Load 1 ASDF system:
; Loading "coref"
..................................................
[package coref]
("coref")
* (coref:coref-client "My sister has a dog Henry. She loves him.")
"My sister has a dog Henry. My sister loves a dog Henry."
* (coref:coref-client "My sister has a dog Henry. He often runs to her.")
"My sister has a dog Henry. a dog Henry often runs to My sister."
Notice that pronouns in the input text are correctly replaced by the noun phrases that the pronoun
refer to.
The implementation for the core client is in the file src/coref_web_client/coref.lisp:
Using Python Deep Learning Models In Common Lisp With a Web Services Interface
(in-package #:coref)
;; (ql:quickload :do-urlencode)
(defvar base-url "http://127.0.0.1:8000?text=")
(defun coref-client (query)
(let ((the-bytes
(drakma:http-request
(concatenate 'string
base-url
(do-urlencode:urlencode
query)
"&no_detail=1")
:content-type "application/text")))
(flexi-streams:octets-to-string the-bytes :external-format :utf-8)))
This code is similar to the example in the last section for setting up a call to http-request but is
simpler: here the Python coreference web service accepts a string as input and returns a string as
output with pronouns replaced by the nouns or noun phrases that they refer to. The example in the
last section had to parse returned JSON data, this example does not.
Trouble Shooting Possible Problems - Skip if this
Example Works on Your System
If you run Common Lisp in an IDE (for example in LispWorks’ IDE or VSCode with a Common Lisp
plugin) make sure you start the IDE from the command line so your PATH environment variable
will be set as it is in our bash or zsh shell.
Make sure you are starting your Common Lisp program or running a Common Lisp repl with the
same Python installation (if you have Quicklisp installed, then you also have the package uiop
installed):
$ which python
/Users/markw/bin/anaconda3/bin/python
$ sbcl
This is SBCL 2.0.2, an implementation of ANSI Common Lisp.
* (uiop:run-program "which python" :output :string)
"/Users/markw/bin/anaconda3/bin/python"
Using Python Deep Learning Models In Common Lisp With a Web Services Interface
Python Interop Wrap-up
Much of my professional work in the last five years involved deep learning models and currently
most available software is written in Python. While there are available libraries for calling Python
code from Common Lisp, these libraries tend to not work well for Python code using libraries like
TensorFlow, spaCy, PyTorch, etc., especially if the Python code is configured to use GPUs via CUDA
of special hardware like TPUs. I find it simpler to simply wrap functionality implemented in Python
as a simple web service.
Using the PY4CL Library to Embed
Python in Common Lisp
We will tackle the same problem as the previous chapter but take a different approach. Now we will
use Ben Dudson’s project Py4CL⁷¹ that automatically starts a Python process and communicates
with the Python process via a stream interface. The approach we took before is appropriate for large
scale systems where you might want scale horizontally by having Python processes running on
different servers than the servers used for the Common Lisp parts of your application. The approach
we now take is much more convenient for what I call “laptop development” where the management
of a Python process and communication is handled for you by the Py4CL library. If you need to
build multi-server distributed systems for scaling reasons then use the examples in the last chapter.
While Py4CL provides a lot of flexibility for passing primitive types between Common Lisp and
Python (in both directions), I find it easiest to write small Python wrappers that only use lists,
arrays, numbers, and strings as arguments and return types. You might want to experiment with
the examples on the Py4CL GitHub page that let you directly call Python libraries without writing
wrappers. When I write code for my own projects I try to make code as simple as possible so when
I need to later revisit my own code it is immediately obvious what it is doing. Since I have been
using Common Lisp for almost 40 years, I often find myself reusing bits of my own old code and I
optimize for making this as easy as possible. In other words I favor readability over “clever” code.
Project Structure, Building the Python Wrapper, and
Running an Example
The packaging of the Lisp code for my spacy-py4cl package is simple. Here is the listing of
package.lisp for this project:
;;;; package.lisp
(defpackage #:spacy-py4cl
(:use #:cl #:py4cl)
(:export #:nlp))
Listing of spacy-py4cl.asd:
⁷¹https://github.com/bendudson/py4cl/
Using the PY4CL Library to Embed Python in Common Lisp
;;;; spacy-py4cl.asd
(asdf:defsystem #:spacy-py4cl
:description "Use py4cl to use Python spaCy library embedded in Common Lisp"
:author "Mark Watson <markw@markwatson.com>"
:license "Apache 2"
:depends-on (#:py4cl)
:serial t
:components ((:file "package")
(:file "spacy-py4cl")))
You need to run a Python setup procedure to install the Python wrapper for space-py4cl on your
system. Some output is removed for conciseness:
$ cd loving-common-lisp/src/spacy-py4cl
$ cd PYTHON_SPACY_SETUP_install/spacystub
$ pip install -U spacy
$ python -m spacy download en
$ python setup.py install
running install
running build
running build_py
running install_lib
running install_egg_info
Writing /Users/markw/bin/anaconda3/lib/python3.7/site-packages/spacystub-0.21-py3.7.\
egg-info
You only need to do this once unless you update to a later version of Python on your system.
If you are not familiar with Python, it is worth looking at the wrapper implementation, otherwise
skip the next few paragraphs.
$ ls -R PYTHON_SPACY_SETUP_install
spacystub
PYTHON_SPACY_SETUP_install/spacystub:
README.md
setup.py
spacystub
PYTHON_SPACY_SETUP_install/spacystub/build/lib:
spacystub
PYTHON_SPACY_SETUP_install/spacystub/spacystub:
parse.py
Using the PY4CL Library to Embed Python in Common Lisp
Here is the implementation of setup.py that specifies how to build and install the wrapper globally
for use on your system:
from distutils.core import setup
setup(name='spacystub',
version='0.21',
packages=['spacystub'],
license='Apache 2',
py_modules=['pystub'],
long_description=open('README.md').read())
The definition of the library in file PYTHON_SPACY_SETUP_install/spacystub/spacystub/parse.py:
import spacy
nlp = spacy.load("en")
def parse(text):
doc = nlp(text)
response = {}
response['entities'] = [(ent.text, ent.start_char, ent.end_char, ent.label_) for e\
nt in doc.ents]
response['tokens'] = [token.text for token in doc]
return [response['tokens'], response['entities']]
Here is a Common Lisp repl session showing you how to use the library implemented in the next
section:
Clozure Common Lisp Version 1.12
DarwinX8664
For more information about CCL, please see http://ccl.clozure.com.
CCL is free software.
It is distributed under the terms of the Apache Licence, Vers\
ion 2.0.
? (ql:quickload "spacy-py4cl")
To load "spacy-py4cl":
Load 1 ASDF system:
spacy-py4cl
; Loading "spacy-py4cl"
[package spacy-py4cl]
Using the PY4CL Library to Embed Python in Common Lisp
("spacy-py4cl")
? (spacy-py4cl:nlp "The President of Mexico went to Canada")
#(#("The" "President" "of" "Mexico" "went" "to" "Canada") #(("Mexico" 17 23 "GPE") (\
"Canada" 32 38 "GPE")))
? (spacy-py4cl:nlp "Bill Clinton bought a red car. He drove it to the beach.")
#(#("Bill" "Clinton" "bought" "a" "red" "car" "." "He" "drove" "it" "to" "the" "beac\
h" ".") #(("Bill Clinton" 0 12 "PERSON")))
Entities in text are identified with the starting and ending character indices that refer to the input
string. For example, the entity “Mexico” starts at character position 17 and character index 23 is the
character after the entity name in the input string. The entity type “GPE” refers to a country name
and “PERSON” refers to a person’s name in the input text.
Implementation of spacy-py4cl
The Common Lisp implementation for this package is simple. In line 5 the call to py4cl:python-exec
starts a process to run Python and imports the function parse from my Python wrapper. The call
to py4cl:import-function in line 6 finds a function named “parse” in the attached Python process
and generates a Common Lisp function with the same name that handles calling into Python and
converting handling the returned values to Common Lisp values:
;;;; spacy-py4cl.lisp
(in-package #:spacy-py4cl)
(py4cl:python-exec "from spacystub.parse import parse")
(py4cl:import-function "parse")
(defun nlp (text)
(parse text))
While it is possible to call Python libraries directly using Py4CL, when I need to frequently use
Python libraries like spaCY, TensorFlow, fast.ai, etc. in Common Lisp, I like to use wrappers that use
simple as possible data types and APIs to communicate between a Common Lisp process and the
spawned Python process.
Trouble Shooting Possible Problems - Skip if this
Example Works on Your System
When you install my wrapper library in Python on the command line whatever your shell if (bash,
zsh, etc.) you should then try to import the library in a Python repl:
Using the PY4CL Library to Embed Python in Common Lisp
$ python
Python 3.7.4 (default, Aug 13 2019, 15:17:50)
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> from spacystub.parse import parse
>>> parse("John Smith is a Democrat")
[['John', 'Smith', 'is', 'a', 'Democrat'], [('John Smith', 0, 10, 'PERSON'), ('Democ\
rat', 16, 24, 'NORP')]]
If this works and the Common Lisp library spacy-py4cl does not, then make sure you are starting
your Common Lisp program or running a Common Lisp repl with the same Python installation (if
you have Quicklisp installed, then you also have the package uiop installed):
$ which python
/Users/markw/bin/anaconda3/bin/python
$ sbcl
This is SBCL 2.0.2, an implementation of ANSI Common Lisp.
* (uiop:run-program "which python" :output :string)
"/Users/markw/bin/anaconda3/bin/python"
If you run Common Lisp in an IDE (for example in LispWorks’ IDE or VSCode with a Common Lisp
plugin) make sure you start the IDE from the command line so your PATH environment variable
will be set as it is in our bash or zsh shell.
Wrap-up for Using Py4CL
While I prefer Common Lisp for general development and also AI research, there are useful Python
libraries that I want to integrate into my projects. I hope that the last chapter and this chapter provide
you with two solid approaches for you to use in your own work to take advantage of Python libraries.
Semantic Web and Linked Data
I have written two previous books on the semantic web and linked data and most of my programming
books have semantic web examples. Please note that the background material here on the semantic
web standards RDF, RDFS, and SPARQL is shared with my book Practical Artificial Intelligence
Programming With Java⁷² so if you have read that book then the first several pages of this chapter
will seem familiar.
Construction of Knowledge Graphs, as we will do in later chapters, is a core technology at many
corporations and organizations to prevent data silos where different database systems are poorly
connected and not as useful in combination than they could be. The use of RDF data stores is a
powerful technique for data interoperability within organizations. Semantic Web standards like
RDF, RDFS, and SPARQL support both building Knowledge Graphs and also key technologies for
automating the collection and use of web data.
I worked as a contractor at Google on an internal Knowledge Graph project and I currently work at
Olive AI⁷³ on their Knowledge Graph team.
The semantic web is intended to provide a massive linked set of data for use by software systems
just as the World Wide Web provides a massive collection of linked web pages for human reading
and browsing. The semantic web is like the web in that anyone can generate any content that they
want. This freedom to publish anything works for the web because we use our ability to understand
natural language to interpret what we read – and often to dismiss material that based upon our own
knowledge we consider to be incorrect.
Semantic web and linked data technologies are also useful for smaller amounts of data, an example
being a Knowledge Graph containing information for a business. We will further explore Knowledge
Graphs in the next two chapters.
The core concept for the semantic web is data integration and use from different sources. As we will
soon see, the tools for implementing the semantic web are designed for encoding data and sharing
data from many different sources.
I cover the semantic web in this book because I believe that semantic web technologies are
complementary to AI systems for gathering and processing data on the web. As more web pages
are generated by applications (as opposed to simply showing static HTML files) it becomes easier to
produce both HTML for human readers and semantic data for software agents.
There are several very good semantic web toolkits for the Java language and platform. Here we use
Apache Jena because it is what I often use in my own work and I believe that it is a good starting
technology for your first experiments with semantic web technologies. This chapter provides an
⁷²https://leanpub.com/javaai
⁷³https://oliveai.com
Semantic Web and Linked Data
incomplete coverage of semantic web technologies and is intended as a gentle introduction to a
few useful techniques and how to implement those techniques in Java. This chapter is the start of
a journey in the technology that I think is as important as technologies like deep learning that get
more public mindshare.
The following figure shows a layered hierarchy of data models that are used to implement semantic
web applications. To design and implement these applications we need to think in terms of physical
models (storage and access of RDF, RDFS, and perhaps OWL data), logical models (how we use
RDF and RDFS to define relationships between data represented as unique URIs and string literals
and how we logically combine data from different sources) and conceptual modeling (higher level
knowledge representation and reasoning using OWL). Originally RDF data was serialized as XML
data but other formats have become much more popular because they are easier to read and manually
create. The top three layers in the figure might be represented as XML, or as LD-JSON (linked data
JSON) or formats like N-Triples and N3 that we will use later.
Semantic Web Data Models
Resource Description Framework (RDF) Data Model
The Resource Description Framework (RDF) is used to encode information and the RDF Schema
(RDFS) facilitates using data with different RDF encodings without the need to convert one set
of schemas to another. Later, using OWL we can simply declare that one predicate is the same as
another, that is, one predicate is a sub-predicate of another (e.g., a property containsCity can be
declared to be a sub-property of containsPlace so if something contains a city then it also contains
a place), etc. The predicate part of an RDF statement often refers to a property.
Semantic Web and Linked Data
RDF data was originally encoded as XML and intended for automated processing. In this chapter we
will use two simple to read formats called “N-Triples” and “N3.” Apache Jena can be used to convert
between all RDF formats so we might as well use formats that are easier to read and understand.
RDF data consists of a set of triple values:
• subject
• predicate
• object
Some of my work with semantic web technologies deals with processing news stories, extracting
semantic information from the text, and storing it in RDF. I will use this application domain for the
examples in this chapter and the next chapter when we implement code to automatically generate
RDF for Knowledge Graphs. I deal with triples like:
• subject: a URL (or URI) of a news article.
• predicate: a relation like “containsPerson”.
• object: a literal value like “Bill Clinton” or a URI representing Bill Clinton.
In the next chapter we will use the entity recognition library we developed in an earlier chapter to
create RDF from text input.
We will use either URIs or string literals as values for objects. We will always use URIs for
representing subjects and predicates. In any case URIs are usually preferred to string literals. We
will see an example of this preferred use but first we need to learn the N-Triple and N3 RDF formats.
I proposed the idea that RDF was more flexible than Object Modeling in programming languages,
relational databases, and XML with schemas. If we can tag new attributes on the fly to existing
data, how do we prevent what I might call “data chaos” as we modify existing data sources? It turns
out that the solution to this problem is also the solution for encoding real semantics (or meaning)
with data: we usually use unique URIs for RDF subjects, predicates, and objects, and usually with
a preference for not using string literals. The definitions of predicates are tied to a namespace and
later with OWL we will state the equivalence of predicates in different namespaces with the same
semantic meaning. I will try to make this idea more clear with some examples and Wikipedia has a
good writeup on RDF⁷⁴.
Any part of a triple (subject, predicate, or object) is either a URI or a string literal. URIs encode
namespaces. For example, the containsPerson predicate in the last example could be written as:
http://knowledgebooks.com/ontology/#containsPerson
The first part of this URI is considered to be the namespace for this predicate “containsPerson.”
When different RDF triples use this same predicate, this is some assurance to us that all users of this
⁷⁴https://en.wikipedia.org/wiki/Resource_Description_Framework
Semantic Web and Linked Data
predicate understand to the same meaning. Furthermore, we will see later that we can use RDFS to
state equivalency between this predicate (in the namespace http://knowledgebooks.com/ontology/)
with predicates represented by different URIs used in other data sources. In an “artificial intelligence”
sense, software that we write does not understand predicates like “containsCity”, “containsPerson”,
or “isLocation” in the way that a human reader can by combining understood common meanings
for the words “contains”, “city”, “is”, “person”, and “location” but for many interesting and useful
types of applications that is fine as long as the predicate is used consistently. We will see shortly
that we can define abbreviation prefixes for namespaces which makes RDF and RDFS files shorter
and easier to read.
The Jena library supports most serialization formats for RDF:
• Turtle
• N-Triples
• NQuads
• TriG
• JSON-LD
• RDF/XML
• RDF/JSON
• TriX
• RDF Binary
A statement in N-Triple format consists of three URIs (two URIs and a string literals for the object)
followed by a period to end the statement. While statements are often written one per line in a source
file they can be broken across lines; it is the ending period which marks the end of a statement. The
standard file extension for N-Triple format files is *.nt and the standard format for N3 format files
is *.n3.
My preference is to use N-Triple format files as output from programs that I write to save data as
RDF. N-Triple files don’t use any abbreviations and each RDF statement is self-contained. I often
use tools like the command line commands in Jena or RDF4J to convert N-Triple files to N3 or other
formats if I will be reading them or even hand editing them. Here is an example using the N3 syntax:
@prefix kb:
<http://knowledgebooks.com/ontology#>
<http://news.com/201234/> kb:containsCountry "China" .
The N3 format adds prefixes (abbreviations) to the N-Triple format. In practice it would be better to
use the URI http://dbpedia.org/resource/China instead of the literal value “China.”
Here we see the use of an abbreviation prefix “kb:” for the namespace for my company Knowledge-
Books.com ontologies. The first term in the RDF statement (the subject) is the URI of a news article.
The second term (the predicate) is “containsCountry” in the “kb:” namespace. The last item in the
Semantic Web and Linked Data
statement (the object) is a string literal “China.” I would describe this RDF statement in English as,
“The news article at URI http://news.com/201234 mentions the country China.”
This was a very simple N3 example which we will expand to show additional features of the N3
notation. As another example, let’s look at the case if this news article also mentions the USA. Instead
of adding a whole new statement like this we can combine them using N3 notation. Here we have
two separate RDF statements:
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/China>
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/United_States>
We can collapse multiple RDF statements that share the same subject and optionally the same
predicate:
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/China> ,
<http://dbpedia.org/resource/United_States>
The indentation and placement on separate lines is arbitrary - use whatever style you like that is
readable. We can also add in additional predicates that use the same subject (I am going to use string
literals here instead of URIs for objects to make the following example more concise but in practice
prefer using URIs):
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/>
kb:containsCountry "China" ,
"USA" .
kb:containsOrganization "United Nations" ;
kb:containsPerson "Ban Ki-moon" , "Gordon Brown" ,
"Hu Jintao" , "George W. Bush" ,
"Pervez Musharraf" ,
"Vladimir Putin" ,
"Mahmoud Ahmadinejad" .
Semantic Web and Linked Data
This single N3 statement represents ten individual RDF triples. Each section defining triples with
the same subject and predicate have objects separated by commas and ending with a period. Please
note that whatever RDF storage system you use (we will be using Jena) it makes no difference if we
load RDF as XML, N-Triple, of N3 format files: internally subject, predicate, and object triples are
stored in the same way and are used in the same way. RDF triples in a data store represent directed
graphs that may not all be connected.
I promised you that the data in RDF data stores was easy to extend. As an example, let us assume
that we have written software that is able to read online news articles and create RDF data that
captures some of the semantics in the articles. If we extend our program to also recognize dates
when the articles are published, we can simply reprocess articles and for each article add a triple to
our RDF data store using a form like:
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/> kb:datePublished "2008-05-11" .
Here we just represent the date as a string. We can add a type to the object representing a specific
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix kb:
<http://knowledgebooks.com/ontology#> .
<http://news.com/201234/> kb:datePublished "2008-05-11"^^xsd:date .
Furthermore, if we do not have dates for all news articles that is often acceptable because when
constructing SPARQL queries you can match optional patterns. If for example you are looking up
articles on a specific subject then some results may have a publication date attached to the results
for that article and some might not. In practice RDF supports types and we would use a date type as
seen in the last example, not a string. However, in designing the example programs for this chapter
I decided to simplify our representation of URIs and often use string literals as simple Java strings.
For many applications this isn’t a real limitation.
Extending RDF with RDF Schema
RDF Schema (RDFS) supports the definition of classes and properties based on set inclusion. In
RDFS classes and properties are orthogonal. Let’s start with looking at an example using additional
namespaces:
Semantic Web and Linked Data
@prefix kb:
<http://knowledgebooks.com/ontology#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
@prefix rdfs:
<http://www.w3.org/2000/01/rdf-schema#>
@prefix dbo: <http://dbpedia.org/ontology/>
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/China>
<http://news.com/201234/>
kb:containsCountry
<http://dbpedia.org/resource/United_States>
<http://dbpedia.org/resource/China>
rdfs:label "China"@en,
rdf:type dbo:Place ,
rdf:type dbo:Country .
Because the semantic web is intended to be processed automatically by software systems it is
encoded as RDF. There is a problem that must be solved in implementing and using the semantic
web: everyone who publishes semantic web data is free to create their own RDF schemas for storing
data; for example, there is usually no single standard RDF schema definition for topics like news
stories and stock market data. The SKOS⁷⁵ is a namespace containing standard schemas and the
most widely used standard is schema.org⁷⁶. Understanding the ways of integrating different data
sources using different schemas helps to understand the design decisions behind the semantic web
applications. In this chapter I often use my own schemas in the knowledgebooks.com namespace for
the simple examples you see here. When you build your own production systems part of the work is
searching through schema.org and SKOS to use standard name spaces and schemas when possible.
The use of standard schemas helps when you link internal proprietary Knowledge Graphs used in
organization with public open data from sources like WikiData⁷⁷ and DBPedia⁷⁸.
We will start with an example that is an extension of the example in the last section that also uses
RDFS. We add a few additional RDF statements:
⁷⁵https://www.w3.org/2009/08/skos-reference/skos.html
⁷⁶https://schema.org/docs/schemas.html
⁷⁷https://www.wikidata.org/wiki/Wikidata:Main_Page
⁷⁸https://wiki.dbpedia.org/about
Semantic Web and Linked Data
@prefix kb:
<http://knowledgebooks.com/ontology#> .
@prefix rdfs:
<http://www.w3.org/2000/01/rdf-schema#> .
kb:containsCity rdfs:subPropertyOf kb:containsPlace .
kb:containsCountry rdfs:subPropertyOf kb:containsPlace .
kb:containsState rdfs:subPropertyOf kb:containsPlace .
The last three lines declare that:
• The property containsCity is a sub-property of containsPlace.
• The property containsCountry is a sub-property of containsPlace.
• The property containsState is a sub-property of containsPlace.
Why is this useful? For at least two reasons:
• You can query an RDF data store for all triples that use property containsPlace and also match
triples with properties equal to containsCity, containsCountry, or containsState. There may not
even be any triples that explicitly use the property containsPlace.
• Consider a hypothetical case where you are using two different RDF data stores that use
different properties for naming cities: cityName and city. You can define cityName to be a
sub-property of city and then write all queries against the single property name city. This
removes the necessity to convert data from different sources to use the same Schema. You can
also use OWL to state property and class equivalency.
In addition to providing a vocabulary for describing properties and class membership by properties,
RDFS is also used for logical inference to infer new triples, combine data from different RDF data
sources, and to allow effective querying of RDF data stores. We will see examples of all of these
features of RDFS when we later start using the Jena libraries to perform SPARQL queries.
The SPARQL Query Language
SPARQL is a query language used to query RDF data stores. While SPARQL may initially look
like SQL, we will see that there are some important differences like support for RDFS and OWL
inferencing and graph-based instead of relational matching operations. We will cover the basics of
SPARQL in this section and then see more examples later when we learn how to embed Jena in Java
applications, and see more examples in the last chapter Knowledge Graph Navigator.
We will use the N3 format RDF file test_data/news.n3 for the examples. I created this file
automatically by spidering Reuters news stories on the news.yahoo.com web site and automatically
extracting named entities from the text of the articles. We saw techniques for extracting named
entities from text in earlier chapters. In this chapter we use these sample RDF files.
You have already seen snippets of this file and I list the entire file here for reference, edited to fit
line width: you may find the file news.n3 easier to read if you are at your computer and open the
file in a text editor so you will not be limited to what fits on a book page:
Semantic Web and Linked Data
@prefix kb:
<http://knowledgebooks.com/ontology#> .
@prefix rdfs:
<http://www.w3.org/2000/01/rdf-schema#> .
kb:containsCity rdfs:subPropertyOf kb:containsPlace .
kb:containsCountry rdfs:subPropertyOf kb:containsPlace .
kb:containsState rdfs:subPropertyOf kb:containsPlace .
<http://yahoo.com/20080616/usa_flooding_dc_16/>
kb:containsCity "Burlington" , "Denver" ,
"St. Paul" ," Chicago" ,
"Quincy" , "CHICAGO" ,
"Iowa City" ;
kb:containsRegion "U.S. Midwest" , "Midwest" ;
kb:containsCountry "United States" , "Japan" ;
kb:containsState "Minnesota" , "Illinois" ,
"Mississippi" , "Iowa" ;
kb:containsOrganization "National Guard" ,
"U.S. Department of Agriculture" ,
"White House" ,
"Chicago Board of Trade" ,
"Department of Transportation" ;
kb:containsPerson "Dena Gray-Fisher" ,
"Donald Miller" ,
"Glenn Hollander" ,
"Rich Feltes" ,
"George W. Bush" ;
kb:containsIndustryTerm "food inflation" , "food" ,
"finance ministers" ,
"oil" .
<http://yahoo.com/78325/ts_nm/usa_politics_dc_2/>
kb:containsCity "Washington" , "Baghdad" ,
"Arlington" , "Flint" ;
kb:containsCountry "United States" ,
"Afghanistan" ,
"Iraq" ;
kb:containsState "Illinois" , "Virginia" ,
"Arizona" , "Michigan" ;
kb:containsOrganization "White House" ,
"Obama administration" ,
"Iraqi government" ;
Semantic Web and Linked Data
kb:containsPerson "David Petraeus" ,
"John McCain" ,
"Hoshiyar Zebari" ,
"Barack Obama" ,
"George W. Bush" ,
"Carly Fiorina" ;
kb:containsIndustryTerm "oil prices" .
<http://yahoo.com/10944/ts_nm/worldleaders_dc_1/>
kb:containsCity "WASHINGTON" ;
kb:containsCountry "United States" , "Pakistan" ,
"Islamic Republic of Iran" ;
kb:containsState "Maryland" ;
kb:containsOrganization "University of Maryland" ,
"United Nations" ;
kb:containsPerson "Ban Ki-moon" , "Gordon Brown" ,
"Hu Jintao" , "George W. Bush" ,
"Pervez Musharraf" ,
"Vladimir Putin" ,
"Steven Kull" ,
"Mahmoud Ahmadinejad" .
<http://yahoo.com/10622/global_economy_dc_4/>
kb:containsCity "Sao Paulo" , "Kuala Lumpur" ;
kb:containsRegion "Midwest" ;
kb:containsCountry "United States" , "Britain" ,
"Saudi Arabia" , "Spain" ,
"Italy" , India" ,
""France" , "Canada" ,
"Russia" , "Germany" , "China" ,
"Japan" , "South Korea" ;
kb:containsOrganization "Federal Reserve Bank" ,
"European Union" ,
"European Central Bank" ,
"European Commission" ;
kb:containsPerson "Lee Myung-bak" , "Rajat Nag" ,
"Luiz Inacio Lula da Silva" ,
"Jeffrey Lacker" ;
kb:containsCompany "Development Bank Managing" ,
"Reuters" ,
"Richmond Federal Reserve Bank" ;
kb:containsIndustryTerm "central bank" , "food" ,
"energy costs" ,
Semantic Web and Linked Data
"finance ministers" ,
"crude oil prices" ,
"oil prices" ,
"oil shock" ,
"food prices" ,
"Finance ministers" ,
"Oil prices" , "oil" .
In the following examples, we will use the main method in the class JenaApi (developed in the
next section) that allows us to load multiple RDF input files and then to interactively enter SPARQL
queries.
We will start with a simple SPARQL query for subjects (news article URLs) and objects (matching
countries) with the value for the predicate equal to containsCountry. Variables in queries start with
a question mark character and can have any names:
SELECT ?subject ?object
WHERE {
?subject
<http://knowledgebooks.com/ontology#containsCountry>
?object .
It is important for you to understand what is happening when we apply the last SPARQL query to
our sample data. Conceptually, all the triples in the sample data are scanned, keeping the ones where
the predicate part of a triple is equal to http://knowledgebooks.com/ontology#containsCountry.
In practice RDF data stores supporting SPARQL queries index RDF data so a complete scan of the
sample data is not required. This is analogous to relational databases where indices are created to
avoid needing to perform complete scans of database tables.
In practice, when you are exploring a Knowledge Graph like DBPedia or WikiData (that are just
very large collections of RDF triples), you might run a query and discover a useful or interesting
entity URI in the triple store, then drill down to find out more about the entity. In a later chapter
Knowledge Graph Navigator we attempt to automate this exploration process using the DBPedia
data as a Knowledge Graph.
We will be using the same code to access the small example of RDF statements in our sample data
as we will for accessing DBPedia or WikiData.
We can make this last query easier to read and reduce the chance of misspelling errors by using a
namespace prefix:
Semantic Web and Linked Data
PREFIX kb:
<http://knowledgebooks.com/ontology#>
SELECT ?subject ?object
WHERE {
?subject kb:containsCountry ?object .
Later in the chapter Knowledge Graph Navigator we will write an application that automatically
generates SPARQL queries for the DBPedia public knowledge Graph. These queries will be be more
complex than the simpler examples here. Reading this chapter before Knowledge Graph Navigator
is recommended.
Case Study: Using SPARQL to Find Information about
Board of Directors Members of Corporations and
Organizations
Before we write software to automate the process of using SPARQL queries to find information
on DBPedia, let’s perform a few manual queries for finding information on board of directors of
corportations. To start with, we would like to find an RDF property that indicates board membership.
There is a common expression for finding information on the web using search engines and also for
using SPARQL queries: “follow your nose,” that is, when you see something interesting, dig down
with more queries on whatever interests you.
SELECT DISTINCT ?s
WHERE {
?s ?p "Board of Directors"@en .
FILTER (?p IN (<http://www.w3.org/2000/01/rdf-schema#label>, <http://xmlns.com/f\
oaf/0.1/name>) && !regex(str(?s), "category", "i"))
We will find the property:
http://dbpedia.org/resource/Board_of_Directors
select ?s ?p { ?s ?p <http://dbpedia.org/resource/Board_of_Directors> } limit 6
Semantic Web and Linked Data
http://en.wikipedia.org/wiki/Board_of_Directors
http://xmlns.com/foaf/0.1/primaryTop\
http://dbpedia.org/resource/Lynn_D._Stewart_(businessman)
http://dbpedia.org/ontolog\
y/board
http://dbpedia.org/resource/Advance_America_Cash_Advance
http://dbpedia.org/ontology\
/keyPerson
http://dbpedia.org/resource/Railways_of_Slovak_Republic
http://dbpedia.org/ontology/\
keyPerson
http://dbpedia.org/resource/Divine_Word_University_of_Tacloban__DWU_Jubilee_Foundati\
on,_Inc.__1
http://dbpedia.org/ontology/keyPerson
http://dbpedia.org/resource/Mathys_Medical
The property http://dbpedia.org/ontology/board is what we are looking for. Let’s keep “following
our nose” to find examples of board members and the companies they server:
select ?person ?company { ?person <http://dbpedia.org/ontology/board> ?company} limi\
The results are:
person
company
http://dbpedia.org/resource/Matthew_Buckland
http://dbpedia.org/resource/Creative_Co\
http://dbpedia.org/resource/Jimmy_Wales
http://dbpedia.org/resource/Creative_Commons
http://dbpedia.org/resource/Nabeel_Rajab
http://dbpedia.org/resource/Human_Rights_Wa\
http://dbpedia.org/resource/Vincent_Tewson
http://dbpedia.org/resource/International\
_Confederation_of_Free_Trade_Unions
http://dbpedia.org/resource/William_T._Young
http://dbpedia.org/resource/KFC
http://dbpedia.org/resource/Colonel_Sanders
http://dbpedia.org/resource/KFC
Let’s see what information we can find on the founder of WikiPedi Jimmy Wales:
select ?p ?o { <http://dbpedia.org/resource/Jimmy_Wales> ?p ?o } limit 200
A few of the many results are:
Semantic Web and Linked Data
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://www.w3.org/2002/07/owl#Thing
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://xmlns.com/foaf/0.1/Person
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://dbpedia.org/ontology/Person
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://www.wikidata.org/entity/Q2156\
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://www.wikidata.org/entity/Q2422\
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://www.wikidata.org/entity/Q5
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://dbpedia.org/ontology/Agent
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://schema.org/Person
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://dbpedia.org/class/yago/Wikica\
tAmericanComputerScientists
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://dbpedia.org/class/yago/Wikica\
tAmericanExpatriatesInTheUnitedKingdom
http://www.w3.org/2000/01/rdf-schema#label
"Jimmy Wales"@en
Installing the Apache Jena Fuseki RDF Server
I have a github repository mark-watson/fuseki-semantic-web-dev-setup⁷⁹mthat you shoud clone:
git clone https://github.com/mark-watson/fuseki-semantic-web-dev-setup.git
cd fuseki-semantic-web-dev-setup
./fuseki-server --file RDF/sample_news.nt /news
This will run the SPARQL server Fuseki locally on your laptop and the default graph is “news” and
you will see output like:
⁷⁹https://github.com/mark-watson/fuseki-semantic-web-dev-setup
Semantic Web and Linked Data
$ ./fuseki-server --file RDF/sample_news.nt /news
[2020-11-07 09:31:13] Server
Dataset: in-memory: load file: RDF/sample_new\
[2020-11-07 09:31:14] Server
Running in read-only mode for /news
[2020-11-07 09:31:14] Server
Apache Jena Fuseki 3.16.0
[2020-11-07 09:31:14] Config
FUSEKI_HOME=/Users/markw/GITHUB/fuseki-semant\
ic-web-dev-setup/.
[2020-11-07 09:31:14] Config
FUSEKI_BASE=/Users/markw/GITHUB/fuseki-semant\
ic-web-dev-setup/run
[2020-11-07 09:31:14] Config
Shiro file: file:///Users/markw/GITHUB/fuseki\
-semantic-web-dev-setup/run/shiro.ini
[2020-11-07 09:31:15] Server
Dataset Type: in-memory, with files loaded
[2020-11-07 09:31:15] Server
Path = /news
[2020-11-07 09:31:15] Server
System
[2020-11-07 09:31:15] Server
Memory: 4.0 GiB
[2020-11-07 09:31:15] Server
14.0.1
[2020-11-07 09:31:15] Server
Mac OS X 10.15.7 x86_64
[2020-11-07 09:31:15] Server
[2020-11-07 09:31:15] Server
Started 2020/11/07 09:31:15 MST on port 3030
You can access a web interface for SPARQL queries by accessing localhost:3030 or http:127.0.0.1:3030.
Common Lisp Client Examples for the Apache Jena
Fuseki RDF Server
Later in the chapter “Knowledge Graph Navigator” we will develop a simple Common Lisp SPARQL
query library and use it for querying DBPedia. Here we will use it to query our local Fuseki server.
$ sbcl
This is SBCL 2.0.7, an implementation of ANSI Common Lisp.
More information about SBCL is available at <http://www.sbcl.org/>.
SBCL is free software, provided as is, with absolutely no warranty.
It is mostly in the public domain; some portions are provided under
BSD-style licenses.
See the CREDITS and COPYING files in the
distribution for more information.
#P"/Users/markw/quicklisp/setup.lisp"
"starting up quicklisp"
* (quicklisp:quickload "sparql")
To load "sparql":
Semantic Web and Linked Data
Load 1 ASDF system:
sparql
; Loading "sparql"
...............
("sparql")
* (sparql::fuseki "select ?s ?p ?o { ?s ?p ?o } limit 20")
(((:s "http://kbsportal.com/trout_season/")
(:p "http://knowledgebooks.com/ontology/#storyType")
(:o "http://knowledgebooks.com/ontology/#recreation"))
((:s "http://kbsportal.com/trout_season/")
(:p "http://knowledgebooks.com/ontology/#storyType")
(:o "http://knowledgebooks.com/ontology/#sports"))
((:s "http://kbsportal.com/bear_mountain_fire/")
(:p "http://knowledgebooks.com/ontology/#storyType")
(:o "http://knowledgebooks.com/ontology/#disaster"))
((:s "http://kbsportal.com/bear_mountain_fire/")
(:p "http://knowledgebooks.com/ontology/#summary")
(:o "The fire on Bear Mountain was caused by lightening"))
((:s "http://kbsportal.com/jc_basketball/")
(:p "http://knowledgebooks.com/ontology/#storyType")
(:o "http://knowledgebooks.com/ontology/#sports"))
((:s "http://kbsportal.com/oak_creek_flooding/")
(:p "http://knowledgebooks.com/ontology/#storyType")
(:o "http://knowledgebooks.com/ontology/#disaster"))
((:s "http://kbsportal.com/oak_creek_flooding/")
(:p "http://knowledgebooks.com/ontology/#summary")
(:o "Oak Creek flooded last week affecting 5 businesses")))
Here is an example of using the same library to query the public DBPedia SPARQL endpoint (most
output is not shown):
* (sparql:dbpedia "select ?s ?p { ?s ?p \"Bill Gates\"@en }")
("ndbpeia SPARQL:n" "select ?s ?p { ?s ?p \"Bill Gates\"@en }" "n")
(((:s "http://dbpedia.org/resource/Category:Bill_Gates")
(:p "http://www.w3.org/2000/01/rdf-schema#label"))
((:s "http://www.wikidata.org/entity/Q5284")
(:p "http://www.w3.org/2000/01/rdf-schema#label"))
((:s "http://dbpedia.org/resource/Bill_Gates")
(:p "http://xmlns.com/foaf/0.1/name"))
The SPARQL library in the github repository for this book also supports the commercial products
Semantic Web and Linked Data
AllegroGraph and Stardog RDF servers.
Automatically Generating Data for
Knowledge Graphs
We develop a complete application. The Knowledge Graph Creator (KGcreator) is a tool for
automating the generation of data for Knowledge Graphs from raw text data. We will see how to
create a single standalone executable file using SBCL Common Lisp. The application can also be run
during development from a repl. This application also implements a web application interface. In
addition to the KGcreator application we will close the chapter with a utiity library that processes
a file of RDF in N-Triple format and generates an extention file with triples pulled from DBedia
defining URIs found in the input data file.
Data created by KGcreator generates data in two formats:
• Neo4j graph database format (text format)
• RDF triples suitable for loading into any linked data/semantic web data store.
This example application works by identifying entities in text. Example entity types are people,
companies, country names, city names, broadcast network names, political party names, and
university names. We saw earlier code for detecting entities in the chapter on natural language
processing (NLP) and we will reuse this code. We will discuss later three strategies for reusing code
from different projects.
When I originally wrote KGCreator I intended to develop a commercial product. I wrote two research
prototypes, one in Common Lisp (the example in this chapter) and one in Haskell (which I also use
as an example in my book Haskell Tutorial and Cookbook⁸⁰. I decided to open source both versions
of KGCreator and if you work with Knowledge Graphs I hope you find KGCreator useful in your
The following figure shows part of a Neo4j Knowledge Graph created with the example code. This
graph has shortened labels in displayed nodes but Neo4j offers a web browser-based console that lets
you interactively explore Knowledge Graphs. We don’t cover setting up Neo4j here so please use the
Neo4j documentation⁸¹. As an introduction to RDF data, the semantic web, and linked data you can
get free copies of my two books Practical Semantic Web and Linked Data Applications, Common
Lisp Edition⁸² and Practical Semantic Web and Linked Data Applications, Java, Scala, Clojure, and
JRuby Edition⁸³.
⁸⁰https://leanpub.com/haskell-cookbook/
⁸¹https://neo4j.com/docs/operations-manual/current/introduction/
⁸²http://markwatson.com/opencontentdata/book_lisp.pdf
⁸³http://markwatson.com/opencontentdata/book_java.pdf
Automatically Generating Data for Knowledge Graphs
Part of a Knowledge Graph shown in Neo4j web application console
Here is a detail view:
Detail of Neo4j console
Implementation Notes
As seen in the file src /kgcreator/package.lisp this application uses several other packages:
Automatically Generating Data for Knowledge Graphs
(defpackage #:kgcreator
(:use #:cl
#:entities_dbpedia #:categorize_summarize #:myutils
#:cl-who #:hunchentoot #:parenscript)
(:export kgcreator))
The implementation of the packages shown on line 3 were in a previous chapter. The packagemyutils
are mostly miscellaneous string utilities that we won’t look at here; I leave it to you to read the source
As seen in the configuration file src/kgcreator/kgcreator.asd we split the implementation of the
application into four source files:
;;;; kgcreator.asd
(asdf:defsystem #:kgcreator
:description "Describe plotlib here"
:author "Mark Watson <mark.watson@gmail.com>"
:license "AGPL version 3"
:depends-on (#:entities_dbpedia #:categorize_summarize
#:myutils #:unix-opts #:cl-who
#:hunchentoot #:parenscript)
:components
((:file "package")
(:file "kgcreator")
(:file "neo4j")
(:file "rdf")
(:file "web"))
The application is separated into four source files:
• kgcreator.lisp: top level APIs and functionality. Uses the code in neo4j.lisp and rdf.lisp. Later
we will generate a standalone application that uses these top level APIs
• neo4j.lisp: generates Cyper text files that can be imported into Neo4j
– rdf.lisp: generates RDF text data that can be loaded or imported into RDF data stores
• web.lisp: a simple web application for running KGCreator
Generating RDF Data
I leave it to you find a tutorial on RDF data on the web, or you can get a PDF for my book “Practical
Semantic Web and Linked Data Applications, Common Lisp Edition”⁸⁴ and read the tutorial sections
on RDF.
⁸⁴http://markwatson.com/opencontentdata/book_lisp.pdf
Automatically Generating Data for Knowledge Graphs
RDF data is comprised of triples, where the value for each triple are a subject, a predicate, and an
object. Subjects are URIs, predicates are usually URIs, and objects are either literal values or URIs.
Here are two triples written by this example application:
<http://dbpedia.org/resource/The_Wall_Street_Journal>
<http://knowledgebooks.com/schema/aboutCompanyName>
"Wall Street Journal" .
<https://newsshop.com/june/z902.html>
<http://knowledgebooks.com/schema/containsCountryDbPediaLink>
<http://dbpedia.org/resource/Canada> .
The following listing of the file src/kgcreator/rdf.lisp generates RDF data:
(in-package #:kgcreator)
(let ((*rdf-nodes-hash*))
(defun rdf-from-files (output-file-path text-and-meta-pairs)
(setf *rdf-nodes-hash* (make-hash-table :test #'equal :size 200))
(print (list "==> rdf-from-files" output-file-path text-and-meta-pairs ))
(with-open-file
(str output-file-path
:direction :output
:if-exists :supersede
:if-does-not-exist :create)
(defun rdf-from-files-handle-single-file (text-input-file meta-input-file)
(let* ((text (file-to-string text-input-file))
(words (myutils:words-from-string text))
(meta (file-to-string meta-input-file)))
(defun generate-original-doc-node-rdf ()
(let ((node-name (node-name-from-uri meta)))
(if (null (gethash node-name *rdf-nodes-hash*))
(let* ((cats (categorize words))
(sum (summarize words cats)))
(print (list "$$$$$$
cats:" cats))
(setf (gethash node-name *rdf-nodes-hash*) t)
(format str (concatenate 'string "<" meta
"> <http:knowledgebooks.com/schema/summary> \""
sum "\" . ~%"))
(dolist (cat cats)
(let ((hash-check (concatenate 'string node-name (car cat))))
Automatically Generating Data for Knowledge Graphs
(if (null (gethash hash-check *rdf-nodes-hash*))
(let ()
(setf (gethash hash-check *rdf-nodes-hash*) t)
(format str
(concatenate 'string "<" meta
"> <http://knowledgebooks.com/schema/"
"topicCategory> "
"<http://knowledgebooks.com/schema/"
(car cat) "> . ~%"))))))))))
(defun generate-dbpedia-contains-rdf (key value)
(generate-original-doc-node-rdf)
(let ((relation-name (concatenate 'string key "DbPediaLink")))
(dolist (entity-pair value)
(let* ((node-name (node-name-from-uri meta))
(object-node-name (node-name-from-uri (cadr entity-pair)))
(hash-check (concatenate 'string node-name object-node-name)))
(if (null (gethash hash-check *rdf-nodes-hash*))
(let ()
(setf (gethash hash-check *rdf-nodes-hash*) t)
(format str (concatenate 'string "<" meta
"> <http://knowledgebooks.com/schema/contains/"
key "> " (cadr entity-pair) " .~%"))))))))))
;; start code for rdf-from-files (output-file-path text-and-meta-pairs)
(dolist (pair text-and-meta-pairs)
(rdf-from-files-handle-single-file (car pair) (cadr pair))
(let ((h (entities_dbpedia:find-entities-in-text (file-to-string (car pair))\
(entities_dbpedia:entity-iterator #'generate-dbpedia-contains-rdf h))))))
(defvar test_files '((#P"~/GITHUB/common-lisp/kgcreator/test_data/test3.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test3.meta")))
(defvar test_filesZZZ '((#P"~/GITHUB/common-lisp/kgcreator/test_data/test3.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test3.meta")
(#P"~/GITHUB/common-lisp/kgcreator/test_data/test2.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test2.meta")
(#P"~/GITHUB/common-lisp/kgcreator/test_data/test1.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test1.meta")))
(defun test3a ()
Automatically Generating Data for Knowledge Graphs
(rdf-from-files "out.rdf" test_files))
You can load all of KGCreator but just execute the test function at the end of this file using:
(ql:quickload "kgcreator")
(in-package #:kgcreator)
(kgcreator:test3a)
This code works on a list of paired files for text data and the meta data for each text file. As an
example, if there is an input text file test123.txt then there would be a matching meta file test123.meta
that contains the source of the data in the file test123.txt. This data source will be a URI on the web
or a local file URI. The top level function rdf-from-files takes an output file path for writing the
generated RDF data and a list of pairs of text and meta file paths.
A global variable *rdf-nodes-hash* will be used to remember the nodes in the RDF graph as it is
generated. Please note that the function rdf-from-files is not re-entrant: it uses the global *rdf-
nodes-hash* so if you are writing multi-threaded applications it will not work to execute the
function rdf-from-files simultaneously in multiple threads of execution.
The function rdf-from-files (and the nested functions) are straightforward. I left a few debug
printout statements in the code and when you run the test code that I left in the bottom of the
file, hopefully it will be clear what rdf.lisp is doing.
Generating Data for the Neo4j Graph Database
Now we will generate Neo4J Cypher data. In order to keep the implementation simple, both the RDF
and Cypher generation code starts with raw text and performs the NLP analysis to find entities. This
example could be refactored to perform the NLP analysis just one time but in practice you will likely
be working with either RDF or NEO4J and so you will probably extract just the code you need from
this example (i.e., either the RDF or Cypher generation code).
Before we look at the code, let’s start with a few lines of generated Neo4J Cypher import data:
CREATE (newsshop_com_june_z902_html_news)-[:ContainsCompanyDbPediaLink]->(Wall_Stree\
t_Journal)
CREATE (Canada:Entity {name:"Canada", uri:"<http://dbpedia.org/resource/Canada>"})
CREATE (newsshop_com_june_z902_html_news)-[:ContainsCountryDbPediaLink]->(Canada)
CREATE (summary_of_abcnews_go_com_US_violent_long_lasting_tornadoes_threaten_oklahom\
a_texas_storyid63146361:Summary {name:"summary_of_abcnews_go_com_US_violent_long_las\
ting_tornadoes_threaten_oklahoma_texas_storyid63146361", uri:"<https://abcnews.go.co\
m/US/violent-long-lasting-tornadoes-threaten-oklahoma-texas/story?id=63146361>", sum\
mary:"Part of the system that delivered severe weather to the central U.S. over the \
weekend is moving into the Northeast today, producing strong to severe storms -- dam\
Automatically Generating Data for Knowledge Graphs
aging winds, hail or isolated tornadoes can't be ruled out. Severe weather is foreca\
st to continue on Tuesday, with the western storm moving east into the Midwest and p\
arts of the mid-Mississippi Valley."})
The following listing of file src/kgcreator/neo4j.lisp is similar to the code that generated RDF in
the last section:
(in-package #:kgcreator)
(let ((*entity-nodes-hash*))
(defun cypher-from-files (output-file-path text-and-meta-pairs)
(setf *entity-nodes-hash* (make-hash-table :test #'equal :size 200))
;;(print (list "==> cypher-from-files"output-file-path text-and-meta-pairs ))
(with-open-file
(str output-file-path
:direction :output
:if-exists :supersede
:if-does-not-exist :create)
(defun generateNeo4jCategoryNodes ()
(let* ((names categorize_summarize::categoryNames))
(dolist (name names)
(format str
(myutils:replace-all
(concatenate
'string "CREATE (" name ":CategoryType {name:\"" name "\"})~%")
"/" "_"))))
(format str "~%"))
(defun cypher-from-files-handle-single-file (text-input-file meta-input-file)
(let* ((text (file-to-string text-input-file))
(words (myutils:words-from-string text))
(meta (file-to-string meta-input-file)))
(defun generate-original-doc-node ()
(let ((node-name (node-name-from-uri meta)))
(if (null (gethash node-name *entity-nodes-hash*))
(let* ((cats (categorize words))
(sum (summarize words cats)))
(setf (gethash node-name *entity-nodes-hash*) t)
(format str (concatenate 'string "CREATE (" node-name ":News {name:\""
Automatically Generating Data for Knowledge Graphs
node-name "\", uri: \"" meta
"\", summary: \"" sum "\"})~%"))
(dolist (cat cats)
(let ((hash-check (concatenate 'string node-name (car cat))))
(if (null (gethash hash-check *entity-nodes-hash*))
(let ()
(setf (gethash hash-check *entity-nodes-hash*) t)
(format str (concatenate 'string "CREATE (" node-name
")-[:Category]->("
(car cat) ")~%"))))))))))
(defun generate-dbpedia-nodes (key entity-pairs)
(dolist (entity-pair entity-pairs)
(if (null (gethash (node-name-from-uri (cadr entity-pair))
*entity-nodes-hash*))
(let ()
(setf (gethash (node-name-from-uri (cadr entity-pair)) *entity-nodes-hash*) t)
(format str
(concatenate 'string "CREATE (" (node-name-from-uri (cadr entity-pair))
key " {name: \"" (car entity-pair)
"\", uri: \"" (cadr entity-pair) "\"})~%"))))))
(defun generate-dbpedia-contains-cypher (key value)
(generate-original-doc-node)
(generate-dbpedia-nodes key value)
(let ((relation-name (concatenate 'string key "DbPediaLink")))
(dolist (entity-pair value)
(let* ((node-name (node-name-from-uri meta))
(object-node-name (node-name-from-uri (cadr entity-pair)))
(hash-check (concatenate 'string node-name object-node-name)))
(if (null (gethash hash-check *entity-nodes-hash*))
(let ()
(setf (gethash hash-check *entity-nodes-hash*) t)
(format str (concatenate 'string
"CREATE (" node-name ")-[:"
relation-name "]->(" object-node-name ")~%"))))))))
;; start code for cypher-from-files (output-file-path text-and-meta-pairs)
(generateNeo4jCategoryNodes) ;; just once, not for every input file
(dolist (pair text-and-meta-pairs)
(cypher-from-files-handle-single-file (car pair) (cadr pair))
(let ((h (entities_dbpedia:find-entities-in-text (file-to-string (car pair)))))
Automatically Generating Data for Knowledge Graphs
(entities_dbpedia:entity-iterator #'generate-dbpedia-contains-cypher h))))))
(defvar test_files '((#P"~/GITHUB/common-lisp/kgcreator/test_data/test3.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test3.meta")
(#P"~/GITHUB/common-lisp/kgcreator/test_data/test2.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test2.meta")
(#P"~/GITHUB/common-lisp/kgcreator/test_data/test1.txt"
#P"~/GITHUB/common-lisp/kgcreator/test_data/test1.meta")))
(defun test2a ()
(cypher-from-files "out.cypher" test_files))
You can load all of KGCreator but just execute the test function at the end of this file using:
(ql:quickload "kgcreator")
(in-package #:kgcreator)
(kgcreator:test2a)
Implementing the Top Level Application APIs
The code in the file src/kgcreator/kgcreator.lisp uses both rdf.lisp and neo4j.lisp that we saw in
the last two sections. The function get-files-and-meta looks at the contents of an input directory
to generate a list of pairs, each pair containing the path to a text file and the meta file for the
corresponding text file.
We are using the opts package to parse command line arguments. This will be used when we
build a single file standalone executable file for the entire KGCreator application, including the
web application that we will see in a later section.
;; KGCreator main program
(in-package #:kgcreator)
(ensure-directories-exist "temp/")
(defun get-files-and-meta (fpath)
(let ((data (directory (concatenate 'string fpath "/" "*.txt")))
(meta (directory (concatenate 'string fpath "/" "*.meta"))))
(if (not (equal (length data) (length meta)))
(let ()
Automatically Generating Data for Knowledge Graphs
(princ "Error: must be matching *.meta files for each *.txt file")
(terpri)
(let ((ret '()))
(dotimes (i (length data))
(setq ret (cons (list (nth i data) (nth i meta)) ret)))
ret))))
(opts:define-opts
(:name :help
:description
"KGcreator command line example: ./KGcreator -i test_data -r out.rdf -c out.cyp\
:short #\h
:long "help")
(:name :rdf
:description "RDF output file name"
:short #\r
:long "rdf"
:arg-parser #'identity ;; <- takes an argument
:arg-parser #'identity) ;; <- takes an argument
(:name :cypher
:description "Cypher output file name"
:short #\c
:long "cypher"
:arg-parser #'identity) ;; <- takes an argument
(:name :inputdir
:description "Cypher output file name"
:short #\i
:long "inputdir"
:arg-parser #'identity)) ;; <- takes an argument
(defun kgcreator () ;; don't need: &aux args sb-ext:*posix-argv*)
(handler-case
(let* ((opts (opts:get-opts))
(input-path
(if (find :inputdir opts)
(nth (1+ (position :inputdir opts)) opts)))
(rdf-output-path
(if (find :rdf opts)
(nth (1+ (position :rdf opts)) opts)))
(cypher-output-path
Automatically Generating Data for Knowledge Graphs
(if (find :cypher opts)
(nth (1+ (position :cypher opts)) opts))))
(format t "input-path: ~a
rdf-output-path: ~a cypher-output-path:~a~%"
input-path rdf-output-path cypher-output-path)
(if (not input-path)
(format t "You must specify an input path.~%")
(locally
(declare #+sbcl(sb-ext:muffle-conditions sb-kernel:redefinition-warning))
(handler-bind
(#+sbcl(sb-kernel:redefinition-warning #'muffle-warning))
;; stuff that emits redefinition-warning's
(let ()
(if rdf-output-path
(rdf-from-files rdf-output-path (get-files-and-meta input-path)))
(if cypher-output-path
(cypher-from-files cypher-output-path (get-files-and-meta input-path))))))))
(t (c)
(format t "We caught a runtime error: ~a~%" c)
(values 0 c)))
(format t "~%Shutting down KGcreator - done processing~%~%"))
(defun test1 ()
(get-files-and-meta
"~/GITHUB/common-lisp/kgcreator/test_data"))
(defun print-hash-entry (key value)
(format t "The value associated with the key ~S is ~S~%" key value))
(defun test2 ()
(let ((h (entities_dbpedia:find-entities-in-text "Bill Clinton and George Bush wen\
t to Mexico and England and watched Univision. They enjoyed Dakbayan sa Dabaw and sh\
oped at Best Buy and listened to Al Stewart. They agree on RepÃºblica de Nicaragua a\
nd support Sweden Democrats and Leicestershire Miners Association and both sent thei\
r kids to Darul Uloom Deoband.")))
(entities_dbpedia:entity-iterator #'print-hash-entry h)))
(defun test7 ()
(rdf-from-files "out.rdf" (get-files-and-meta "test_data")))
You can load all of KGCreator but just execute the three test functions at the end of this file using:
Automatically Generating Data for Knowledge Graphs
(ql:quickload "kgcreator")
(in-package #:kgcreator)
(kgcreator:test1)
(kgcreator:test2)
(kgcreator:test7)
Implementing The Web Interface
When we build a standalone single file application for KGCreator, we include a simple web
application interface that allows users to enter input text and see generated RDF and Neo4j Cypher
The file src/kgcreator/web.lisp uses the libraries cl-who hunchentoot parenscript that we used
earlier. The function write-files-run-code** (lines 8-43) takes raw text, and writes generated RDF and
Neo4j Cypher data to local temporary files that are then read and formatted to HTML for display.
The code in rdf.lisp and neo4j.lisp is file oriented, and I wrote web.lisp as an afterthought so it was
easier writing temporary files than refactoring rdf.lisp and neo4j.lisp to write to strings.
(in-package #:kgcreator)
(ql:quickload '(cl-who hunchentoot parenscript))
(setf (html-mode) :html5)
(defun write-files-run-code (a-uri raw-text)
(if (< (length raw-text) 10)
(list "not enough text" "not enough text")
;; generate random file number
(let* ((filenum (+ 1000 (random 5000)))
(meta-name (concatenate 'string "temp/" (write-to-string filenum) ".meta"))
(text-name (concatenate 'string "temp/" (write-to-string filenum) ".txt"))
(rdf-name (concatenate 'string "temp/" (write-to-string filenum) ".rdf"))
(cypher-name (concatenate 'string "temp/" (write-to-string filenum) ".cypher"))
;; write meta file
(with-open-file (str meta-name
:direction :output
:if-exists :supersede
:if-does-not-exist :create)
(format str a-uri))
;; write text file
Automatically Generating Data for Knowledge Graphs
(with-open-file (str text-name
:direction :output
:if-exists :supersede
:if-does-not-exist :create)
(format str raw-text))
;; generate rdf and cypher files
(rdf-from-files rdf-name (list (list text-name meta-name)))
(cypher-from-files cypher-name (list (list text-name meta-name)))
;; read files and return results
(setf ret
(replace-all
(replace-all
(uiop:read-file-string rdf-name)
">" "&gt;")
"<" "&lt;")
(uiop:read-file-string cypher-name)))
(print (list "ret:" ret))
ret)))
(defvar *h* (make-instance 'easy-acceptor :port 3000))
;; define a handler with the arbitrary name my-greetings:
(define-easy-handler (my-greetings :uri "/") (text)
(setf (hunchentoot:content-type*) "text/html")
(let ((rdf-and-cypher (write-files-run-code "http://test.com/1" text)))
(print (list "*** rdf-and-cypher:" rdf-and-cypher))
(with-html-output-to-string
(*standard-output* nil :prologue t)
(:html
(:head (:title "KGCreator Demo")
(:link :rel "stylesheet" :href "styles.css" :type "text/css"))
(:body
:style "margin: 90px"
(:h1 "Enter plain text for the demo to create RDF and Cypher")
(:p "For more information on the KGCreator product please visit the web site:"
(:a :href "https://markwatson.com/products/" "Mark Watson's commercial products"\
(:p "The KGCreator product is a command line tool that processes all text "
"web applications and files in a source directory and produces both RDF data "
"triples for semantic Cypher input data files for the Neo4j graph database. "
"For the purposes of this demo the URI for your input text is hardwired to "
Automatically Generating Data for Knowledge Graphs
"&lt;http://test.com/1&gt; but the KGCreator product offers flexibility "
"for assigning URIs to data sources and further, "
"creates links for relationships between input sources.")
(:p :style "text-align: left"
"To try the demo paste plain text into the following form that contains "
"information on companies, news, politics, famous people, broadcasting "
"networks, political parties, countries and other locations, etc. ")
(:p "Do not include and special characters or character sets:")
(:form
:method :post
(:textarea
:rows "20"
:cols "90"
:name "text"
:value text)
(:input :type :submit :value "Submit text to process"))
(:h3 "RDF:")
(:pre (str (car rdf-and-cypher)))
(:h3 "Cypher:")
(:pre (str (cadr rdf-and-cypher))))))))
(defun kgcweb ()
(hunchentoot:start *h*))
You can load all of KGCreator and start the web application using:
(ql:quickload "kgcreator")
(in-package #:kgcreator)
(kgcweb)
You can access the web app at http://localhost:3000⁸⁵.
Creating a Standalone Application Using SBCL
When I originally wrote KGCreator I intended to develop a commercial product so it was important
to be able to create standalone single file executables. This is simple to do using SBCL:
⁸⁵http://localhost:3000
Automatically Generating Data for Knowledge Graphs
$ sbcl
(ql:quickload "kgcreator")
(in-package #:kgcreator)
(sb-ext:save-lisp-and-die "KGcreator"
:toplevel #'kgcreator :executable t)
As an example, you could run the application on the command line using:
./KGcreator -i test_data -r out.rdf -c out.cyper
Augmenting RDF Triples in a Knowledge Graph Using
DBPedia
You can augment RDF-based Knowledge Graphs that you build with the KGcreator application by
using the library in the directory kg-add-dbpedia-triples.
As seen in the kg-add-dbpedia-triples.asd and package.lisp configuration files, we use two other
libraries developed in this book:
;;;; kg-add-dbpedia-triples.asd
(asdf:defsystem #:kg-add-dbpedia-triples
:description "Add DBPedia triples from an input N-Triples RDF file"
:author "markw@markwatson.com"
:license "Apache 2"
:depends-on (#:myutils #:sparql)
:components ((:file "package")
(:file "add-dbpedia-triples")))
;;;; package.lisp
(defpackage #:kg-add-dbpedia-triples
(:use #:cl #:myutils #:sparql)
(:export #:add-triples))
The library is implemented in the file kg-add-dbpedia-triples.lisp:
Automatically Generating Data for Knowledge Graphs
(in-package #:kg-add-dbpedia-triples)
(defun augmented-triples (a-uri ostream)
(let ((results
(sparql:dbpedia
(format nil "construct { ~A ?p ?o } where { ~A ?p ?o } limit 5" a-uri a-ur\
(dolist (x results)
(dolist (sop x)
(let ((val (second sop)))
(if (and
(stringp val)
(> (length val) 9)
(equal (subseq val 0 7) "http://")
(equal (subseq val 0 8) "https://")))
(format ostream "<~A> " val)
(format ostream "~A " val))))
(format ostream " .~%"))))
(defun add-triples (in-file-name out-file-name)
(let* ((nt-data (myutils:file-to-string in-file-name))
(tokens (myutils:tokenize-string-keep-uri nt-data))
(remove-duplicates
(mapcan #'(lambda (s) (if
(stringp s)
(> (length s) 19)
(equal (subseq s 0 19) "<http://dbpedia.org"))
(list s)))
tokens)
:test #'equal)))
(with-open-file (str out-file-name
:direction :output
:if-exists :supersede
:if-does-not-exist :create)
(dolist (uri uris)
(augmented-triples uri str)))))
Automatically Generating Data for Knowledge Graphs
KGCreator Wrap Up
When developing applications or systems using Knowledge Graphs it is useful to be able to quickly
generate test data which is the primary purpose of KGCreator. A secondary use is to generate
Knowledge Graphs for production use using text data sources. In this second use case you will want
to manually inspect the generated data to verify its correctness or usefulness for your application.
Knowledge Graph Sampler for
Creating Small Custom Knowledge
Graphs
I find it convenient to be able to “sample” small parts of larger knowledge graphs. The example
program in this chapter accepts a list of DBPedia entity URIs, attempts top find links between these
entities, and writes these nodes and discovered edges to a RDF triples file.
The code is in the directory src/kgsampler. As seen in the configuration files kg-add-dbpedia-
triples.asd and package.lisp, we will use the sparql library we developed earlier as well as the
libraries uiop and drakma:
;;;; kgsampler.asd
(asdf:defsystem #:kgsampler
:description "sample knowledge graphs"
:author "Mark Watson markw@markwatson.com"
:license "Apache 2"
:depends-on (#:uiop #:drakma #:sparql)
:components ((:file "package")
(:file "kgsampler")))
;;;; package.lisp
(defpackage #:kgsampler
(:use #:cl #:uiop #:sparql)
(:export #:sample))
The program starts with a list of entities and tries to find links on DBPedia between the entities.
A small sample graph of the input entities and any discovered links is written to a file. The
function dbpedia-as-nt spawns a process to use the curl utility to make a HTTP request to DBPedia.
The function construct-from-dbpedia takes a list of entities and writes SPARQL CONSTRUCT
statements with the entity as the subject and the object filtered to a string value in the English
language to an output stream. The function find-relations runs at O(N^2) where N is the number
of input entities so you should avoid using this program with a large number of input entities.
I offer this code with little explanation since much of it is similar to the techniques you saw in the
previous chapter Knowledge Graph Navigator.
Knowledge Graph Sampler for Creating Small Custom Knowledge Graphs
;; kgsampler main program
(in-package #:kgsampler)
(defun dbpedia-as-nt (query)
(print query)
(uiop:run-program
"curl"
(concatenate 'string
"https://dbpedia.org/sparql?format=text/ntriples&query="
;; formats that work: csv, text/ntriples, text/ttl
(drakma:url-encode query :utf-8)))
:output :string))
(defun construct-from-dbpedia (entity-uri-list &key (output-stream t))
(dolist (entity-uri entity-uri-list)
(format output-stream "~%~%# ENTITY NAME: ~A~%~%" entity-uri)
(format
output-stream
(dbpedia-as-nt
(format nil
"CONSTRUCT { ~A ?p ?o } where { ~A ?p ?o
. FILTER(lang(?o) = 'en') }"\
entity-uri entity-uri)))))
(defun ensure-angle-brackets (s)
"make sure URIs have angle brackets"
(if (equal #\< (char s 0))
(concatenate 'string "<" s ">")))
(defun find-relations (entity-uri-list &key (output-stream t))
(dolist (entity-uri1 entity-uri-list)
(dolist (entity-uri2 entity-uri-list)
(if (not (equal entity-uri1 entity-uri2))
(let ((possible-relations
(mapcar #'cadar
(sparql::dbpedia
(format nil
"select ?p where { ~A ?p ~A . filter(!regex(str(?p\
), \"page\", \"i\"))} limit 50"
entity-uri1 entity-uri2)))))
Knowledge Graph Sampler for Creating Small Custom Knowledge Graphs
(print "** possible-relations:") (print possible-relations)
(dolist (pr possible-relations)
(format output-stream "~A ~A ~a .~%"
entity-uri1
(ensure-angle-brackets pr)
entity-uri2)))))))
(defun sample (entity-uri-list output-filepath)
(with-open-file (ostream
(pathname output-filepath) :direction :output :if-exists\
:supersede)
(construct-from-dbpedia entity-uri-list :output-stream ostream)
(find-relations entity-uri-list :output-stream ostream)))
Let’s start by running the two helper functions interactively so you can see their output (output
edited for brevity). The top level function kgsampler:sample for this example takes a list of entity
URIs and an output file name, and uses the functions construct-from-dbpedia entity-uri-list and
find-relations to write triples for the entities and then for the relationships discovered between
entities. The following listing also calls the helper function kgsampler::find-relations to show you
what its output looks like.
$ sbcl
* (ql:quickload "kgsampler")
(kgsampler::construct-from-dbpedia '("<http://dbpedia.org/resource/Bill_Gates>" \
"<http://dbpedia.org/resource/Steve_Jobs>") :output-stream nil)
"CONSTRUCT { <http://dbpedia.org/resource/Bill_Gates> ?p ?o } where { <http://dbpedi\
a.org/resource/Bill_Gates> ?p ?o
. FILTER (lang(?o) = 'en') }"
"CONSTRUCT { <http://dbpedia.org/resource/Bill_Gates> <http://purl.org/dc/terms/subj\
ect> ?o } where { <http://dbpedia.org/resource/Bill_Gates> <http://purl.org/dc/terms\
/subject> ?o
* (kgsampler::find-relations '("<http://dbpedia.org/resource/Bill_Gates>" "<http://d\
bpedia.org/resource/Microsoft>") :output-stream nil)
("dbpedia SPARQL:"
"select ?p where { <http://dbpedia.org/resource/Bill_Gates> ?p <http://dbpedia.org/\
resource/Microsoft> . filter(!regex(str(?p), \"page\", \"i\"))} limit 50"
"** possible-relations:"
("http://dbpedia.org/ontology/knownFor")
"http://dbpedia.org/ontology/knownFor"
Knowledge Graph Sampler for Creating Small Custom Knowledge Graphs
("dbpedia SPARQL:"
"select ?p where { <http://dbpedia.org/resource/Microsoft> ?p <http://dbpedia.org/r\
esource/Bill_Gates> . filter(!regex(str(?p), \"page\", \"i\"))} limit 50"
"** possible-relations:"
("http://dbpedia.org/property/founders" "http://dbpedia.org/ontology/foundedBy")
"http://dbpedia.org/property/founders"
"http://dbpedia.org/ontology/foundedBy"
We now use the main function to generate an output RDF triple file:
$ sbcl
* (ql:quickload "kgsampler")
* (kgsampler:sample '("<http://dbpedia.org/resource/Bill_Gates>" "<http://dbpedia.or\
g/resource/Steve_Jobs>" "<http://dbpedia.org/resource/Microsoft>")
"test.nt")
"CONSTRUCT { <http://dbpedia.org/resource/Bill_Gates> ?p ?o } where { <http://dbpedi\
a.org/resource/Bill_Gates> ?p ?o
. FILTER (lang(?o) = 'en') }"
("ndbpedia SPARQL:n"
"select ?p where { <http://dbpedia.org/resource/Bill_Gates> ?p <http://dbpedia.org/\
resource/Microsoft> . filter(!regex(str(?p), \"page\", \"i\"))} limit 50"
"** possible-relations:"
("http://dbpedia.org/ontology/board")
("dbpedia SPARQL:"
"select ?p where { <http://dbpedia.org/resource/Steve_Jobs> ?p <http://dbpedia.org/\
resource/Bill_Gates> . filter(!regex(str(?p), \"page\", \"i\"))} limit 50"
Output RDF N-Triple data is written to the file sample-KG.nt. A very small part of this file is listed
# ENTITY NAME: <http://dbpedia.org/resource/Bill_Gates>
<http://dbpedia.org/resource/Bill_Gates>
<http://dbpedia.org/ontology/abstrac\
"William Henry \"Bill\" Gates III (born October 28, 1955) is an American busines\
s magnate,...."@en .
<http://dbpedia.org/resource/Bill_Gates>
<http://xmlns.com/foaf/0.1/name>
"Bill Gates"@en .
<http://dbpedia.org/resource/Bill_Gates>
<http://xmlns.com/foaf/0.1/surname>
Knowledge Graph Sampler for Creating Small Custom Knowledge Graphs
"Gates"@en .
<http://dbpedia.org/resource/Bill_Gates>
<http://dbpedia.org/ontology/title>
"Co-Chairmanof theBill & Melinda Gates Foundation"@en .
The same data in Turtle RDF format can be seen in the file sample-KG.ttl that was produced by
importing the triples file into the free edition of GraphDB exporting it to the Turtle file sample-
KG.ttl that I find easier to read. GraphDB has visualization tools which I use here to generate an
interactive graph display of this data:
GraphDB Visual graph of generated RDF triples
This example is also set up for people and companies. I may expand it in the future to other types
of entities as I need them.
This example program takes several minutes to run since many SPARQL queries are made to
DBPedia. I am a non-corporate member of the DBPedia organization. Here is a membership
application⁸⁶ if you are interested in joining me there.
⁸⁶https://www.dbpedia.org/members/membership/
Knowledge Graph Navigator Common
Library Implementation
The Knowledge Graph Navigator (which I will often refer to as KGN) is a tool for processing a set
of entity names and automatically exploring the public Knowledge Graph DBPedia⁸⁷ using SPARQL
queries. I started to write KGN for my own use, to automate some things I used to do manually
when exploring Knowledge Graphs, and later thought that KGN might be also useful for educational
purposes. KGN shows the user the auto-generated SPARQL queries so hopefully the user will learn
by seeing examples. KGN uses NLP code developed in earlier chapters and we will reuse that code
with a short review of using the APIs.
In previous versions of this book, this example was hard-wired to use LispWork CAPI for the
user interface. This old version is in src/kgn in the main GitHub repository for this book:
https://github.com/mark-watson/loving-common-lisp⁸⁸ and has a few UI components like a progress
bar that I removed since the previous edition. The new version has separate GitHub repositories for:
• https://github.com/mark-watson/kgn-common⁸⁹ for the Knowledge Graph Navigator common
library.
• https://github.com/mark-watson/kgn-text-ui⁹⁰ for a text interface for the Knowledge Graph
Navigator.
• https://github.com/mark-watson/kgn-capi-ui⁹¹ for a LispWorks CAPI GUI interface for the
Knowledge Graph Navigator.
If you followed the code example setup instructions in the book Preface or in the README file in
the main repo https://github.com/mark-watson/loving-common-lisp⁹² then all three of these projects
are available for loading via Quicklisp on your computer.
After looking at SPARQL generated by this example for an example query, we will start a process
of bottom up development, first writing low level functions to automate SPARQL queries, writing
utilities we will need for the UIs developed in later chapters.
Since the DBPedia SPARQL queries are time consuming, we will also implement a caching layer
using SQLite that will make the app more responsive. The cache is especially helpful during
development when the same queries are repeatedly used for testing.
⁸⁷http://dbpedia.org
⁸⁸https://github.com/mark-watson/loving-common-lisp
⁸⁹https://github.com/mark-watson/kgn-common
⁹⁰https://github.com/mark-watson/kgn-text-ui
⁹¹https://github.com/mark-watson/kgn-capi-ui
⁹²https://github.com/mark-watson/loving-common-lisp
Knowledge Graph Navigator Common Library Implementation
The code for this reusable library is in the directory src/kgn-common. This is common library that
will be used for user interfaces developed in later chapters. There is a lot of code in the following
program listings and I hope to provide you with a roadmap overview of the code, diving in on code
that you might want to reuse for your own projects and some representative code for generating
SPARQL queries.
Let’s start by looking at the files for the common library:
• Makefile - contains development shortcuts.
• data - data used to remove stop words from text.
• kgn-common.lisp - main code file for library.
• package.lisp - standard Common Lisp package definition.
• utils.lisp - miscelanious utility functions.
• README.txt
• kgn-common.asd - standard Common Lisp ASDF definition.
Example Output
Before we get started studying the implementation, let’s look at sample output in order to help give
meaning to the code we will look at later. Consider a query that a user might type into the top query
field in the KGN app:
Steve Jobs lived near San Francisco and was
a founder of <http://dbpedia.org/resource/Apple_Inc.>
The system will try to recognize entities in a query. If you know the DBPedia URI of an entity, like
the company Apple in this example, you can use that directly. Note that in the SPARQL URIs are
surrounded with angle bracket characters.
The application prints out automatically generated SPARQL queries. For the above listed example
query the following output will be generated (some editing to fit page width):
Trying to get entity by name = Steve Jobs using SPARQL with type:
select distinct ?s ?comment { ?s ?p "Steve Jobs"@en .
?s <http://www.w3.org/2000/01/rdf-schema#comment> ?comment .
FILTER ( lang ( ?comment ) = 'en' ) .
?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://dbpedia.org/ontology/Person> .
} LIMIT 15
Knowledge Graph Navigator Common Library Implementation
Trying to get entity by name = San Francisco using SPARQL with type:
select distinct ?s ?comment { ?s ?p "San Francisco"@en .
?s <http://www.w3.org/2000/01/rdf-schema#comment> ?comment .
FILTER ( lang ( ?comment ) = 'en' ) .
?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://dbpedia.org/ontology/City> .
} LIMIT 15
SPARQL to get PERSON data for <http://dbpedia.org/resource/Steve_Jobs>:
SELECT DISTINCT ?label ?comment
( GROUP_CONCAT ( DISTINCT ?birthplace ; SEPARATOR=' | ' ) AS ?birthplace )
( GROUP_CONCAT ( DISTINCT ?almamater ; SEPARATOR=' | ' ) AS ?almamater )
( GROUP_CONCAT ( DISTINCT ?spouse ; SEPARATOR=' | ' ) AS ?spouse ) {
<http://dbpedia.org/resource/Steve_Jobs>
<http://www.w3.org/2000/01/rdf-schema#comment>
?comment .
FILTER ( lang ( ?comment ) = 'en' ) .
OPTIONAL { <http://dbpedia.org/resource/Steve_Jobs>
<http://dbpedia.org/ontology/birthPlace>
?birthplace } .
OPTIONAL { <http://dbpedia.org/resource/Steve_Jobs>
<http://dbpedia.org/ontology/almaMater>
?almamater } .
OPTIONAL { <http://dbpedia.org/resource/Steve_Jobs>
<http://dbpedia.org/ontology/spouse>
?spouse } .
OPTIONAL { <http://dbpedia.org/resource/Steve_Jobs>
<http://www.w3.org/2000/01/rdf-schema#label>
?label .
FILTER ( lang ( ?label ) = 'en' ) }
} LIMIT 10
Remember, the SPARQL is generated by KGN from natural language queries. Some more examples:
SPARQL to get CITY data for <http://dbpedia.org/resource/San_Francisco>:
Knowledge Graph Navigator Common Library Implementation
SELECT DISTINCT ?label ?comment
( GROUP_CONCAT ( DISTINCT ?latitude_longitude ; SEPARATOR=' | ' )
AS ?latitude_longitude )
( GROUP_CONCAT ( DISTINCT ?populationDensity ; SEPARATOR=' | ' )
AS ?populationDensity )
( GROUP_CONCAT ( DISTINCT ?country ; SEPARATOR=' | ' )
AS ?country ) {
<http://dbpedia.org/resource/San_Francisco>
<http://www.w3.org/2000/01/rdf-schema#comment>
?comment .
FILTER ( lang ( ?comment ) = 'en' ) .
OPTIONAL { <http://dbpedia.org/resource/San_Francisco>
<http://www.w3.org/2003/01/geo/wgs84_pos#geometry>
?latitude_longitude } .
OPTIONAL { <http://dbpedia.org/resource/San_Francisco>
<http://dbpedia.org/ontology/PopulatedPlace/populationDensity>
?populationDensity } .
OPTIONAL { <http://dbpedia.org/resource/San_Francisco>
<http://dbpedia.org/ontology/country>
?country } .
OPTIONAL { <http://dbpedia.org/resource/San_Francisco>
<http://www.w3.org/2000/01/rdf-schema#label>
?label . }
} LIMIT 30
SPARQL to get COMPANY data for <http://dbpedia.org/resource/Apple_Inc.>:
SELECT DISTINCT ?label ?comment ( GROUP_CONCAT ( DISTINCT ?industry ; SEPARATOR=' | \
AS ?industry )
( GROUP_CONCAT ( DISTINCT ?netIncome ; SEPARATOR=' | ' )
AS ?netIncome )
( GROUP_CONCAT ( DISTINCT ?numberOfEmployees ; SEPARATOR=' | ' )
AS ?numberOfEmployees ) {
<http://dbpedia.org/resource/Apple_Inc.>
<http://www.w3.org/2000/01/rdf-schema#comment> ?comment .
FILTER ( lang ( ?comment ) = 'en' ) .
OPTIONAL { <http://dbpedia.org/resource/Apple_Inc.>
<http://dbpedia.org/ontology/industry>
?industry } .
OPTIONAL { <http://dbpedia.org/resource/Apple_Inc.>
Knowledge Graph Navigator Common Library Implementation
<http://dbpedia.org/ontology/netIncome> ?netIncome } .
OPTIONAL { <http://dbpedia.org/resource/Apple_Inc.>
<http://dbpedia.org/ontology/numberOfEmployees> ?numberOfEmployees } .
OPTIONAL { <http://dbpedia.org/resource/Apple_Inc.>
<http://www.w3.org/2000/01/rdf-schema#label> ?label .
FILTER ( lang ( ?label ) = 'en' ) }
} LIMIT 30
Once KGN has identified DBPedia entire URIs, it also searches for relationships between these
entities:
DISCOVERED RELATIONSHIP LINKS:
<http://dbpedia.org/resource/Steve_Jobs>
<http://dbpedia.org/ontology/birthPlace>
<http://dbpedia.org/resource/San_Francisco>
<http://dbpedia.org/resource/Steve_Jobs>
<http://dbpedia.org/ontology/occupation>
<http://dbpedia.org/resource/Apple_Inc.>
<http://dbpedia.org/resource/Steve_Jobs>
<http://dbpedia.org/ontology/board>
<http://dbpedia.org/resource/Apple_Inc.>
<http://dbpedia.org/resource/Steve_Jobs>
<http://www.w3.org/2000/01/rdf-schema#seeAlso> ->
<http://dbpedia.org/resource/Apple_Inc.>
<http://dbpedia.org/resource/Apple_Inc.>
<http://dbpedia.org/property/founders>
<http://dbpedia.org/resource/Steve_Jobs>
After listing the generated SPARQL for finding information for the entities in the query, KGN
searches for relationships between these entities. These discovered relationships can be seen at
the end of the last listing. Please note that this step makes SPARQL queries on O(n^2) where n
is the number of entities. Local caching of SPARQL queries to DBPedia helps make processing many
entities possible.
In addition to showing generated SPARQL and discovered relationships in the middle text pane of
the application, KGN also generates formatted results that are also displayed in the bottom text pane:
Knowledge Graph Navigator Common Library Implementation
- - - ENTITY TYPE: PEOPLE - - -
LABEL: Steve Jobs
COMMENT: Steven Paul "Steve" Jobs was an American information technology
entrepreneur and inventor. He was the co-founder, chairman, and chief
executive officer (CEO) of Apple Inc.; CEO and majority shareholder
of Pixar Animation Studios; a member of The Walt Disney Company's
board of directors following its acquisition of Pixar; and founder,
chairman, and CEO of NeXT Inc. Jobs is widely recognized as a pioneer of
the microcomputer revolution of the 1970s and 1980s, along with Apple
co-founder Steve Wozniak. Shortly after his death, Jobs's official
biographer, Walter Isaacson, described him as a "creative entrepreneur
whose passion for perfection and ferocious drive revolutionized six industries:
personal computers, animated movies, music, phones
BIRTHPLACE: http://dbpedia.org/resource/San_Francisco
ALMAMATER: http://dbpedia.org/resource/Reed_College
SPOUSE: http://dbpedia.org/resource/Laurene_Powell_Jobs
- - - ENTITY TYPE: CITIES - - -
LABEL:
San Francisco
COMMENT: San Francisco, officially the City and County of San Francisco, is the
cultural, commercial, and financial center of Northern California and
the only consolidated city-county in California. San Francisco encompasses a
land area of about 46.9 square miles (121 km2) on the northern end of the
San Francisco Peninsula, which makes it the smallest county in the state.
It has a density of about 18,451 people per square mile (7,124 people per km2),
making it the most densely settled large city (population greater than
200,000) in the state of California and the second-most densely populated
major city in the United States after New York City. San Francisco is
the fourth-most populous city in California, after Los Angeles, San Diego, and
San Jose, and the 13th-most populous cit
LATITUDE--LONGITUDE: POINT(-122.41666412354 37.783332824707)
POPULATION-DENSITY: 7123.97092726667
COUNTRY: http://dbpedia.org/resource/United_States
Knowledge Graph Navigator Common Library Implementation
- - - ENTITY TYPE: COMPANIES - - -
LABEL: Apple Inc.
COMMENT: Apple Inc. is an American multinational technology company headquartered
in Cupertino,
California, that designs, develops, and sells consumer electronics,
computer software, and online services. Its hardware products include the
iPhone smartphone, the iPad tablet computer, the Mac personal computer, the
iPod portable media player, the Apple Watch smartwatch, and the Apple TV digital
media player. Apple's consumer software includes the macOS and iOS operating
systems, the iTunes media player, the Safari web browser, and the iLife and
iWork creativity and productivity suites. Its online services include the
iTunes Store, the iOS App Store and Mac App Store, Apple Music, and iCloud.
INDUSTRY: http://dbpedia.org/resource/Computer_hardware |
http://dbpedia.org/resource/Computer_software |
http://dbpedia.org/resource/Consumer_electronics |
http://dbpedia.org/resource/Corporate_Venture_Capital |
http://dbpedia.org/resource/Digital_distribution |
http://dbpedia.org/resource/Fabless_manufacturing
NET-INCOME: 5.3394E10
NUMBER-OF-EMPLOYEES: 115000
Hopefully after reading through sample output and seeing the screen shot of the application, you
now have a better idea what this example application does. Now we will look at project configuration
and then implementation.
Project Configuration and Running the Application
The following listing of kgn.asd shows the ten packages this example depends on (five of these are
also examples in this book, and five are in the public Quicklisp repository):
Knowledge Graph Navigator Common Library Implementation
;;;; knowledgegraphnavigator.asd
((asdf:defsystem #:kgn-common
:description "common utilities for Knowledge Graph Navigator"
:author "Mark Watson <markw@markwatson.com>"
:license "Apache 2"
:depends-on (#:sqlite #:cl-json #:alexandria #:drakma #:myutils #:entities #:entit\
y-uris #:kbnlp #:sparql-cache)
:components ((:file "package")
(:file "utils")
(:file "kgn-common")))
Listing of package.lisp:
;;;; package.lisp
(defpackage #:kgn-common
(:use #:cl #:alexandria #:myutils
#:myutils #:sparql-cache
#:entities #:entity-uris #:kbnlp)
(:export #:kgn-common #:remove-stop-words
#:entity-results->relationship-links
#:get-entity-data-helper #:handle-URIs-in-query
#:remove-uris-from-query #:get-URIs-in-query #:display-entity-results
#:string-shorten #:prompt-string #:dbpedia-get-product-detail
#:dbpedia-get-person-detail #:dbpedia-get-country-detail
#:dbpedia-get-city-detail #:dbpedia-get-company-detail #:clean-results
#:dbpedia-get-entities-by-name #:clean-comment))
We use ql:quickload to load the KGN common library and call a few APIs (some output removed
for brevity):
$ sbcl
This is SBCL 2.1.10, an implementation of ANSI Common Lisp.
* (ql :kgn-common)
To load "kgn-common":
Load 1 ASDF system:
kgn-common
; Loading "kgn-common"
..................
"Starting to load data...."
"....done loading data."
To load "sqlite":
Knowledge Graph Navigator Common Library Implementation
Load 1 ASDF system:
sqlite
; Loading "sqlite"
To load "cl-json":
Load 1 ASDF system:
cl-json
; Loading "cl-json"
To load "drakma":
Load 1 ASDF system:
drakma
; Loading "drakma"
[package kgn-common]..
(:kgn-common)
(kgn-common::dbpedia-get-relationships "<http://dbpedia.org/resource/Bill_Gates>"\
"<http://dbpedia.org/resource/Microsoft>")
("<http://dbpedia.org/ontology/knownFor>")
(kgn-common:dbpedia-get-entities-by-name "Bill Gates" "<http://dbpedia.org/ontolo\
gy/Person>" "<http://schema.org/Person>" :message-stream nil)
(((:s "http://dbpedia.org/resource/Bill_Gates")
(:comment
"William Henry Gates III (born October 28, 1955) is an American business magnate,\
software developer, investor, author, and philanthropist. He is a co-founder of Mic\
rosoft, along with his late childhood friend Paul Allen. During his career at Micros\
oft, Gates held the positions of chairman, chief executive officer (CEO), president \
and chief software architect, while also being the largest individual shareholder un\
til May 2014. He is considered one of the best known entrepreneurs of the microcompu\
ter revolution of the 1970s and 1980s."))
((:s "http://dbpedia.org/resource/Harry_R._Lewis")
(:comment
"Harry Roy Lewis (born 1947) is an American computer scientist, mathe 00ADma 00AD\
ti 00ADcian, and uni 00ADver 00ADsity admin 00ADi 00ADstra 00ADtor known for his res\
earch in com 00ADpu 00ADta 00ADtional logic, textbooks in theoretical computer scien\
ce, and writings on computing, higher education, and technology. He is Gordon McKay \
Professor of Computer Science at Harvard University, and was Dean of Harvard College\
from 1995 to 2003. A new professorship in Engineering and Applied Sciences, endowed\
by a former student, will be named for Lewis and his wife upon their retirements.")\
((:s "http://dbpedia.org/resource/Cascade_Investment")
Knowledge Graph Navigator Common Library Implementation
(:comment
"Cascade Investment, L.L.C. is an American holding company and private investment\
firm headquartered in Kirkland, Washington, United States. It is controlled by Bill\
Gates, and managed by Michael Larson. More than half of Gates' fortune is held in a\
ssets outside his holding of Microsoft shares. Cascade is the successor company to D\
ominion Income Management, the former investment vehicle for Gates' holdings, which \
was managed by convicted felon Andrew Evans."))
((:s "http://dbpedia.org/resource/Jerry_Dyer")
(:comment
"Jerry P. Dyer (born May 3, 1959) is an American politician and former law enforc\
ement officer. He is the 26th and current mayor of Fresno, California. Previously, h\
e served as the chief of the Fresno Police Department.")))
select distinct ?s ?comment { ?s ?p "Bill Gates"@en . @@ ?s <http://www.w3.org/2000/\
01/rdf-schema#comment>
?comment
. FILTER
(lang(?comment) = 'en') . @@ ?s <http://\
www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/Person> . @@ \
} LIMIT 15
(((:s "http://dbpedia.org/resource/Bill_Gates")
(:comment
"William Henry Gates III (born October 28, 1955) is an American business magnate,\
software developer, investor, author, and philanthropist. He is a co-founder of Mic\
rosoft, along with his late childhood friend Paul Allen. During his career at Micros\
oft, Gates held the positions of chairman, chief executive officer (CEO), president \
and chief software architect, while also being the largest individual shareholder un\
til May 2014. He is considered one of the best known entrepreneurs of the microcompu\
ter revolution of the 1970s and 1980s."))
((:s "http://dbpedia.org/resource/Harry_R._Lewis")
(:comment
"Harry Roy Lewis (born 1947) is an American computer scientist, mathe 00ADma 00AD\
ti 00ADcian, and uni 00ADver 00ADsity admin 00ADi 00ADstra 00ADtor known for his res\
earch in com 00ADpu 00ADta 00ADtional logic, textbooks in theoretical computer scien\
ce, and writings on computing, higher education, and technology. He is Gordon McKay \
Professor of Computer Science at Harvard University, and was Dean of Harvard College\
from 1995 to 2003. A new professorship in Engineering and Applied Sciences, endowed\
by a former student, will be named for Lewis and his wife upon their retirements.")\
((:s "http://dbpedia.org/resource/Cascade_Investment")
(:comment
"Cascade Investment, L.L.C. is an American holding company and private investment\
firm headquartered in Kirkland, Washington, United States. It is controlled by Bill\
Gates, and managed by Michael Larson. More than half of Gates' fortune is held in a\
ssets outside his holding of Microsoft shares. Cascade is the successor company to D\
ominion Income Management, the former investment vehicle for Gates' holdings, which \
Knowledge Graph Navigator Common Library Implementation
was managed by convicted felon Andrew Evans."))
((:s "http://dbpedia.org/resource/Jerry_Dyer")
(:comment
"Jerry P. Dyer (born May 3, 1959) is an American politician and former law enforc\
ement officer. He is the 26th and current mayor of Fresno, California. Previously, h\
e served as the chief of the Fresno Police Department.")))
In this last example, using :message-stream nil effectively turns off printing generated SPARQL
queries used by these APIs. You can use :message-stream t to see generated SPARQL.
Every time the KGN common library makes a web service call to DBPedia the query and response are
cached in a SQLite database in ∼/.kgn_cache.db which can greatly speed up the program, especially
in development mode when testing a set of queries. This caching also takes some load off of the public
DBPedia endpoint, which is a polite thing to do.
Review of NLP Utilities Used in Application
Here is a quick review of NLP utilities we saw in an earlier chapter:
• kbnlp:make-text-object
• kbnlp::text-human-names
• kbnlp::text-place-name
• entity-uris:find-entities-in-text
• entity-uris:pp-entities
The following code snippets show example calls to the relevant NLP functions and the generated
output:
KGN 39 > (setf text "Bill Clinton went to Canada")
"Bill Clinton went to Canada"
KGN 40 > (setf txtobj (kbnlp:make-text-object text))
#S(TEXT :URL "" :TITLE "" :SUMMARY "<no summary>" :CATEGORY-TAGS (("computers_micros\
oft.txt" 0.00641) ("religion_islam.txt" 0.00357)) :KEY-WORDS NIL :KEY-PHRASES NIL :H\
UMAN-NAMES ("Bill Clinton") :PLACE-NAMES ("Canada") :COMPANY-NAMES NIL :TEXT #("Bill\
" "Clinton" "went" "to" "Canada") :TAGS #("NNP" "NNP" "VBD" "TO" "NNP"))
KGN 41 > (kbnlp::text-human-names txtobj)
("Bill Clinton")
KGN 42 >
Knowledge Graph Navigator Common Library Implementation
(loop for key being the hash-keys of
(entity-uris:find-entities-in-text text)
using (hash-value value)
do (format t "key: ~S value: ~S~%" key value))
key: "people" value: (("Bill Clinton" "<http://dbpedia.org/resource/Bill_Clinton>"))
key: "countries" value: (("Canada" "<http://dbpedia.org/resource/Canada>"))
The code using loop at the end of the last repl listing that prints keys and values of a hash table is
from the Common Lisp Cookbook web site⁹³ in the section “Traversing a Hash Table.”
Developing Low-Level SPARQL Utilities
I use the standard command line curl utility program with the Common Lisp package uiop to
make HTML GET requests to the DBPedia public Knowledge Graph and the package drakma to
url-encode parts of a query. The source code is in a separate Quicklisp library located in src/sparql-
cache/sparql.lisp. A non-caching library is also available in src/sparql/sparql.lisp.
In the following listing of src/sparql-cache/sparql.lisp, lines 8, 24, 39, and 55 I use some caching
code that we will look at later. The nested replace-all statements in lines 12-13 are a kluge to remove
Unicode characters that occasionally caused runtime errors in the KGN application.
(in-package #:kgn)
(ql:quickload "cl-json")
(ql:quickload "drakma")
(defun sparql-dbpedia (query)
(let* (ret
(cr (fetch-result-dbpedia query))
(response
(replace-all
(replace-all
(uiop:run-program
"curl"
(concatenate 'string
"https://dbpedia.org/sparql?query="
(drakma:url-encode query :utf-8)
"&format=json"))
⁹³http://cl-cookbook.sourceforge.net/hashes.html
Knowledge Graph Navigator Common Library Implementation
:output :string)
"\\u2013" " ")
"\\u" " "))))
(save-query-result-dbpedia query response)
(ignore-errors
(with-input-from-string
(s response)
(let ((json-as-list (json:decode-json s)))
(mapcar #'(lambda (x)
;;(pprint x)
(mapcar #'(lambda (y)
(list (car y) (cdr (assoc :value (cdr y))))) x))
(cdr (cadddr (cadr json-as-list))))))))
(defun sparql-ask-dbpedia (query)
(let* ((cr (fetch-result-dbpedia query))
(response
(replace-all
(replace-all
(uiop:run-program
"curl"
(concatenate 'string
"https://dbpedia.org/sparql?query="
(drakma:url-encode query :utf-8)
"&format=json"))
:output :string)
"\\u2013" " ")
"\\u" " "))))
(save-query-result-dbpedia query response)
(search "true" response)
nil)))
The code for replacing Unicode characters is messy but prevents problems later when we are using
the query results in the example application.
The code (json-as-list (json:decode-json s)) on line 28 converts a deeply nested JSON response
to nested Common Lisp lists. You may want to print out the list to better understand the mapcar
Knowledge Graph Navigator Common Library Implementation
expression on lines 31-35. There is no magic to writing expressions like this, in a repl I set json-as-
list to the results of one query, and I spent a minute or two experimenting with the nested mapcar
expression to get it to work with my test case.
The implementation for sparql-ask-dbpedia in lines 38-58 is simpler because we don’t have to fully
parse the returned SPARQL query results. A SPARQL ask type query returns a true/false answer
to a query. We will use this to determine the types of entities in query text. While our NLP library
identifies entity types, making additional ask queries to DBPedia to verify entity types will provide
better automated results.
Implementing the Caching Layer
While developing KGN and also using it as an end user, many SPARQL queries to DBPedia contain
repeated entity names so it makes sense to write a caching layer. We use a SQLite database “∼/.kgn_-
cache.db” to store queries and responses.
The caching layer is implemented in the file src/sparql-cache/sparql.lisp and some of the relevant
code is listed here:
;;; SqList caching for SPARQL queries:
(defvar *db-path* (pathname "~/.kgn_cache.db"))
(defun create-dbpedia ()
(sqlite:with-open-database (d *db-path*)
(ignore-errors
(sqlite:execute-single d
"CREATE TABLE dbpedia (query string
PRIMARY KEY ASC, result string)"))))
(defun save-query-result-dbpedia (query result)
(sqlite:with-open-database (d *db-path*)
(ignore-errors
(sqlite:execute-to-list d
"insert into dbpedia (query, result) values (?, ?)"
query result))))
(defun fetch-result-dbpedia (query)
(sqlite:with-open-database (d *db-path*)
(cadar
(sqlite:execute-to-list d
"select * from dbpedia where query = ?" query))))
This caching layer greatly speeds up my own personal use of KGN. Without caching, queries that
contain many entity references simply take too long to run. The UI for the KGN applications in
Knowledge Graph Navigator Common Library Implementation
later chapters have a menu option for clearing the local cache but I almost never use this option
because growing a large cache that is tailored for the types of information I search for makes the
entire system much more responsive.
Utilities in the Main Library File kgn-common.lisp
The utilities in the file src/kgn-common/kgn-common.lisp can be seen in this complete code listing:
(in-package #:kgn-common)
(defun pprint-results (results &key (stream t))
(pprint results stream))
(defun colorize-sparql-local (s
&key (stream nil))
"placeholder - some applications, like the kgn-capi-ui example need to
colorize the sparql output"
(princ s stream))
(defun check-uri (uri)
"sloppy code fix: URIs have different forms - normalize these"
(if (equal (type-of uri) 'cons) (setf uri (second uri)))
(entity-uris:ensure-uri-brackets uri))
(defun clean-comment (comment-string)
"When getting comment strings from DBPedia, there are parts
that I remove for display"
(let ((i1 (search "(" comment-string))
(i2 (search ")" comment-string)))
(if (and i1 i2 (> i2 i1) (> i1 0))
(concatenate 'string (subseq comment-string 0 (- i1 1))
(subseq comment-string (+ i2 1)))
(let ((j1 (search " / " comment-string)))
(if j1
(let ((j2 (search "/" comment-string :start2 (+ j1 2))))
(if (and j1 j2 (> j2 j1) (< (+ j2 1) (length comment-string)))
(concatenate 'string (subseq comment-string 0 j1)
(subseq comment-string (+ j2 1)))
comment-string))
comment-string)
comment-string))))
(defun clean-results (results)
Knowledge Graph Navigator Common Library Implementation
"This function is replaced when we later build GUI apps"
results)
(defun get-name-and-description-for-uri (uri)
(let* ((sparql
(replace-all
(format nil
"select distinct ?name ?comment { @@ ~
values ?nameProperty {<http://www.w3.org/2000/01/rdf-schema#label>
<http://xmlns.com/foaf/0.1/name> } . @@ ~
~A ?nameProperty ?name . @@ ~
~A <http://www.w3.org/2000/01/rdf-schema#comment>
?comment
FILTER
(lang(?comment) = 'en') . @@ ~
} LIMIT 1" uri uri)
"@@" " "))
(results (sparql-cache:dbpedia sparql)))
(list (second (assoc :name (car results)))
(second (assoc :comment (car results))))))
;; (kgn-common::get-name-and-description-for-uri
"<http://dbpedia.org/resource/Apple_Inc.>")
(defun ask-is-type-of (entity-uri type-value-uri) ;; both URIs expected to use surro\
unding < > brackets for SPARQL
(let* ((sparql
(format nil
"ASK { ~A <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ~A }"
(check-uri entity-uri) (check-uri type-value-uri)))
(results (sparql-cache:ask-dbpedia sparql)))
(print sparql)
results))
;; (kgn-common::ask-is-type-of
"<http://dbpedia.org/resource/Apple_Inc.>"
"<http://dbpedia.org/ontology/Company>")
(defun dbpedia-get-entities-by-name (name dbpedia-type schema-org-type
&key (message-stream t)
(colorize-sparql-function #'colorize-sparql-local))
;; http://www.w3.org/1999/02/22-rdf-syntax-ns#type <http://schema.org/Person>
(let* ((sparql
(format nil
Knowledge Graph Navigator Common Library Implementation
"select distinct ?s ?comment {
?s ?p \"~A\"@en . @@ ~
?s <http://www.w3.org/2000/01/rdf-schema#comment>
?comment
. FILTER
(lang(?com\
ment) = 'en') . @@ ~
?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ~A . @@ ~
} LIMIT 15" name dbpedia-type))
(results (sparql-cache:dbpedia (replace-all sparql "@@" " "))))
(print results)
(terpri message-stream)
(format message-stream
"Trying to get entity by name = ~A using SPARQL with type:"
name dbpedia-type)
(terpri message-stream)
(apply colorize-sparql-function (list sparql :stream message-stream))
(if (null results)
(let* ((sparql2
(format nil
"select distinct ?s ?comment {
?s ?p \"~A\"@en . @@ ~
?s <http://www.w3.org/2000/01/rdf-schema#comment>
?comment
. FILTER
(lang(?co\
mment) = 'en') . @@ ~
?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ~A . @@ ~
} LIMIT 15" name schema-org-type)))
(format
"No results for ~A for last SPARQL query using type ~A so trying type ~A"\
name dbpedia-type schema-org-type)
(terpri message-stream)
(setf results (sparql-cache:dbpedia (replace-all sparql2 "@@" " ")))
(if (null results)
(format
"No results for ~A for last SPARQL query using type ~A"
name schema-org-type)
(let* ((filtered (remove-if
#'(lambda (x)
(search "," (cadar x))
(not (equal (first x) :comment))
(not (search "/resource/" (cadar x))))))
results))
(uris (remove-duplicates
Knowledge Graph Navigator Common Library Implementation
#'(lambda (x)
(list (concatenate 'string "<" (cadar x) ">") (cadad\
r x)))
filtered) :test #'equal)))
(format t "~%~%********** dbpedia-get-entities-by-name: uris:~%")
(pprint uris) (terpri)
uris))))
results))
;; (kgn-common:dbpedia-get-entities-by-name
"Bill Gates" "<http://dbpedia.org/ontology/Person>"
"<http://schema.org/Person>" :message-stream nil)
;; in above, pass t for message-stream to see generated SPARQL queries
(defun dbpedia-get-person-detail (person-uri-raw
&key (message-stream t)
(colorize-sparql-function #'colorize-sparql-local))
;; http://dbpedia.org/ontology/birthPlace
(let* ((person-uri (check-uri person-uri-raw))
(query
(format nil
"SELECT DISTINCT ?label ?comment@@ ~
(GROUP_CONCAT (DISTINCT ?birthplace; SEPARATOR=' | ') AS ?birthplace) @@ ~
(GROUP_CONCAT (DISTINCT ?almamater; SEPARATOR=' | ') AS ?almamater) @@ ~
(GROUP_CONCAT (DISTINCT ?spouse; SEPARATOR=' | ') AS ?spouse) { @@ ~
~A <http://www.w3.org/2000/01/rdf-schema#comment>
?comment .@@
FILTER
(lang(?comment) = 'en') . @@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/birthPlace> ?birthplace } . @@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/almaMater> ?almamater } . @@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/spouse> ?spouse } . @@ ~
OPTIONAL { ~A
<http://www.w3.org/2000/01/rdf-schema#label> ?label .@@ ~
FILTER
(lang(?label) = 'en') } @@ ~
} LIMIT 10@@" person-uri person-uri person-uri person-uri person-uri))
(results (sparql-cache:dbpedia
(replace-all query "@@" " "))))
(format message-stream "~%SPARQL to get PERSON data for ~A:~%~%" person-uri)
(apply colorize-sparql-function (list query :stream message-stream))
(format message-stream "~%")
;;results))
(clean-results results)))
;; (kgn-common:dbpedia-get-person-detail "<http://dbpedia.org/resource/Bill_Gates>")
Knowledge Graph Navigator Common Library Implementation
(defun dbpedia-get-company-detail (company-uri-raw
&key (message-stream t)
(colorize-sparql-function #'colorize-sparql-local))
(let* ((company-uri (check-uri company-uri-raw))
(query
(format nil
"SELECT DISTINCT ?label ?comment (GROUP_CONCAT (DISTINCT ?industry; SEPARATOR=' | ')\
AS ?industry)@@ ~
(GROUP_CONCAT (DISTINCT ?netIncome; SEPARATOR=' | ') AS ?netIncome)@@ ~
(GROUP_CONCAT (DISTINCT ?numberOfEmployees; SEPARATOR=' | ') AS ?numberOfEmployees\
) {@@ ~
~A <http://www.w3.org/2000/01/rdf-schema#comment>
?comment .@@
FILTER
(lang(?comment) = 'en') .@@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/industry> ?industry } .@@
OPTIONAL { ~A <http://dbpedia.org/ontology/netIncome> ?netIncome } .@@
OPTIONAL { ~A <http://dbpedia.org/ontology/numberOfEmployees> ?numberOfEmployees }\
OPTIONAL { ~A <http://www.w3.org/2000/01/rdf-schema#label> ?label . FILTER (lang(?\
label) = 'en') } @@ ~
} LIMIT 30@@"
company-uri company-uri company-uri company-uri company-uri))
(results (sparql-cache:dbpedia
(replace-all query "@@" " "))))
(format message-stream "~%SPARQL to get COMPANY data for ~A:~%~%" company-uri)
(apply colorize-sparql-function (list query :stream message-stream))
(format message-stream "~%")
(clean-results results)))
;; (kgn-common:dbpedia-get-company-detail "<http://dbpedia.org/resource/Microsoft>")
(defun dbpedia-get-country-detail (country-uri-raw
&key (message-stream t)
(colorize-sparql-function #'colorize-sparql-local))
(let* ((country-uri (check-uri country-uri-raw))
(query
(format nil
"SELECT DISTINCT ?label ?comment (GROUP_CONCAT (DISTINCT ?areaTotal; SEPARATOR=' | '\
) AS ?areaTotal)@@ ~
(GROUP_CONCAT (DISTINCT ?populationDensity; SEPARATOR=' | ') AS ?populationDensity\
) {@@ ~
~A <http://www.w3.org/2000/01/rdf-schema#comment>
?comment .@@
FILTER
(lang(?comment) = 'en') .@@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/areaTotal> ?areaTotal } .@@
Knowledge Graph Navigator Common Library Implementation
OPTIONAL { ~A <http://dbpedia.org/ontology/populationDensity> ?populationDensity }\
OPTIONAL { ~A <http://www.w3.org/2000/01/rdf-schema#label> ?label . }@@ ~
} LIMIT 30@@"
country-uri country-uri country-uri country-uri country-uri))
(results (sparql-cache:dbpedia
(replace-all query "@@" " "))))
(format message-stream "~%SPARQL to get COUNTRY data for ~A:~%~%" country-uri)
(apply colorize-sparql-function (list query :stream message-stream))
(format message-stream "~%")
(clean-results results)))
;; (kgn-common:dbpedia-get-country-detail "<http://dbpedia.org/resource/Canada>")
(defun dbpedia-get-city-detail (city-uri-raw
&key (message-stream t)
(colorize-sparql-function #'colorize-sparql-local))
(let* ((city-uri (check-uri city-uri-raw))
(query
(format
"SELECT DISTINCT ?label ?comment @@ ~
(GROUP_CONCAT (DISTINCT ?latitude_longitude; SEPARATOR=' | ')
AS ?latitude_longit\
ude) @@ ~
(GROUP_CONCAT (DISTINCT ?populationDensity; SEPARATOR=' | ') AS ?populationDensity\
) @@ ~
(GROUP_CONCAT (DISTINCT ?country; SEPARATOR=' | ') AS ?country) { @@ ~
~A <http://www.w3.org/2000/01/rdf-schema#comment>
?comment . FILTER
(lang(?comme\
nt) = 'en') . @@ ~
OPTIONAL { ~A <http://www.w3.org/2003/01/geo/wgs84_pos#geometry> ?latitude_longitu\
de } . @@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/PopulatedPlace/populationDensity> ?popu\
lationDensity } . @@ ~
OPTIONAL { ~A <http://dbpedia.org/ontology/country> ?country } .@@ ~
OPTIONAL { ~A <http://www.w3.org/2000/01/rdf-schema#label> ?label . } @@ ~
} LIMIT 30@@"
city-uri city-uri city-uri city-uri city-uri))
(results (sparql-cache:dbpedia (replace-all query "@@" " "))))
(format message-stream "~%SPARQL to get CITY data for ~A:~%~%" city-uri)
(apply colorize-sparql-function (list query :stream message-stream))
(format message-stream "~%")
(clean-results results)))
;; (kgn-common:dbpedia-get-city-detail "<http://dbpedia.org/resource/London>")
Knowledge Graph Navigator Common Library Implementation
(defun dbpedia-get-product-detail (product-uri-raw
&key (message-stream t)
(colorize-sparql-function #'colorize-sparql-local))
(let* ((product-uri (check-uri product-uri-raw))
(query
(format
"SELECT DISTINCT ?label ?comment {
~A <http://www.w3.org/2000/01/rdf-schema#comment>
?comment . FILTER
(lang(?comme\
nt) = 'en') . @@ ~
OPTIONAL { ~A <http://www.w3.org/2000/01/rdf-schema#label> ?label . } ~
} LIMIT 30@@"
product-uri product-uri))
(results (sparql-cache:dbpedia (replace-all query "@@" " "))))
(format message-stream "~%SPARQL to get PRODUCT data for ~A:~%~%" product-uri)
(apply colorize-sparql-function (list query :stream message-stream))
(format message-stream "~%")
(clean-results results)))
;; (kgn-common:dbpedia-get-product-detail "<http://dbpedia.org/resource/Pepsi>")
(defun dbpedia-get-relationships (s-uri o-uri) ;;
&key (message-stream t))
(let* ((query
(format
"SELECT DISTINCT ?p { ~A ?p ~A . FILTER (!regex(str(?p), 'wikiPage', 'i'))} LIMIT 5"
(check-uri s-uri) (check-uri
o-uri)))
(results (sparql-cache:dbpedia query)))
(alexandria:flatten (map 'list
#'(lambda (x)
(format nil "~{<~A>~}" (cdar x)))
results))))
;; (kgn-common::dbpedia-get-relationships
"<http://dbpedia.org/resource/Bill_Gates>"
"<http://dbpedia.org/resource/Microsoft>")
(defun entities (text)
(let ((txt-obj (kbnlp:make-text-object text)))
(list (kbnlp::text-human-names txt-obj) (kbnlp::text-place-names txt-obj) (kbnlp\
::text-company-names txt-obj))))
Knowledge Graph Navigator Common Library Implementation
;; (kgn-common::entities "Bill Clinton went to Canada")
(defun entities-dbpedia (text)
(let ((e-hash (entity-uris:find-entities-in-text text)))
(gethash "people" e-hash)
(gethash "companies" e-hash)
(gethash "countries" e-hash)
(gethash "cities" e-hash))))
;; (kgn-common::entities-dbpedia "Bill Clinton went to Canada")
Wrap-up
This is a long example application for a book, split between this chapter and the next two chapters
offering different user interface implementations.
I got the idea for the KGN application because I was spending quite a bit of time manually setting up
SPARQL queries for DBPedia (and other public sources like WikiData) and I wanted to experiment
with partially automating this process. Now in the next two chapters we will write user interfaces
for this KGN common library.
Knowledge Graph Navigator
Text-Based User Interface
We developed the The Knowledge Graph Navigator (which I will often refer to as KGN) common
library in the last chapter. Here we write a simple console or text-based user interface for the library.
In later chapters we implement UIs using LispWorks CAPI, McCLIM, and Franz Common Graphics.
This Quicklisp library can be found in a separate GitHub repository https://github.com/mark-
watson/kgn-text-ui⁹⁴ and contains the files:
• kgn-text-ui.asd - specifies dependencies, including the KGN common library
• kgn-text-ui.lisp - Contains the complete user interface
• package.lisp - specifies dependencies, including the KGN common library
We start by looking at sample output using the text user interface and then look at the implementa-
Example Output
We will look at a very simple example query Bill Gates worked at Microsoft and his competitor
was IBM that only contains a few entities. In practice, I usually use queries with five to ten entities to
get more discovered relationships. I remove a lot of the generated output in the following listing for
brevity, especially the many generated SPARQL queries that the code generates and uses (comments
on the output appear after this listing):
$ sbcl
*(ql:quickload :kgn-text-ui)
; Loading "kgn-common"
; Loading "sqlite"
; Loading "cl-json"
; Loading "drakma"
* (kgn-text-ui:kgn-text-ui)
"Enter entity names (people, places, companies, etc.":
Bill Gates worked at Microsoft and his competitor was IBM
⁹⁴https://github.com/mark-watson/kgn-text-ui
Knowledge Graph Navigator Text-Based User Interface
- - - - Enter zero or more indices for your desired selections:
"William Henry Gates III (born October 28, 1955) is an American business magn\
ate, software developer, investor, author, and philanthropist. He is a co-founder of\
Microsoft, along with his late childhood friend Paul Allen. During his career at Mi\
crosoft, Gates held the positions of chairman, chief executive officer (CEO), presid\
ent and chief software architect, while also being the largest individual shareholde\
r until May 2014. He is considered one of the best known entrepreneurs of the microc\
omputer revolution of the 1970s and 1980s."
"Harry Roy Lewis (born 1947) is an American computer scientist, mathe 00ADma \
00ADti 00ADcian, and uni 00ADver 00ADsity admin 00ADi 00ADstra 00ADtor known for his\
research in com 00ADpu 00ADta 00ADtional logic, textbooks in theoretical computer s\
cience, and writings on computing, higher education, and technology. He is Gordon Mc\
Kay Professor of Computer Science at Harvard University, and was Dean of Harvard Col\
lege from 1995 to 2003. A new professorship in Engineering and Applied Sciences, end\
owed by a former student, will be named for Lewis and his wife upon their retirement\
"Cascade Investment, L.L.C. is an American holding company and private invest\
ment firm headquartered in Kirkland, Washington, United States. It is controlled by \
Bill Gates, and managed by Michael Larson. More than half of Gates' fortune is held \
in assets outside his holding of Microsoft shares. Cascade is the successor company \
to Dominion Income Management, the former investment vehicle for Gates' holdings, wh\
ich was managed by convicted felon Andrew Evans."
"Jerry P. Dyer (born May 3, 1959) is an American politician and former law en\
forcement officer. He is the 26th and current mayor of Fresno, California. Previousl\
y, he served as the chief of the Fresno Police Department."
- - - - Enter zero or more indices for your desired selections:
"Kenexa, an IBM Company, provides employment and retention services. This inc\
ludes recruitment process outsourcing onboarding tools, employee assessment, abiliti\
es assessment for employment candidates (Kenexa Prove It); and Kenexa Interview Buil\
der, a structured interview archive with example questions."
"Sequent Computer Systems was a computer company that designed and manufactur\
ed multiprocessing computer systems. They were among the pioneers in high-performanc\
e symmetric multiprocessing (SMP) open systems, innovating in both hardware (e.g., c\
Knowledge Graph Navigator Text-Based User Interface
ache management and interrupt handling) and software (e.g., read-copy-update). Vesti\
ges of Sequent's innovations live on in the form of data clustering software from Po\
lyServe (subsequently acquired by HP), various projects within OSDL, IBM contributio\
ns to the Linux kernel, and claims in the SCO v. IBM lawsuit."
"i2 Limited was the UK-based arm of software company i2 Group which produced \
visual intelligence and investigative analysis software for military intelligence, l\
aw enforcement and commercial agencies. After a number of acquisitions, in 2011 it b\
ecame part of IBM."
"The International Technology Alliance in Distributed Analytics and Informati\
on Sciences (DAIS-ITA) is a research program initiated by the UK Ministry of Defence\
(United Kingdom) (MOD) and the US Army Research Laboratory (ARL), in September 2016\
. It is led by IBM Research in the U.S. and IBM Hursley in the UK. DAIS ITA is the s\
econd International Technology Alliance started by the two countries, succeeding the\
previous ten year alliance NIS-ITA, which was of similar nature."
"The International Technology Alliance in Network and Information Sciences (N\
IS-ITA) was a research program initiated by the UK Ministry of Defence (United Kingd\
om) (MoD) and the US Army Research Laboratory (ARL), which was active for 10 years f\
rom May 2006 to May 2016. It was led by IBM Research in the U.S. and IBM Hursley in \
the UK. NIS ITA was the first International Technology Alliance started by the two c\
ountries."
"Applix Inc. was a computer software company founded in 1983 based in Westbor\
ough, Massachusetts that published Applix TM1, a multi-dimensional online analytical\
processing (MOLAP) database server, and related presentation tools, including Appli\
x Web and Applix Executive Viewer. Together, Applix TM1, Applix Web and Applix Execu\
tive Viewer were the three core components of the Applix Business Analytics Platform\
. (Executive Viewer was subsequently discontinued by IBM.)"
"Ounce Labs (an IBM company) is a Waltham, Massachusetts-based security softw\
are vendor. The company was founded in 2002 and created a software analysis product \
that analyzes source code to identify and remove security vulnerabilities. The secur\
ity software looks for a range of vulnerabilities that leave an application open to \
attack. Customers have included GMAC, Lockheed Martin, and the U.S. Navy. On July 28\
, 2009, Ounce was acquired by IBM, for an undisclosed sum, with the intention of int\
egrating it into IBM's Rational Software business."
"IBM Watson Health is a digital tool that helps clients facilitate medical re\
search, clinical research, and healthcare solutions, through the use of artificial i\
ntelligence, data, analytics, cloud computing, and other advanced information techno\
logy. It is a division of the International Business Machines Corporation, (IBM), an\
Knowledge Graph Navigator Text-Based User Interface
American multinational information technology company headquartered in Armonk, New \
York."
"International Business Machines Corporation (IBM) is an American multination\
al technology corporation headquartered in Armonk, New York, with operations in over\
171 countries. The company began in 1911, founded in Endicott, New York by trust bu\
sinessman Charles Ranlett Flint, as the Computing-Tabulating-Recording Company (CTR)\
and was renamed \"International Business Machines\" in 1924. IBM is incorporated in\
New York."
"Microsoft Corporation is an American multinational technology corporation wh\
ich produces computer software, consumer electronics, personal computers, and relate\
d services. Its best known software products are the Microsoft Windows line of opera\
ting systems, the Microsoft Office suite, and the Internet Explorer and Edge web bro\
wsers. Its flagship hardware products are the Xbox video game consoles and the Micro\
soft Surface lineup of touchscreen personal computers. Microsoft ranked No. 21 in th\
e 2020 Fortune 500 rankings of the largest United States corporations by total reven\
ue; it was the world's largest software maker by revenue as of 2016. It is considere\
d one of the Big Five companies in the U.S. information technology industry, along w\
ith Amazon, Google (Alphabet), Apple, and Facebook ("
"The CSS Working Group (Cascading Style Sheets Working Group) is a working g\
roup created by the World Wide Web Consortium (W3C) in 1997, to tackle issues that h\
ad not been addressed with CSS level 1. As of December 2019, the CSSWG had 142 membe\
rs. The working group is co-chaired by and ."
"The AMD Professional Gamers League (PGL), founded around 1997, was one of t\
he first professional computer gaming eSports leagues. The PGL was run by Total Ente\
rtainment Network and was sponsored by AMD. The first professional tournament they h\
eld was for StarCraft in September 1997. The league was official unveiled at a press\
conference at Candlestick Park in San Francisco on November 3, 1997. It was sponsor\
ed by Microsoft, Nvidia, and Levi Strauss & Co. The organization raised over $1.2mil\
USD in sponsorship money."
"Secure Islands Technologies Ltd. was an Israeli privately held technology c\
ompany headquartered in Beit Dagan which was subsequently acquired by Microsoft. The\
company develops and markets Information Protection and Control (IPC) solutions."
"Microsoft Innovation Centers (MICs) are local government organizations, uni\
versities, industry organizations, or software or hardware vendors who partner with \
Microsoft with a common goal to foster the growth of local software economies. These\
are state of the art technology facilities which are open to students, developers, \
IT professionals, entrepreneurs, startups and academic researchers. While each Cente\
Knowledge Graph Navigator Text-Based User Interface
r tunes its programs to local needs, they all provide similar content and services d\
esigned to accelerate technology advances and stimulate local software economies thr\
ough skills and professional training, industry partnerships and innovation. As of 1\
0 September 2010, there are 115 Microsoft Innovation Centers worldwide, most of whic\
h are open to the public. Recently it was reported th"
"Press Play ApS was a Danish video game development studio based in central \
Copenhagen in Denmark. Since 2006, Press Play have released five titles, including t\
he Max & the Magic Marker, Max: The Curse of Brotherhood and Kalimba. On November 10\
, 2016, Flashbulb acquired Press Play and its library of games to republish under th\
e Flashbulb name including Kalimba, Tentacles: Enter the Mind, and Max: The Curse of\
Brotherhood."
- - - ENTITY TYPE: people - - -
SPARQL to get PERSON data for <http://dbpedia.org/resource/Bill_Gates>:
"SELECT DISTINCT ?label ?comment@@ (GROUP_CONCAT (DISTINCT ?birthplace; SEPARATOR=' \
| ') AS ?birthplace) @@ (GROUP_CONCAT (DISTINCT ?almamater; SEPARATOR=' | ') AS ?alm\
amater) @@ (GROUP_CONCAT (DISTINCT ?spouse; SEPARATOR=' | ') AS ?spouse) { @@ <http:\
//dbpedia.org/resource/Bill_Gates> <http://www.w3.org/2000/01/rdf-schema#comment>
comment .@@
FILTER
(lang(?comment) = 'en') . @@ OPTIONAL { <http://d\
bpedia.org/resource/Bill_Gates> <http://dbpedia.org/ontology/birthPlace> ?birthplace\
} . @@ OPTIONAL { <http://dbpedia.org/resource/Bill_Gates> <http://dbpedia.org/onto\
logy/almaMater> ?almamater } . @@ OPTIONAL { <http://dbpedia.org/resource/Bill_Gates\
> <http://dbpedia.org/ontology/spouse> ?spouse } . @@ OPTIONAL { <http://dbpedia.org\
/resource/Bill_Gates>
<http://www.w3.org/2000/01/rdf-schema#label> ?label .@@ FILTE\
(lang(?label) = 'en') } @@ } LIMIT 10@@"
label: Bill Gates
comment: William Henry Gates III (born October 28, 1955) is an American business mag\
nate, software developer, investor, author, and philanthropist. He is a co-founder o\
f Microsoft, along with his late childhood friend Paul Allen. During his career at M\
icrosoft, Gates held the positions of chairman, chief executive officer (CEO), presi\
dent and chief software architect, while also being the largest individual sharehold\
er until May 2014. He is considered one of the best known entrepreneurs of the micro\
computer revolution of the 1970s and 1980s.
Knowledge Graph Navigator Text-Based User Interface
birthplace: http://dbpedia.org/resource/Seattle | http://dbpedia.org/resource/Washin\
gton_(state)
almamater:
spouse: http://dbpedia.org/resource/Melinda_French_Gates
- - - ENTITY TYPE: companies - - -
SPARQL to get COMPANY data for <http://dbpedia.org/resource/IBM>:
"SELECT DISTINCT ?label ?comment (GROUP_CONCAT (DISTINCT ?industry; SEPARATOR=' | ')\
AS ?industry)@@ (GROUP_CONCAT (DISTINCT ?netIncome; SEPARATOR=' | ') AS ?netIncome)\
@@ (GROUP_CONCAT (DISTINCT ?numberOfEmployees; SEPARATOR=' | ') AS ?numberOfEmployee\
s) {@@ <http://dbpedia.org/resource/IBM> <http://www.w3.org/2000/01/rdf-schema#comme\
?comment .@@
FILTER
(lang(?comment) = 'en') .@@ OPTIONAL { <http://db\
pedia.org/resource/IBM> <http://dbpedia.org/ontology/industry> ?industry } .@@
ONAL { <http://dbpedia.org/resource/IBM> <http://dbpedia.org/ontology/netIncome> ?ne\
tIncome } .@@
OPTIONAL { <http://dbpedia.org/resource/IBM> <http://dbpedia.org/onto\
logy/numberOfEmployees> ?numberOfEmployees } .@@
OPTIONAL { <http://dbpedia.org/res\
ource/IBM> <http://www.w3.org/2000/01/rdf-schema#label> ?label . FILTER (lang(?label\
) = 'en') } @@ } LIMIT 30@@"
label: IBM
comment: International Business Machines Corporation (IBM) is an American multinatio\
nal technology corporation headquartered in Armonk, New York, with operations in ove\
r 171 countries. The company began in 1911, founded in Endicott, New York by trust b\
usinessman Charles Ranlett Flint, as the Computing-Tabulating-Recording Company (CTR\
) and was renamed "International Business Machines" in 1924. IBM is incorporated in \
New York.
industry: http://dbpedia.org/resource/Artificial_intelligence | http://dbpedia.org/r\
esource/Automation | http://dbpedia.org/resource/Blockchain | http://dbpedia.org/res\
ource/Cloud_computing | http://dbpedia.org/resource/Computer_hardware | http://dbped\
ia.org/resource/Quantum_computing | http://dbpedia.org/resource/Robotics | http://db\
pedia.org/resource/Software
net-income: 5.59E9
Knowledge Graph Navigator Text-Based User Interface
number-of-employees: 345900
SPARQL to get COMPANY data for <http://dbpedia.org/resource/Microsoft>:
"SELECT DISTINCT ?label ?comment (GROUP_CONCAT (DISTINCT ?industry; SEPARATOR=' | ')\
AS ?industry)@@ (GROUP_CONCAT (DISTINCT ?netIncome; SEPARATOR=' | ') AS ?netIncome)\
@@ (GROUP_CONCAT (DISTINCT ?numberOfEmployees; SEPARATOR=' | ') AS ?numberOfEmployee\
s) {@@ <http://dbpedia.org/resource/Microsoft> <http://www.w3.org/2000/01/rdf-schema\
#comment>
?comment .@@
FILTER
(lang(?comment) = 'en') .@@ OPTIONAL { <http://db\
pedia.org/resource/Microsoft> <http://dbpedia.org/ontology/industry> ?industry } .@@\
OPTIONAL { <http://dbpedia.org/resource/Microsoft> <http://dbpedia.org/ontology/ne\
tIncome> ?netIncome } .@@
OPTIONAL { <http://dbpedia.org/resource/Microsoft> <http:\
//dbpedia.org/ontology/numberOfEmployees> ?numberOfEmployees } .@@
OPTIONAL { <http\
://dbpedia.org/resource/Microsoft> <http://www.w3.org/2000/01/rdf-schema#label> ?lab\
el . FILTER (lang(?label) = 'en') } @@ } LIMIT 30@@"
label: Microsoft
comment: Microsoft Corporation is an American multinational technology corporation w\
hich produces computer software, consumer electronics, personal computers, and relat\
ed services. Its best known software products are the Microsoft Windows line of oper\
ating systems, the Microsoft Office suite, and the Internet Explorer and Edge web br\
owsers. Its flagship hardware products are the Xbox video game consoles and the Micr\
osoft Surface lineup of touchscreen personal computers. Microsoft ranked No. 21 in t\
he 2020 Fortune 500 rankings of the largest United States corporations by total reve\
nue; it was the world's largest software maker by revenue as of 2016. It is consider\
ed one of the Big Five companies in the U.S. information technology industry, along \
with Amazon, Google (Alphabet), Apple, and Facebook (
industry: http://dbpedia.org/resource/Cloud_computing | http://dbpedia.org/resource/\
Computer_hardware | http://dbpedia.org/resource/Consumer_electronics | http://dbpedi\
a.org/resource/Corporate_venture_capital | http://dbpedia.org/resource/Internet | ht\
tp://dbpedia.org/resource/Social_networking_service | http://dbpedia.org/resource/So\
ftware_development | http://dbpedia.org/resource/Video_game_industry
net-income: 6.06E10
number-of-employees: 182268
DISCOVERED RELATIONSHIP LINKS:
Knowledge Graph Navigator Text-Based User Interface
<http://dbpedia.org/resource/Bill_Gates>
<http://dbpedia.org/ontology/knownFor>
<http://dbpedia.org/resource/Microsoft> .
<http://dbpedia.org/resource/Microsoft>
<http://dbpedia.org/ontology/foundedBy>
<http://dbpedia.org/resource/Bill_Gates> .
<http://dbpedia.org/resource/Microsoft>
<http://dbpedia.org/property/founders>
<http://dbpedia.org/resource/Bill_Gates> .
"Enter entity names (people, places, companies, etc.":
On line 10 I input a test phrase “Bill Gates worked at Microsoft and his competitor was IBM.” In lines
13-41 the test program prints out matching human entities from DBPedia that are indexed starting
at 0. On line 43 I entered 0 to choose just the first entity “William Henry Gates III”.
The prompt on line 45 asks the user to enter the indices for the company DBPedia entities they want
to use. These companies are listed in lines 47-152. On line 154 I entered “8 9” to select two entities
to use.
Lines 156-171 show the automatically generated SPARQL query to get information about Bill Gates.
This information is printed on lines 174-189. I list more generated SPARQL queries and results (which
we will not discuss further).
Lines 269-283 show discovered links found between the entities in the input text.
In the LispWorks CAPI user interface developed in the next chapter I use two text output stream
window panes, one for the generated SPARQL and one for the results.
Text User Interface Implementation
We will skip looking at the kgn-text-ui.asd and package.lisp files for this library but look at src/kgn-
text-ui/kgn-text-ui.lisp in its entirety. When entities are identified in input text we find candidate
DBPedia entity URIs that we present to the user. We precede each entire DBPedia description with
an index starting at 0. The user enters the indices for entities to further process. For example, in the
example listing in the previous section I entered “8 9” to indicate two company URIs.
Knowledge Graph Navigator Text-Based User Interface
(in-package #:kgn-text-ui)
(defun pprint-results (results)
(dolist (result (car results))
(terpri)
(format t
"~A:" (first result))
(format t " ~A~%" (second result))))
(defun multiple-selections (sel-list)
(if (not (null sel-list))
(let ()
(pprint sel-list)
(format t
"~%- - - - Enter zero or more indices for your desired selections:~%~%")
(let ((count 0))
(dolist (sel sel-list)
(format t "~A
~S ~%~%" count (cadr (assoc :comment (car sel))))
(setf count (1+ count))))
(let* ((line (read-line))
(indices
(if (> (length line) 0)
(mapcar
#'parse-integer
(myutils:tokenize-string line)))))
(print indices)
;(dolist (index indices)
(setf ret (cons (nth index str-list)
indices))))
;; (kgn-text-ui::multiple-selections
'("Option 1" "Option 2" "And yet another option 3"))
(defun prompt-selection-list (a-list-of-choices)
;; e.g., '((:people (("11" "data1")
("22" "data2"))) (:places (("p1" "data3"))))
(let (ret)
(dolist (choice a-list-of-choices)
(setf choice (remove-if #'null choice))
(let* ((topic-type (car choice))
(choice-list-full (rest choice))
(choice-list (remove-duplicates
(map 'list #'(lambda (z)
Knowledge Graph Navigator Text-Based User Interface
(string-shorten
(kgn-common:clean-comment
(kgn-common:clean-comment (cadr z)))
140 :first-remove-stop-words t)))
;; top level list flatten:
(apply #'append choice-list-full))
:test #'equal)))
(let (ret2
(dialog-results (multiple-selections choice-list)))
(dolist (index dialog-results)
(setf ret2 (cons (nth index choice-list) ret2)))
(if (> (length ret2) 0)
(setf ret (cons (list topic-type (reverse ret2)) ret))))))
(reverse ret)))
;; (kgn-text-ui::prompt-selection-list
'((:people (("11" "data1")
("22" "data2")))
(:places (("p1" "data3") ("p2" "data4") ("p3" "data5")))))
;; (kgn-text-ui::prompt-selection-list
(get-entity-data-helper "Bill Gates went to Seattle to Microsoft"))
(defun colorize-sparql (str &key (stream t))
" this could be used to colorize text (as it is in kgn-capi-ui example)"
;;(declare (ignore message-stream))
(declare (ignore stream))
(format t "~%~S~%" str))
(defun get-string-from-user (text-prompt)
(format t "~%~S:~%" text-prompt)
(read-line))
;; Main funtion
(defun kgn-text-ui ()
(let (prompt
(message-stream t)
(results-stream t))
Knowledge Graph Navigator Text-Based User Interface
(length
(setf prompt
(get-string-from-user
"Enter entity names (people, places, companies, etc.")))
(let* ((entity-data (get-entity-data-helper prompt :message-stream t)))
(let ((user-selections (prompt-selection-list entity-data)))
(dolist (ev user-selections)
(if (> (length (cadr ev)) 0)
(let ()
(terpri results-stream)
(format results-stream "- - - ENTITY TYPE: ~A - - -" (car ev))
;;(terpri results-stream)
(dolist (uri (cadr ev))
(setf uri (assoc :s (car uri)))
(case (car ev)
(:people
(pprint-results
(kgn-common:dbpedia-get-person-detail
:message-stream message-stream
:colorize-sparql-function #'colorize-sparql)))
(:companies
(pprint-results
(kgn-common:dbpedia-get-company-detail uri
:message-stream message-stream
:colorize-sparql-function #'colorize-sparql)))
(:countries
(pprint-results
(kgn-common:dbpedia-get-country-detail uri
:message-stream message-stream
:colorize-sparql-function #'colorize-sparql)))
(:cities
(pprint-results
(kgn-common:dbpedia-get-city-detail
:message-stream message-stream
:colorize-sparql-function #'colorize-sparql)))
(:products
(pprint-results
(kgn-common:dbpedia-get-product-detail uri
:message-stream message-stream
:colorize-sparql-function #'colorize-sparql)))))))))))))
Knowledge Graph Navigator Text-Based User Interface
;; (kgn-text-ui:kgn-text-ui)
The utility function multiple-selections listed in lines 10-29 displays a list of user choices, adding a
zero-based index for each list item. The user can enter zero or more indices to indicate their choices
using the function prompt-selection-list listed in lines 35-59.
The commented out code in lines 61-65 can be used to test these two functions.
The main function kgn-text-ui is listed in lines 80-129.
Wrap-up
In the previous chapter we implemented the Knowledge Graph Navigator library. Here we developed
a text-based user interface. In the next chapter we use the library to develop a LispWorks specific
CAPI user interface.
Knowledge Graph Navigator User
Interface Using LispWorks CAPI
As we have seen in the last two chapters the Knowledge Graph Navigator (which I will often refer
to as KGN) is a tool for processing a set of entity names and automatically exploring the public
Knowledge Graph DBPedia⁹⁵ using SPARQL queries. I started to write KGN for my own use, to
automate some things I used to do manually when exploring Knowledge Graphs, and later thought
that KGN might also be useful for educational purposes. KGN shows the user the auto-generated
SPARQL queries so hopefully the user will learn by seeing examples. KGN uses NLP code developed
in earlier chapters and we will reuse that code with a short review of using the APIs. Here is a
screenshot showing the application we develop here:
UI for the Knowledge Graph Navigator
We will use the KGN common library developed earlier. This example replaces the text bases UI
from the last chapter and requires either the free or professional version of LispWorks to run.
The code for the CAPI user interface is found in the GitHub repository https://github.com/mark-
watson/kgn-capi-ui⁹⁶.
⁹⁵http://dbpedia.org
⁹⁶https://github.com/mark-watson/kgn-capi-ui
Knowledge Graph Navigator User Interface Using LispWorks CAPI
Project Configuration and Running the Application
The following listing of kgn.asd shows the five packages this example depends on in addition to
#:kgn-common that was developed in an earlier chapter that is referenced in the file package.lisp:
;;;; knowledgegraphnavigator.asd
(asdf:defsystem #:kgn-capi-ui
:description "top level Knowledge Graph Navigator package"
:author "Mark Watson <markw@markwatson.com>"
:license "Apache 2"
:depends-on (#:kgn-common #:sparql #:kbnlp #:lw-grapher #:trivial-open-browser)
:components ((:file "package")
(:file "kgn-capi-ui")
(:file "option-pane")
(:file "colorize")
(:file "user-interface")))
Other dependency libraries specified in project.lisp are trivial-open-browser which we will use
to open a web browser to URIs for human readable information on DBPedia and sparql that was
developed in an earlier chapter.
Listing of package.lisp:
;;;; package.lisp
(defpackage #:kgn-capi-ui
(:use #:cl)
(:use #:kgn-common #:sparql #:lw-grapher #:trivial-open-browser)
(:export #:kgn-capi-ui))
The free personal edition of LispWorks does not support initialization files so you must manually
load Quicklisp from the Listener Window when you first start LispWorks Personal as seen in the
following repl listing (edited to remove some output for brevity). Once Quicklisp is loaded we then
use ql:quickload to load the example in this chapter (some output removed for brevity):
Knowledge Graph Navigator User Interface Using LispWorks CAPI
CL-USER 1 > (load "~/quicklisp/setup.lisp")
; Loading text file /Users/markw/quicklisp/setup.lisp
; Loading /Applications/LispWorks Personal 7.1/...
;; Creating system "COMM"
#P"/Users/markw/quicklisp/setup.lisp"
CL-USER 2 > (ql:quickload "kgn")
To load "kgn":
Load 1 ASDF system:
; Loading "kgn"
"Starting to load data...."
"....done loading data."
"#P\"/Users/markw/GITHUB/common-lisp/entity-uris/entity-uris.lisp\""
"current directory:"
"/Users/markw/GITHUB/common-lisp/entity-uris"
"Starting to load data...."
"....done loading data."
[package kgn]
To load "sqlite":
Load 1 ASDF system:
sqlite
; Loading "sqlite"
To load "cl-json":
Load 1 ASDF system:
cl-json
; Loading "cl-json"
To load "drakma":
Load 1 ASDF system:
drakma
; Loading "drakma"
.To load "entity-uris":
Load 1 ASDF system:
entity-uris
; Loading "entity-uris"
("kgn")
CL-USER 3 > (kgn:kgn)
#<KGN::KGN-INTERFACE "Knowledge Graph Navigator" 40201E91DB>
Please note that I assume you have configured all of the examples for this book for discoverability
by Quicklisp as per the section Setup for Local Quicklisp Projects in Appendix A.
When the KGN application starts a sample query is randomly chosen. Queries with many entities
Knowledge Graph Navigator User Interface Using LispWorks CAPI
can take a while to process, especially when you first start using this application. Every time KGN
makes a web service call to DBPedia the query and response are cached in a SQLite database in
∼/.kgn_cache.db which can greatly speed up the program, especially in development mode when
testing a set of queries. This caching also takes some load off of the public DBPedia endpoint, which
is a polite thing to do.
I use LispWorks Professional and add two utility functions to the bottom on my ∼/.lispworks
configuration file (you can’t do this with LispWorks Personal):
;;; The following lines added by ql:add-to-init-file:
#-quicklisp
(let ((quicklisp-init
(merge-pathnames
"quicklisp/setup.lisp"
(user-homedir-pathname))))
(when (probe-file quicklisp-init)
(load quicklisp-init)))
(defun ql (x) (ql:quickload x))
(defun qlp (x)
(ql:quickload x)
(SYSTEM::%IN-PACKAGE (string-upcase x) :NEW T))
Function ql is just a short alias to avoid frequently typing ql:quickload and qlp loads a Quicklisp
project and then performs an in-package of the Common Lisp package with the same name as the
Quicklisp project.
Utilities to Colorize SPARQL and Generated Output
When I first had the basic functionality of KGN with a CAPI UI working, I was disappointed by how
the application looked as all black text on a white background. Every editor and IDE I use colorizes
text in an appropriate way so I took advantage of the function capi::write-string-with-properties
to easily implement color hilting SPARQL queries.
The code in the following listing is in the file kgn/colorize.lisp. When I generate SPARQL queries
to show the user I use the characters “@@” as placeholders for end of lines in the generated output.
In line 5 I am ensuring that there are spaces around these characters so they get tokenized properly.
In the loop starting at line 7 I process the tokens checking each one to see if it should have a color
associated with it when it is written to the output stream.
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(in-package #:kgn)
(defun colorize-sparql (s
&key (stream nil))
(let ((tokens (tokenize-string-keep-uri
(replace-all s "@@" " @@ ")))
in-var)
(dolist (token tokens)
(if (> (length token) 0)
(if (or in-var (equal token "?"))
(capi::write-string-with-properties
'(:highlight :compiler-warning-highlight)
stream)
(if (find token '("where" "select" "distinct" "option" "filter"
"FILTER" "OPTION" "DISTINCT"
"SELECT" "WHERE")
:test #'equal)
(capi::write-string-with-properties
'(:highlight :compiler-note-highlight)
stream)
(if (equal (subseq token 0 1) "<")
(capi::write-string-with-properties
'(:highlight :bold)
stream)
(if (equal token "@@")
(terpri stream)
(if (not (equal token "~")) (write-string token stream)))))))
(if (equal token "?")
(setf in-var t)
(setf in-var nil))
(if (and
(not in-var)
(not (equal token "?")))
(write-string " " stream)))
(terpri stream)))
Here is an example call to function colorize-sparql:
Knowledge Graph Navigator User Interface Using LispWorks CAPI
KGN 25 > (colorize-sparql "select ?s ?p
where {@@
?s ?p \"Microsoft\" } @@
FILTER\
(lang(?comment) = 'en')")
select ?s ?p where {
?s ?p "Microsoft" }
FILTER ( lang ( ?comment ) = 'en' )
Main Implementation File kgn-capi-ui.lisp
;;----------------------------------------------------------------------------
;; To try it, compile and load this file and then execute:
(kgn::kgn)
;;----------------------------------------------------------------------------
;; Copyright (c) 2020-2022 Mark Watson. All rights reserved.
;;----------------------------------------------------------------------------
(in-package #:kgn-capi-ui)
(defvar *width* 1370)
(defvar *best-width* 1020)
(defvar *show-info-pane* t)
(defvar *pane2-message*
"In order to process your query a series of SPARQL queries will be formed based on\
the query. These generated SPARQL queries will be shown here and the reuslts of the\
queries will be formatted and displayed in the results display pane below.")
(defvar *pane3-message*
"Enter a query containing entities like people's names, companys, places, etc. fol\
lowing by the RETURN key to start processing your query. You can also directly use a\
DBPedia URI for an entity, for example: <http://dbpedia.org/resource/Apple_Inc.> Wh\
en you start this application, a sample query is randomly chosen to get you started.\
(defun test-callback-click (selected-node-name)
(ignore-errors
(format nil "* user clicked on node: ~A~%" selected-node-name)))
(defun test-callback-click-shift (selected-node-name)
(ignore-errors
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(if (equal (subseq selected-node-name 0 5) "<http")
(trivial-open-browser:open-browser
(subseq selected-node-name 1 (- (length selected-node-name) 1))))
(format nil "* user shift-clicked on node: ~A - OPEN WEB BROWSER~%" selected-nod\
e-name)))
(defun cache-callback (&rest x) (declare (ignore x))
(if *USE-CACHING*
(capi:display (make-instance 'options-panel-interface))))
(defun website-callback (&rest x) (declare (ignore x)) (trivial-open-browser:open-br\
owser "http://www.knowledgegraphnavigator.com/"))
(defun toggle-grapher-visibility (&rest x)
(declare (ignore x))
(setf *show-info-pane* (not *show-info-pane*)))
(defvar *examples*)
(setf *examples* '("Bill Gates and Melinda Gates at Microsoft in Seattle"
"Bill Clinton <http://dbpedia.org/resource/Georgia_(U.S._state)>"
"Bill Gates and Steve Jobs visited IBM and Microsoft in Berlin, S\
an Francisco, Toronto, Canada"
"Steve Jobs lived near San Francisco and was a founder of <http:/\
/dbpedia.org/resource/Apple_Inc.>"
"<http://dbpedia.org/resource/Bill_Gates> visited IBM"
"<http://dbpedia.org/resource/Bill_Gates> visited <http://dbpedia\
.org/resource/Apple_Inc.>"
"Bill Gates visited <http://dbpedia.org/resource/Apple_Inc.>"))
(capi:define-interface kgn-interface ()
(:menus
(action-menu
"Actions"
("Copy generated SPARQL to clipboard"
:callback
#'(lambda (&rest x) (declare (ignore x))
(let ((messages (capi:editor-pane-text text-pane2)))
(capi::set-clipboard text-pane2 (format nil "---- Generated SPARQL and c\
omments:~%~%~A~%~%" messages) nil))))
("Copy results to clipboard"
:callback
Knowledge Graph Navigator User Interface Using LispWorks CAPI
#'(lambda (&rest x) (declare (ignore x))
(let ((results (capi:editor-pane-text text-pane3)))
(capi::set-clipboard text-pane2 (format nil "---- Results:~%~%~A~%" resu\
lts) nil))))
("Copy generated SPARQL and results to clipboard"
:callback
#'(lambda (&rest x) (declare (ignore x))
(let ((messages (capi:editor-pane-text text-pane2))
(results (capi:editor-pane-text text-pane3)))
(capi::set-clipboard
text-pane2
(format nil "---- Generated SPARQL and comments:~%~%~A~%~%---- Results:\
~%~%~A~%" messages results) nil))))
("Visit Knowledge Graph Navigator Web Site" :callback 'website-callback)
("Clear query cache" :callback 'cache-callback)
((if *show-info-pane*
"Stop showing Grapher window for new results"
"Start showing Grapher window for new results")
:callback 'toggle-grapher-visibility)
(:menu-bar action-menu)
(:panes
(text-pane1
capi:text-input-pane
:text (nth (random (length *examples*)) *examples*)
:title "Query"
:min-height 80
:max-height 100
:max-width *width*
;;:min-width (- *width* 480)
:width *best-width*
:callback 'start-background-thread)
(text-pane2
capi:collector-pane
:font "Courier"
:min-height 210
:max-height 250
:title "Generated SPARQL queries to get results"
:text "Note: to answer queries, this app makes multipe SPARQL queries to DBPedia\
. These SPARQL queries will be shown here."
:vertical-scroll t
:create-callback #'(lambda (&rest x)
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(declare (ignore x))
(setf (capi:editor-pane-text text-pane2) *pane2-message*))
:max-width *width*
:width *best-width*
:horizontal-scroll t)
(text-pane3
capi:collector-pane ;; capi:display-pane ;; capi:text-input-pane
:text *pane3-message*
:font "Courier"
:line-wrap-marker nil
:wrap-style :split-on-space
:vertical-scroll :with-bar
:title "Results"
:horizontal-scroll t
:min-height 220
:width *best-width*
:create-callback #'(lambda (&rest x)
(declare (ignore x))
(setf (capi:editor-pane-text text-pane3) *pane3-message*))
:max-height 240
:max-width *width*)
capi:title-pane
:text "Use natural language queries to generate SPARQL"))
(:layouts
(main-layout
capi:grid-layout
'(nil info
nil text-pane1
nil text-pane2
nil text-pane3)
:x-ratios '(1 99)
:has-title-column-p t))
(:default-initargs
:layout 'main-layout
:title "Knowledge Graph Navigator"
:best-width *best-width*
:max-width *width*))
(defun start-background-thread (query-text self)
(format t "~%** ** entering start-progress-bar-test-from-background-thread:~%~%sel\
f=~S~%~%" self)
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(with-slots (text-pane2 text-pane3) self
(print text-pane2)
(mp:process-run-function "progress-bar-test-from-background-thread"
'run-and-monitor-progress-background-thread
query-text text-pane2 text-pane3)))
;; This function runs in a separate thread.
(defun run-and-monitor-progress-background-thread (text text-pane2 text-pane3)
(unwind-protect
(setf (capi:editor-pane-text text-pane2) "")
(setf (capi:editor-pane-text text-pane3) "")
;;(capi:display-message "done")
(let ((message-stream (capi:collector-pane-stream text-pane2))
(results-stream (capi:collector-pane-stream text-pane3)))
(format message-stream "# Starting to process query....~%")
(format results-stream *pane3-message*)
(let ((user-selections (prompt-selection-list (get-entity-data-helper text :me\
ssage-stream message-stream))))
(print "***** from prompt selection list:") (print user-selections)
(setf (capi:editor-pane-text text-pane3) "")
(dolist (ev user-selections)
(if (> (length (cadr ev)) 0)
(let ()
(terpri results-stream)
(capi::write-string-with-properties
(format nil "- - - ENTITY TYPE: ~A - - -" (car ev))
'(:highlight :compiler-error-highlight) results-stream)
;;(terpri results-stream)
(dolist (uri (cadr ev))
(setf uri (car uri))
(case (car ev)
(:people
(pprint-results
(kgn-common:dbpedia-get-person-detail
uri :message-stream mes\
sage-stream :colorize-sparql-function #'colorize-sparql)
:stream results-stream))
(:companies
(pprint-results
(kgn-common:dbpedia-get-company-detail uri :message-stream mes\
sage-stream :colorize-sparql-function #'colorize-sparql)
:stream results-stream))
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(:countries
(pprint-results
(kgn-common:dbpedia-get-country-detail uri :message-stream mes\
sage-stream :colorize-sparql-function #'colorize-sparql)
:stream results-stream))
(:cities
(pprint-results
(kgn-common:dbpedia-get-city-detail
uri :message-stream mes\
sage-stream :colorize-sparql-function #'colorize-sparql)
:stream results-stream))
(:products
(pprint-results
(kgn-common:dbpedia-get-product-detail uri :message-stream mes\
sage-stream :colorize-sparql-function #'colorize-sparql)
:stream results-stream)))))))
(let (links x)
(dolist (ev user-selections)
(dolist (uri (second ev))
(setf uri (car uri))
(if (> (length ev) 2)
(setf x (caddr ev)))
(setf links (cons (list (symbol-name (first ev)) uri x) links)))
(append
(entity-results->relationship-links
user-selections
:message-stream message-stream))))
*show-info-pane*
(lw-grapher:make-info-panel-grapher '("PEOPLE" "COMPANIES" "COUNTRIES"\
"CITIES" "PRODUCTS" "PLACES")
links 'test-callback-click 'test-c\
allback-click-shift))) ;; do
not use #' !!
(terpri results-stream)
(princ "** Done wih query **" results-stream)))))
Knowledge Graph Navigator User Interface Using LispWorks CAPI
;; MAIN entry point for application:
(defun kgn-capi-ui ()
;;(ignore-errors (create-dbpedia))
(capi:display (make-instance 'kgn-interface)))
User Interface Utilites File user-interface.lisp
In the previous chapter, the function prompt-selection-list was defined in the file kgn-text-ui/kgn-
text-ui.lisp for text based (console) UIs. Here it is implemented in a separate file user-interface.lisp
in the project directory kgn-capi-ui.
(in-package #:kgn-capi-ui)
;; (use-package "CAPI")
(defun prompt-selection-list (a-list-of-choices)
(let (ret)
(dolist (choice a-list-of-choices)
(setf choice (remove-if #'null choice))
(let* ((topic-type (car choice))
(choice-list-full (rest choice))
(choice-list (remove-duplicates
(map 'list #'(lambda (z)
z ;; (first z)
(string-shorten
(kgn-common:clean-comment
(kgn-common:clean-comment (cadr z)))
140 :first-remove-stop-words t)))
(apply #'append choice-list-full))
:test #'equal)))
(let ((dialog-results (alexandria:flatten
(capi:prompt-with-list ;; SHOW SELECTION LIST
(map 'list #'second choice-list)
(symbol-name topic-type)
:interaction :multiple-selection
:choice-class 'capi:button-panel
:pane-args '(:visible-min-width 910
:layout-class capi:column-layout))))
(ret2))
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(dolist (x choice-list)
(if (find (second x) dialog-results)
(setf ret2 (cons (car x) ret2))))
(if (> (length ret2) 0)
(setf ret (cons (list topic-type (reverse ret2)) ret))))))
(reverse ret)))
;; (get-entity-data-helper "Bill Gates went to Seattle to Microsoft")
;; (prompt-selection-list
(get-entity-data-helper
"Bill Gates went to Seattle to Microsoft"))
User Interface CAPI Options Panes Definition File
option-pane.lisp
In the following listing we define functions to implement CAPI menus:
(in-package #:kgn-capi-ui)
;; options for:
1. programming language to generate code snippets for
2. colorization options (do we really need this??)
3. show disk space used by caching
4. option to remove local disk cache
(defvar *width-options-panel* 800)
(defun get-cache-disk-space ()
(let ((x (ignore-errors
(floor
(with-open-file
(file "~/Downloads/knowledge_graph_navigator_cache.db")
(file-length file)) 1000)))))
0))) ;; units in megabytes
(defun clear-cache-callback (&rest val)
(declare (ignore val))
(ignore-errors (delete-file "~/Downloads/knowledge_graph_navigator_cache.db")))
(defvar *code-snippet-language* nil)
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(defun set-prog-lang (&rest val)
(format t "* set-prog-lang: val=~S~%" val)
(setf *code-snippet-language* (first val)))
(capi:define-interface options-panel-interface ()
(:panes
(prog-lang-pane
capi:option-pane
:items '("No language set" "Python" "Common Lisp")
:visible-items-count 6
:selection (if (equal *code-snippet-language* nil)
(if (equal *code-snippet-language* "No language set")
(if (equal *code-snippet-language* "Python")
(if (equal *code-snippet-language* "Common Lisp")
:interaction :single-selection
:selection-callback
'set-prog-lang)|#
(disk-space-pane
capi:text-input-pane
:text (format nil "~A (megabytes)"
(let ((x
(ignore-errors
(floor
(with-open-file (file "~/.kgn_cache.db")
(file-length file))
1000)))))
:title "Current size of cache:"
:min-width 170
:max-width *width-options-panel*)
(clear-disk-cache-pane
capi:push-button-panel
;;:title "Clear local query cache:"
Knowledge Graph Navigator User Interface Using LispWorks CAPI
:items
'("Clear local query cache")
:selection-callback
#'(lambda (&rest val)
(declare (ignore val))
(ignore-errors (delete-file "~/.kgn_cache.db"))
(ignore-errors (setf (capi:text-input-pane-text disk-space-pane)
"0 (megabytes)"))))
(toggle-graph-display
capi:option-pane
:items '("Show Graph Info Pane Browser" "Hide Graph Info Pane Browser")
:selected-item (if *show-info-pane* 0 1)
;;:title ""
:selection-callback 'toggle-grapher-visibility))
(:layouts
(main-layout
capi:grid-layout
'(nil disk-space-pane
nil clear-disk-cache-pane)
:x-ratios '(1 99)
:has-title-column-p nil))
(:default-initargs
:layout 'main-layout
:title "Knowledge Graph Navigator Options"
:max-width *width-options-panel*))
;; MAIN entry point for application:
(capi:display (make-instance 'options-panel-interface))
(defun ui2 () (capi:display (make-instance 'options-panel-interface)))
The popup list in the last example looks like:
Knowledge Graph Navigator User Interface Using LispWorks CAPI
Popup list shows the user possible entity resolutions for each entity found in the input query. The user selects the
resolved entities to use.
In this example there were two “Bill Gates” entities, one an early American frontiersman, the other
the founder of Microsoft and I chose the latter person to continue finding information about.
Using LispWorks CAPI UI Toolkit
You can use the free LispWorks Personal Edition for running KGN. Using other Common Lisp
implementations like Clozure-CL and SBCL will not work because the CAPI user interface library
is proprietary to LispWorks. I would like to direct you to three online resources for learning CAPI:
• [LispWorks’ main web page introducing CAPI⁹⁷
• LispWorks’ comprehensive CAPI documentation⁹⁸ for LispWorks version 7.1
• An older web site (last updated in 2011 but I find it useful for ideas): CAPI Cookbook⁹⁹
I am not going to spend too much time in this chapter explaining my CAPI-based code. If you
use LispWorks (either the free Personal or the Professional editions) you are likely to use CAPI
and spending time on the official documentation and especially the included example programs is
strongly recommended.
In the next section I will review the KGN specific application parts of the CAPI-based UI.
The following figure shows a popup window displaying a graph of discovered entities and
relationships:
⁹⁷http://www.lispworks.com/products/capi.html
⁹⁸http://www.lispworks.com/products/capi.html
⁹⁹http://capi.plasticki.com/show?O4
Knowledge Graph Navigator User Interface Using LispWorks CAPI
UI for info-pane-grapher
Since I just showed the info-pane-grapher this is a good time to digress to its implementation. This is
in a different package and you will find the source code in src/lw-grapher/info-pane-grapher.lisp.
I used the graph layout algorithm from ISI-Grapher Manual (by Gabriel Robbins)¹⁰⁰. There is another
utility in src/lw-grapher/lw-grapher.lisp that also displays a graph without mouse support and an
attached information pane that is not used here but you might prefer it for reuse in your projects if
you don’t need mouse interactions.
The graph nodes are derived from the class capi:pinboard-object:
(defclass text-node (capi:pinboard-object)
((text :initarg :text :reader text-node-text)
(string-x-offset :accessor text-node-string-x-offset)
(string-y-offset :accessor text-node-string-y-offset)))
I customized how my graph nodes are drawn in a graph pane (this is derived from LispWorks
example code):
¹⁰⁰http://www.cs.virginia.edu/~robins/papers/The_ISI_Grapher_Manual.pdf
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(defmethod capi:draw-pinboard-object (pinboard (self text-node)
&key &allow-other-keys)
(multiple-value-bind (X Y
width height)
(capi:static-layout-child-geometry self)
(let* ((half-width
(floor (1- width)
(half-height (floor (1- height) 2))
(circle-x (+ X half-width))
(circle-y (+ Y half-height))
(background :white)
(foreground (if background
:black
(capi:simple-pane-foreground pinboard)))
(text (text-node-text self)))
(gp:draw-ellipse pinboard
circle-x circle-y
half-width half-height
:filled t
:foreground background)
(gp:draw-ellipse pinboard
circle-x circle-y
half-width half-height
:foreground foreground)
(gp:draw-string pinboard
(+ X (text-node-string-x-offset self))
(+ Y (text-node-string-y-offset self))
:foreground foreground))))
Most of the work is done in the graph layout method that uses Gabriel Robbins’ algorithm. Here I
just show the signature and we won’t go into implementation. If you are interested in modifying
the layout code, I include a screen shot from ISI-Grapher manual showing the algorithm in a single
page; see the file src/lw-grapher/Algorithm from ISI-Grapher Manual.png.
The following code snippets show the method signature for the layout algorithm function in the file
src/lw-grapher/grapher.lisp. I also include the call to capi:graph-pane-nodes that is the CLOS
reader method for getting the list of node objects in a graph pane:
(defun graph-layout (self &key force)
(declare (ignore force))
(let* ((nodes (capi:graph-pane-nodes self))
The CAPI graph node model uses a function that is passed a node object and returns a list of this
Knowledge Graph Navigator User Interface Using LispWorks CAPI
node’s child node objects. There are several examples of this in the CAPI graph examples that are
included with LispWorks (see the CAPI documentation).
In src/lw-grapher/lw-grapher.lisp I wrote a function that builds a graph layout and instead of
passing in a “return children” function I found it more convenient to wrap this process, accepting a
list of graph nodes and graph edges as function arguments:
(in-package :lw-grapher)
;; A Grapher (using the layout algorithm from the ISI-Grapher
;; user guide) with an info panel
(defun make-info-panel-grapher (h-root-name-list h-edge-list
h-callback-function-click
h-callback-function-shift-click)
(let (edges roots last-selected-node node-callback-click
node-callback-click-shift output-pane)
(labels
((handle-mouse-click-on-pane (pane x y)
(ignore-errors
(let ((object (capi:pinboard-object-at-position pane x y)))
(if object
(let ()
(if last-selected-node
(capi:unhighlight-pinboard-object pane
last-selected-node))
(setf last-selected-node object)
(capi:highlight-pinboard-object pane object)
(let ((c-stream (collector-pane-stream output-pane)))
(format c-stream
(funcall node-callback-click
(text-node-full-text object)))
(terpri c-stream)))))))
(handle-mouse-click-shift-on-pane (pane x y)
(ignore-errors
(let ((object
(capi:pinboard-object-at-position pane x y)))
(if object
(let ()
(if last-selected-node
(capi:unhighlight-pinboard-object
pane last-selected-node))
(setf last-selected-node object)
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(capi:highlight-pinboard-object pane object)
(let ((c-stream
(collector-pane-stream output-pane)))
(format c-stream
(funcall node-callback-click-shift
(text-node-full-text object)))
(terpri c-stream)))))))
(info-panel-node-children-helper (node-text)
(let (ret)
(dolist (e edges)
(if (equal (first e) node-text)
(setf ret (cons (second e) ret))))
(reverse ret)))
(make-info-panel-grapher-helper
(root-name-list edge-list callback-function-click
callback-function-click-shift)
;; example: root-name-list: '("n1") edge-list:
'(("n1" "n2") ("n1" "n3"))
(setf edges edge-list
roots root-name-list
node-callback-click callback-function-click
node-callback-click-shift callback-function-click-shift)
(capi:contain
(make-instance
'column-layout
:title "Entity Browser"
:description
(make-instance 'capi:graph-pane
:min-height 330
:max-height 420
:roots roots
:layout-function 'graph-layout
:children-function #'info-panel-node-children-helper
:edge-pane-function
#'(lambda(self from to)
(declare (ignore self))
(let ((prop-name ""))
(dolist (edge edge-list)
(if (and
Knowledge Graph Navigator User Interface Using LispWorks CAPI
(equal from (first edge))
(equal to (second edge)))
(if (and (> (length edge) 2) (third edge))
(let ((last-index
(search
"/" (third edge)
:from-end t)))
(if last-index
(setf prop-name
(subseq (third edge)
(1+ last-index)))
(setf prop-name (third edge)))))))
(make-instance
'capi:labelled-arrow-pinboard-object
:data (format nil "~A" prop-name))))
:node-pinboard-class 'text-node
:input-model `(((:button-1 :release)
,#'(lambda (pane x y)
(handle-mouse-click-on-pane
pane x y)))
((:button-1 :release :shift) ;; :press)
,#'(lambda (pane x y)
(handle-mouse-click-shift-on-pane
pane x y))))
:node-pane-function 'make-text-node)
output-pane
(make-instance 'capi:collector-pane
:min-height 130
:max-height 220
:title "Message collection pane"
:text "..."
:vertical-scroll t
:horizontal-scroll t))))
:title
"Info Pane Browser: mouse click for info, mouse click + shift for web browser"
:best-width 550 :best-height 450)))
(make-info-panel-grapher-helper h-root-name-list
h-edge-list h-callback-function-click
h-callback-function-shift-click))))
Knowledge Graph Navigator User Interface Using LispWorks CAPI
Wrap-up
This is a long example application for a book so I did not discuss all of the code in the project. If you
enjoy running and experimenting with this example and want to modify it for your own projects
then I hope that I provided a sufficient road map for you to do so.
I got the idea for the KGN application because I was spending quite a bit of time manually setting up
SPARQL queries for DBPedia (and other public sources like WikiData) and I wanted to experiment
with partially automating this process. I wrote the CAPI user interface for fun since this example
application could have had similar functionality as a command line tool.
Using the OpenAI APIs
I have been working as an artificial intelligence practitioner since 1982 and the capability of the beta
OpenAI APIs is the most impressive thing that I have seen in my career so far. These APIs use the
GPT-3 model.
I recommend reading the online documentation for the online documentation for the APIs¹⁰¹ to see
all the capabilities of the beta OpenAI APIs. Let’s start by jumping into the example code. As seen in
the package.lisp file we use the UIOP and cl-json libraries and we export three top level functions:
;;;; package.lisp
(defpackage #:openai
(:use #:cl #:uiop #:cl-json)
(:export #:completions #:summarize #:answer-question))
The library that I wrote for this chapter supports three functions that are exported from the
package openai: for completing text, summarizing text, and answering general questions. The single
OpenAI model that the beta OpenAI APIs use is fairly general purpose and can generate cooking
directions when given an ingredient list, grammar correction, write an advertisement from a product
description, generate spreadsheet data from data descriptions in English text, etc.
Given the examples from https://beta.openai.com¹⁰² and the Common Lisp examples here, you
should be able to modify my example code to use any of the functionality that OpenAI documents.
;;;; openai.asd
(asdf:defsystem #:openai
:description "Library for using the beta OpenAI APIs"
:author "Mark Watson"
:license "Apache 2"
:depends-on (#:uiop #:cl-json)
:components ((:file "package")
(:file "openai")))
We will look closely at the function completions and then just look at the small differences to the
other two example functions. The definitions for all three exported functions are kept in the file
openai.lisp. You need to request an API key (I had to wait a few weeks to recieve my key) and set
the value of the environment variable OPENAI_KEY to your key. You can add a statement like:
¹⁰¹https://beta.openai.com/docs/introduction/key-concepts
¹⁰²https://beta.openai.com
Using the OpenAI APIs
export OPENAI_KEY=sa-hdffds7&dhdhsdgffd
to your .profile or other shell resource file.
While I sometimes use pure Common Lisp libraries to make HTTP requests, I prefer running the
curl utility as a separate process for these reasons:
• No problems with system specific dependencies.
• Use the standard library UIOP to run a shell command and capture the output as a string.
• I use curl from the command line when experimenting with web services. After I get working
curl options, it is very easy to translate this into Common Lisp code.
An example curl command line call to the beta OpenAI APIs is:
curl \
https://api.openai.com/v1/engines/davinci/completions \
-H "Content-Type: application/json"
-H "Authorization: Bearer sa-hdffds7&dhdhsdgffd" \
-d '{"prompt": "The President went to Congress", \
"max_tokens": 22}'
Here the API token “sa-hdffds7&dhdhsdgffd” on line 4 is made up - that is not my API token. All
of the OpenAI APIs expect JSON data with query parameters. To use the completion API, we set
values for prompt and max_tokens. The value of max_tokens is the requested number of returns
words or tokens. We will look at several examples later.
In the file openai.lisp we start with a helper function openai-helper that takes a string with the
OpenAI API call arguments encoded as a curl command, calls the service, and then extracts the
results from the returned JSON data:
(defun openai-helper (curl-command)
(let ((response
(uiop:run-program
curl-command
:output :string)))
(with-input-from-string
(s response)
(let* ((json-as-list (json:decode-json s)))
;; extract text (this might change if OpenAI changes JSON return format):
(cdar (cadr (nth 4 json-as-list)))))))
I convert JSON data to a Lisp list in line 8 and in line 10 I reach into the nested results list for
the generated text string. You might want to add a debug printout statement to see the value of
json-as-list.
Using the OpenAI APIs
The three example functions all use this openai-helper function. The first example function
completions sets the parameters to complete a text fragment. You have probably seen examples
of the OpenAI GPT-3 model writing stories, given a starting sentence. We are using the same model
and functionality here:
(defun completions (starter-text max-tokens)
(let* ((curl-command
(concatenate
'string
"curl " open-api-davinci-model-host
" -H \"Content-Type: application/json\""
" -H \"Authorization: Bearer " (uiop:getenv "OPENAI_KEY") "\" "
" -d '{\"prompt\": \"" starter-text "\", \"max_tokens\": "
(write-to-string max-tokens)
"}'")))
(openai-helper curl-command)))
Note that the OpenAI API models are stochastic. When generating output words (or tokens),
the model assigns probabilities to possible words to generate and samples a word using these
probabilities. As a simple example, suppose given prompt text “it fell and”, then the model could
only generate three words, with probabilities for each word based on this prompt text:
• the 0.9
• that 0.1
• a 0.1
The model would emit the word the 90% of the time, the word that 10% of the time, or the word
a 10% of the time. As a result, the model can generate different completion text for the same text
prompt. Let’s look at some examples. We request 22 output tokens (words or punctuation) in the
first two examples and 100 tokens in the third example:
cl-user> (openai:completions "The President went to Congress" 22)
" yesterday and proposed a single tax rate for all corporate taxpayers, which he env\
isions will be lower than what our"
cl-user> (openai:completions "The President went to Congress" 22)
" last month, asking for authorization of a program, which had previously been appro\
ved by the Foreign Intelligence Surveillance court as"
cl-user> (openai:completions "The President went to Congress" 100)
" worried about what the massive unpopular bill would do to his low approvals. Democ\
rats lost almost every situation to discuss any legislation about this controversial\
subject. Even more so, President Obama failed and had to watch himself be attacked \
Using the OpenAI APIs
by his own party for not leading.
There were also two celebrated (in DC) pieces of student loan legislation, which aim\
ed to make college cheaper. Harkin teamed up with Congressman Roddenbery on one, Stu\
dent Loan Affordability Act, and Senator Jack Reed (D"
cl-user>
The function summarize is very similar to the function completions except the JSON data passed
to the API has a few additional parameters that let the API know that we want a text summary:
• presence_penalty - penalize words found in the original text (we set this to zero)
• temperature - higher values the randomness used to select output tokens. If you set this to zero,
then the same prompt text will always yield the same results (I never use a zero value).
• top_p - also affects randomness. All examples I have seen use a value of 1.
• frequency_penalty - penalize using the same words repeatedly (I usually set this to zero, but
you should experiment with different values)
When summarizing text, try varying the number of generated tokens to get shorter or longer
summaries; in the following two examples we ask for 15 output tokens and 50 output tokens:
(defvar s "Jupiter is the fifth planet from the Sun and the largest in the Solar Sy\
stem. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-ha\
lf times that of all the other planets in the Solar System combined. Jupiter is one \
of the brightest objects visible to the naked eye in the night sky, and has been kno\
wn to ancient civilizations since before recorded history. It is named after the Rom\
an god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its ref\
lected light to cast visible shadows,[20] and is on average the third-brightest natu\
ral object in the night sky after the Moon and Venus.")
cl-user> (openai:summarize s 15)
"Jupiter is a gas giant because it is primarily composed of hydrogen"
cl-user> (openai:summarize s 50)
"Jupiter is a gas giant because it is predominantly composed of hydrogen and helium;\
it has a solid core that is composed of heavier elements. It is the largest of the \
four giant planets in the Solar System and the largest in the Solar System"
The function answer-question is very similar to the function summarize except the JSON data
passed to the API has one additional parameter that let the API know that we want a question
answered:
• stop - The OpenAI API examples use the value: [\n], which is what I use here.
Additionally, the model returns a series of answers with the string “nQ:” acting as a delimiter
between the answers.
Using the OpenAI APIs
(let ((index (search "nQ:" answer)))
(if index
(string-trim " " (subseq answer 0 index))
(string-trim " " answer)))
I strongly urge you to add a debug printout to the question answering code to print the full answer
before we check for the delimiter string. For some questions, the OpenAI APIs generate a series of
answers that increase in generality. In the example code we just take the most specific answer.
Let’s look at a few question answering examples and we will discuss possible problems and
workarounds:
cl-user> (openai:answer-question "Where is the Valley of Kings?" 60)
"It's in Egypt."
Let’s explore some issues with the question answering model. In the last example there is one good
answer and the model works well. The next example “What rivers are in Arizona?” shows some
problems because there are many rivers in Arizona. Sometimes the model misses a few rivers and
often river names are repeated in the output. You also don’t necessarily get the same answer for the
same input arguments. Here are three examples requesting 70, 90, and 160 output tokens:
cl-user> (openai:answer-question "What rivers are in Arizona?" 70)
"The Colorado River, the Gila River, the Little Colorado River, the Salt River, the \
Verde River, the San Pedro River, the Santa Cruz River, the San Juan River, the Agua\
Fria River, the Hassayampa River, the Bill Williams River, the Little Colorado Rive\
r, the San Francisco River, the San Pedro River"
cl-user> (openai:answer-question "What rivers are in Arizona?" 90)
"The Colorado River, the Gila River, the Little Colorado River, the Salt River, the \
Verde River, the San Pedro River, the Santa Cruz River, the San Juan River, the Agua\
Fria River, the Hassayampa River, the Bill Williams River, the Little Colorado Rive\
r, the San Francisco River, the San Pedro River, the Santa Cruz River, the San Juan \
River, the Agua Fria River, the Hass"
cl-user> (openai:answer-question "What rivers are in Arizona?" 160)
"Colorado, Gila, Salt, Verde, and the Little Colorado."
cl-user> (openai:answer-question "What rivers are in Arizona?" 160)
"The Colorado River, the Gila River, the Little Colorado River, the Salt River, the \
Verde River, the San Pedro River, the Santa Cruz River, the San Juan River, the Agua\
Fria River, the Hassayampa River, the Bill Williams River, the Little Colorado Rive\
r, the San Francisco River, the San Pedro River, the Santa Cruz River, the San Juan \
River, the Agua Fria River, the Hassayampa River, the Bill Williams River, the Littl\
e Colorado River, the San Francisco River, the San Pedro River, the Santa Cruz River\
Using the OpenAI APIs
, the San Juan River, the Agua Fria River, the Hassayampa River, the Bill Williams R\
iver, the Little Colorado River, the San Francisco River, the San Pedro River, the S\
anta Cruz"
My library does not handle embedded single quote characters in questions so the question “Who is
Bill Clinton’s wife?” will throw an error. Leaving out the single quote character works fine:
cl-user> (openai:answer-question "Who is Bill Clintons wife?" 120)
"Hillary Clinton."
cl-user>
In addition to reading the beta OpenAI API documentation you might want to read general material
on the use of OpenAI’s GPT-3 model. Since the APIs we are using are beta they may change. I will
update this chapter and the source code on GitHub if the APIs change.
Using the Hugging Face Deep
Learning Natural Language
Processing APIs
TBD: this chapter is currently being written and will be completed along with other manuscript
changes in Mark 2023.
Accessing the HuggingFace NLP APIs is similar to the code we used previously to access the OpenAI
(in-package #:huggingface)
;; define the environment variable "HF_API_TOKEN" with the value of your Hugging Fac\
e API key
(defun huggingface-helper (curl-command)
(let ((response
(uiop:run-program
curl-command
:output :string)))
(with-input-from-string
(s response)
(let* ((json-as-list (json:decode-json s)))
json-as-list))))
(defun summarize (some-text max-tokens)
(let* ((curl-command
(concatenate
'string
"curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
" -H \"Content-Type: application/json\""
" -H \"Authorization: Bearer " (uiop:getenv "HF_API_TOKEN") "\" "
" -d '{\"inputs\": \"" some-text "\", \"max_length\": "
(write-to-string max-tokens) " }'")))
(cdaar (huggingface-helper curl-command))))
(defun answer-question (question-text context-text)
(let* ((curl-command
Using the Hugging Face Deep Learning Natural Language Processing APIs
(concatenate
'string
"curl https://api-inference.huggingface.co/models/deepset/roberta-base-sq\
" -H \"Content-Type: application/json\""
" -H \"Authorization: Bearer " (uiop:getenv "HF_API_TOKEN") "\" "
" -d '{\"question\": \"" question-text "\", \"context\": \""
context-text "\" }'"))
(answer (huggingface-helper curl-command)))
(cdar (last answer))))
CL-USER>
(ql:quickload :huggingface)
To load "huggingface":
Load 1 ASDF system:
huggingface
; Loading "huggingface"
(:HUGGINGFACE)
CL-USER> (huggingface:summarize "Jupiter is the fifth planet from the Sun and the la\
rgest in the Solar System. It is a gas giant with a mass one-thousandth that of the \
Sun, but two-and-a-half times that of all the other planets in the Solar System comb\
ined. Jupiter is one of the brightest objects visible to the naked eye in the night \
sky, and has been known to ancient civilizations since before recorded history. It i\
s named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be brig\
ht enough for its reflected light to cast visible shadows,[20] and is on average the\
third-brightest natural object in the night sky after the Moon and Venus." 30)
"Jupiter is the fifth planet from the Sun and the largest in the Solar System. When \
viewed from Earth, Jupiter can be bright enough for its reflected light to cast visi\
ble shadows. It is on average the third-brightest natural object in the night sky af\
ter the Moon and Venus. It has been known to ancient civilizations since before reco\
rded history."
"Jupiter is the fifth planet from the Sun and the largest in the Solar System. When \
viewed from Earth, Jupiter can be bright enough for its reflected light to cast visi\
ble shadows. It is on average the third-brightest natural object in the night sky af\
ter the Moon and Venus. It has been known to ancient civilizations since before reco\
rded history."
CL-USER> (huggingface:answer-question "Where were the 1992 Olympics held?" "The 1992\
Summer Games were the first since the end of the Cold War, and the first unaffected\
by boycotts since the 1972 Summer Games. The 1992 Olympics were in Greece. 1992 was\
also the first year South Africa was re-invited to the Olympic Games by the Interna\
Using the Hugging Face Deep Learning Natural Language Processing APIs
tional Olympic Committee, after a 32-year ban from participating in international sp\
ort.")
"Greece"
CL-USER>
Using a Local Document Embeddings
Vector Database With OpenAI GPT3
APIs for Semantically Querying Your
Own Data
This project is inspired by the Python LangChain and LlamaIndex projects, with just the parts I
need for my projects written from scratch in Common Lisp. I wrote a Python book “LangChain and
LlamaIndex Projects Lab Book: Hooking Large Language Models Up to the Real World Using GPT-3,
ChatGPT, and Hugging Face Models in Applications” in March 2023: https://leanpub.com/langchain¹⁰³
that you might also be interested in.
The GitHub repository for this example can be found here: https://github.com/mark-watson/docs-
qa¹⁰⁴. This code also requires my OpenAI Common Lisp library https://github.com/mark-wat-
son/openai¹⁰⁵.
Overview of Local Embeddings Vector Database to
Enhance the Use of GPT3 APIs With Local Documents
In this example we will use the SqLite database to store the text from documents as well as
OpenAI embedding vectors for the text. Each embedding vector is 1536 floating point numbers.
Two documents are semantically similar if the dot product of their embedding vectors is large.
For long documents, we extract the text and create multiple chunks of text. Each chunk is stored
as a row in a SqLite database table. This is an easy way to implement a vector datastore. There are
many open source and commercial vector datastores if you reach performance limits with the simple
techniques we use here.
For each text chunk we call an OpenAI API to get an embedding vector. Later when we want to
have a GPT enabled conversation or just semantically query our local documents, we take the user’s
query and call an OpenAI API to get an embedding vector for the query text. We then compute
the vector dot product between the query embedding vector and each chunk embedding vector. We
save the text of the chunks that are semantically similar to the query embedding vector and use this
text as “context text” that we pass to an OpenAI Large Language Model (LLM) API along with the
user’s original query text.
¹⁰³https://leanpub.com/langchain
¹⁰⁴https://github.com/mark-watson/docs-qa
¹⁰⁵https://github.com/mark-watson/openai
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
What does this process really do? Normally when you query ChatGPT or similar LLMs, we are
querying against knowledge gained from all the original model training text. This process can lead
to so-called “model hallucinations” where the model “makes stuff up.” The advantage to the using the
Python libraries LangChain and LlamaIndex is that a LLM is effectively using all original training
data but is also primed with hopefully relevant context text from your local documents that might
be useful for answering the user’s query. We will replicate a small amount of this functionality in
Common Lisp.
At the end of this chapter we will extend our code for single queries with a conversational example.
Our approach to this is simple: when we pass context text and a query, we also pass previous
conversational queries from the user. I am still experimenting with the ideas in this chapter so please
do occasionally look for updates to the GitHub repository https://github.com/mark-watson/docs-
qa¹⁰⁶ and updates to this book.
Implementing a Local Vector Database for Document
Embeddings
In the following listing of the file docs-qa.lisp we start in lines 6-31 with a few string utility functions
we will need: write-floats-to-string, read-file, concat-strings, truncate-string, and break-into-
chunks.
The function break-into-chunks is a work in progress. For now we simply cut long input texts into
specific chunk lengths, often cutting words in half. A future improvement will be detecting sentence
boundaries and breaking text on sentences. The Python libraries LangChain and LlamaIndex have
multiple chunking strategies.
In lines 33-37 function decode-row takes data from a SQL query to fetch a database table row and
extracts the original chunk text and the embedding vector. Because of the overhead of making many
calls to the OpenAI APIs the time spent running the local Common Lisp example code is very small
so I have not yet worked on making my code efficient.
(ql:quickload :sqlite)
(use-package :sqlite)
;; define the environment variable "OPENAI_KEY" with the value of your OpenAI API key
(defun write-floats-to-string (lst)
(with-output-to-string (out)
(format out "( ")
(loop for i in lst
do (format out "~f " i))
(format out " )")))
¹⁰⁶https://github.com/mark-watson/docs-qa
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
(defun read-file (infile) ;; from Bing+ChatGPT
(with-open-file (instream infile
:direction :input
:if-does-not-exist nil)
(when instream
(let ((string (make-string (file-length instream))))
(read-sequence string instream)
string))))
(defun concat-strings (list)
(apply #'concatenate 'string list))
(defun truncate-string (string length)
(subseq string 0 (min length (length string))))
(defun break-into-chunks (text chunk-size)
"Breaks TEXT into chunks of size CHUNK-SIZE."
(loop for start from 0 below (length text) by chunk-size
collect (subseq text start (min (+ start chunk-size) (length text)))))
(defun decode-row (row)
(let ((id (nth 0 row))
(context (nth 1 row))
(embedding (read-from-string (nth 2 row))))
(list id context embedding)))
The next listing shows of parts of docs-qa.lisp that contain code to use SqLite. I wrapped the calls
to initialize the database inside of handler-case for convenience during development (file reloads
don’t throw top level errors and the existing database is untouched).
(defvar *db* (connect ":memory:"))
;;(defvar *db* (connect "test.db"))
(pprint *db*)
(handler-case
(progn
(execute-non-query
"CREATE TABLE documents (document_path TEXT, content TEXT, embedding TEXT);")
(execute-non-query
"CREATE INDEX idx_documents_id ON documents (document_path);")
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
(execute-non-query
"CREATE INDEX idx_documents_content ON documents (content);")
(execute-non-query
"CREATE INDEX idx_documents_embedding ON documents (embedding);"))
(error (c)
(print "Database and indices is already created")))
(defun insert-document (document_path content embedding)
;;(format t "insert-document:~% content:~A~%
embedding: ~A~%" content embedding)
(format t "~%insert-document:~%
content:~A~%~%" content)
(execute-non-query
"INSERT INTO documents (document_path, content, embedding) VALUES (?, ?, ?);"
document_path content (write-floats-to-string embedding)))
(defun get-document-by-document_path (document_path)
(mapcar #'decode-row
(execute-to-list *db*
"SELECT * FROM documents WHERE document_path = ?;"
document_path)))
(defun get-document-by-content (content)
(mapcar #'decode-row
(execute-to-list *db*
"SELECT * FROM documents WHERE content LIKE ?;" content)))
(defun get-document-by-embedding (embedding)
(mapcar #'decode-row
(execute-to-list *db*
"SELECT * FROM documents WHERE embedding LIKE ?;" embedding)))
(defun all-documents ()
(mapcar #'decode-row
(execute-to-list *db* "SELECT * FROM documents;")))
(defun create-document (fpath)
(let ((contents (break-into-chunks (read-file fpath) 200)))
(dolist (content contents)
(handler-case
(let ((embedding (openai::embeddings content)))
(insert-document fpath content embedding))
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
(error (c)
(format t "Error: ~&~a~%" c))))))
Using Local Embeddings Vector Database With OpenAI
GPT APIs
The next listing showing of parts of docs-qa.lisp interfaces with the OpenAI APIs:
(defun qa (question)
(let ((answer (openai:answer-question question 60)))
(format t "~&~a~%" answer)))
(defun semantic-match (query custom-context &optional (cutoff 0.7))
(let ((emb (openai::embeddings query))
(ret))
(dolist (doc (all-documents))
(let ((context (nth 1 doc)) ;; ignore fpath for now
(embedding (nth 2 doc)))
(let ((score (openai::dot-product emb embedding)))
(when (> score cutoff)
(push context ret)))))
(format t "~%semantic-search: ret=~A~%" ret)
(let* ((context (join-strings " . " (reverse ret)))
(query-with-context
(join-strings
(list context custom-context
"Question:" query))))
(openai:answer-question query-with-context 40))))
(defun QA (query &optional (quiet nil))
(let ((answer (semantic-match query "")))
(if (not quiet)
(format t "~%~%** query: ~A~%** answer: ~A~%~%" query answer))
answer))
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
Testing Local Embeddings Vector Database With
OpenAI GPT APIs
In the next part of the listing of docs-qa.lisp we write a test function to create two documents. The
two calls to create-document actually save text and embeddings for about 20 text chunks in the
database.
(defun test()
"Test code for Semantic Document Search Using
OpenAI GPT APIs and local vector database"
(create-document "data/sports.txt")
(create-document "data/chemistry.txt")
(QA "What is the history of the science of chemistry?")
(QA "What are the advantages of engainging in sports?"))
The output is (with a lot of debug printout not shown):
$ sbcl
* (quicklisp:quickload :docs-qa)
To load "docs-qa":
Load 1 ASDF system:
docs-qa
; Loading "docs-qa"
..................................................
[package docs-qa]To load "sqlite":
Load 1 ASDF system:
sqlite
; Loading "sqlite"
#<sqlite-handle {7005CA3783}>
(:docs-qa)
* (in-package :docs-qa)
#<package "DOCS-QA">
* (test)
** query: What is the history of the science of chemistry?
** answer: The history of chemistry as a science began in the 6th century BC, when t\
he Greek philosopher Leucippus and his student Democritus posited the existence of a\
n endless number of worlds
** query: What are the advantages of engainging in sports?
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
** answer: The advantages of engaging in sports are:n1. It helps to develop the body\
and mind.n2. It helps to develop the character.n3. It helps to develop the personal\
Adding Chat History
In the last part of the listing of docs-qa.lisp we experiment with supporting a conversation/chat of
multiple semantic queries against our local documents.
(defun CHAT ()
(let ((messages '(""))
(responses '("")))
(format t "~%Enter chat (STOP or empty line to stop) >> ")
(let ((string (read-line))
response)
(cond ((or (string= string "STOP") (< (length string) 1)) (return))
(t (let (prompt
custom-context)
(setf custom-context
(concatenate
'string
"PREVIOUS CHAT: "
(join-strings
(reverse messages))))
(push string messages)
(print messages) ;; (print responses)
(print prompt)
(setf response (semantic-match string custom-context))
(push response responses)
(format t "~%Response: ~A~%" response))))))
(list (reverse messages) (reverse responses))))
The output (with lots of debug printouts removed) looks like:
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
$ sbcl
* (quicklisp:quickload :docs-qa)
To load "docs-qa":
Load 1 ASDF system:
docs-qa
; Loading "docs-qa"
..................................................
[package docs-qa].To load "sqlite":
Load 1 ASDF system:
sqlite
; Loading "sqlite"
#<sqlite-handle {7005D9B9D3}>
* (in-package :docs-qa)
#<package "DOCS-QA">
* (create-document "data/chemistry.txt")
insert-document:
content:Amyl alcohol is an organic compound with the formula C 5 H 12 O. All eight\
isomers of amyl alcohol are known. The most important is isobutyl carbinol, this be\
ing the chief constituent of fermentation
;; output from all other document chunks is not shown
* (CHAT)
Enter chat (STOP or empty line to stop) >> what is the history of chemistry?
Response: Chemistry is the science of matter, its composition, structure and its pro\
perties. Chemistry is concerned with atoms and their interactions with other atoms, \
and thus is central to all other sciences. Chemistry is also concerned
Enter chat (STOP or empty line to stop) >> what is the boiling temperature?
Response: The boiling temperature of a liquid is the temperature at which the vapor \
pressure of the liquid equals the pressure surrounding the liquid, and the liquid ch\
anges into a vapor. At the boiling temperature, bubbles of vapor
Enter chat (STOP or empty line to stop) >>
Using a Local Document Embeddings Vector Database With OpenAI GPT3 APIs for Semantically Querying Your Own Data
Wrap Up for Using Local Embeddings Vector Database
to Enhance the Use of GPT3 APIs With Local
Documents
As I write this in early April 2023, I have been working almost exclusively with OpenAI APIs for the
last year and using the Python libraries for LangChain and LlamaIndex for the last three months.
I prefer using Common Lisp over Python when I can, so I am implementing a tiny subset of
the LangChain and LlamaIndex libraries in Common Lisp for my own use. By writing about
my Common Lisp experiments here I hope that I get pull requests for https://github.com/mark-
watson/docs-qa¹⁰⁷ from readers who are interested in helping to extend the Common Lisp library.
¹⁰⁷https://github.com/mark-watson/docs-qa
Prompt Engineering for Large
Language Models
We have been using prompt engineering in an informal way in the last few chapters. In this chapter
we look at prompt engineering in more detail. My hope is that, dear reader, you find this chapter to
be both a short reference for Large Language Models (LLMS) and a useful source of prompt examples
at the end of the chapter.
Two Types of LLMS
In order to avoid “model hallucinations” (i.e., models making up answers with no basis in fact) when
using Large Language Models (LLMS) it is important to explain in detail instructions and add context
text to help the model better answer questions of solve problems.
Base LLMs and Instruction Tuned LLMs are both types of advanced AI language models. While
they share some similarities in their underlying architecture and training processes, they have
distinct differences in terms of their fine-tuning and usage. Here, we will discuss the similarities
and differences in detail.
The similarities are:
• Architecture: Both models are based on the Transformer architecture, which has been shown
to be highly effective for various natural language processing tasks. This architecture relies on
self-attention mechanisms to process input data and generate outputs.
• Pretraining: Both models undergo a pretraining phase where they learn from a large corpus
of text, such as websites, books, and articles. This helps the models acquire a general
understanding of language, grammar, facts, and some reasoning abilities.
• Transfer learning: Both models can use transfer learning, which involves fine-tuning the
pretrained models on specific tasks or datasets. This process adapts the model’s general
knowledge to the target application, improving its performance on the desired task.
The differences are:
• Fine-tuning objectives: The key difference between the two models lies in their fine-tuning
process. Base Large Language Models are fine-tuned to generate human-like text with minimal
explicit guidance, whereas Instruction Tuned Large Language Models are specifically optimized
to follow instructions provided in the input prompt.
Prompt Engineering for Large Language Models
• Task performance: Due to their different fine-tuning objectives, Instruction Tuned Large
Language Models tend to perform better on tasks that require following explicit instructions,
while Base Large Language Models may produce more diverse and creative responses that may
not always adhere to the given instructions.
• Usage: Base Large Language Models can be used for a wide variety of tasks, including
text generation, summarization, translation, and question-answering. Instruction Tuned Large
Language Models, on the other hand, are designed to be more controllable and responsive
to input instructions, making them more suitable for applications that require precise output
based on specific instructions.
• Prompt engineering: To get the desired output from a Base Large Language Model, users often
need to carefully craft prompts, which can be a trial-and-error process. With Instruction Tuned
Large Language Models, users can provide more explicit instructions to achieve the desired
output more consistently.
In summary, both Base and Instruction Tuned Large Language Models share the same foundation
in terms of architecture and pretraining but differ in their fine-tuning objectives and application
strengths. While Base Large Language Models generate more diverse and creative text, Instruction
Tuned Large Language Models excel at tasks that require following specific instructions and are
more controllable.
Prompt Examples
We have used both OpenAI and Hugging Face Models so far in this book. When you practice prompt
engineering and learn new techniques, your knowledge of prompt engineering can be applied to the
OpenAI and Hugging Face APIs and models.
As Andrew Ng and Isa Fulford point out in their one hour class ChatGPT Prompt Engineering
for Developers¹⁰⁸, you can craft prompts as if you were instructing an intelligent person who is
not necessarily familiar with the current problem you are trying to solve with prompt engineering.
There is a bit of a tradeoff here: you pay per input and output tokens with the OpenAI APIs so you
may be tempted to compress your context text, instructions, and questions when creating prompts.
However, in general to more context and information you can provide increases the quality of the
resulting output.
The following sections show examples of prompt engineering that I use in my own work. In order
to learn more techniques I recommend also performing web search for prompt engineering with a
description of the problem you are working on. As an example, I asked Microsoft Bing+ChatGPT in
a chat to help me with I need help with prompt engineering using the OpenAI APIs. My problem
is: I have a text file that contains customer names and phone numbers. I need a prompt to list
names and phone numbers. and the advice was a useful starting point. In a similar way I received
useful advice when chatting with Google Bard.
¹⁰⁸https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson
Prompt Engineering for Large Language Models
Find Entities and Relationships
Prompt:
The task is to extract as many relevant entities and the relationships between these\
entities.\n
When possible, return the type of an entity using the Wikipedia class system.\n
Extract all relevant relationships between identified entities.\n
The relationships should follow the Wikipedia schema type.
The output of a relationship should be in a form of a triple Head, Relationship, Tai\
l, for example
Bill Gates, BORN_AT, Seatle.\n
As an example "Bill Gates worked at Microsoft" should have an output with the follow\
ing format:\n
entity
Bill Gates, person
Microsoft, organization
relationships
Bill Gates, WORKET AT, Microsoft\n
Task: process this text:
Steve Jobes founded Apple Computer and Bill Gates started Microsoft. Seattle is in t\
he state of Washington.
Steve lived in Palo Alto. Palo Alto is located in California.
ChatGPT 4 returns:
entities
Steve Jobs, person
Apple Computer, organization
Bill Gates, person
Microsoft, organization
Seattle, city
Washington, administrative territorial entity
relationships
Steve Jobs, FOUNDED, Apple Computer
Bill Gates, FOUNDED, Microsoft
Seattle, IS_IN, Washington
Disambiguate Relationship Names
Prompt:
Prompt Engineering for Large Language Models
Given a list of relationships like:
brother
located in
sister
sibling
found at
father
location
mother
relative
Then return a list with an index for each unique relationship, like:
brother, 1
located in, 2
sister, 1
sibling, 1
found at, 2
father, 1
location, 2
mother, 1
relative, 1
Task: process the following text:
Sam has a brother Mike. Suzy has a sister Sally. Suszy ahs a mother Jill, and a fath\
er Bill.
Mike is in San Diego. Sam is located is Oakland. Suzy's family all live in Los Angel\
ChatGPT 4 results are:
Based on the provided list of relationships, here is the processed text with an inde\
x for each unique relationship:
Sam has a brother Mike. (brother, 1)
Suzy has a sister Sally. (sister, 1)
Suzy has a mother Jill, and a father Bill. (mother, 1; father, 1)
Mike is in San Diego. (located in, 2)
Sam is located in Oakland. (located in, 2)
Suzy's family all live in Los Angeles. (location, 2)
Prompt Engineering for Large Language Models
Here, relationships like "brother," "sister," "mother," and "father" are assigned an\
index of 1, while relationships like "located in" and "location" are assigned an in\
dex of 2.
Disambiguate Relationship Names and Rewrite Text
Prompt:
We want to disambiguate relationship names and then rewrite text using canonical rel\
ationship names.
Given a list of relationships like:
brother
located in
sister
sibling
found at
father
location
mother
relative
Then generate a list with an index for each unique relationship, like:
brother, 1
located in, 2
sister, 1
sibling, 1
found at, 2
father, 1
location, 2
mother, 1
relative, 1
And then rewrite the original text with the relation indices.
Task: process the following text:
Sam has a brother Mike. Suzy has a sister Sally. Suszy ahs a mother Jill, and a fath\
er Bill.
Mike is in San Diego. Sam is located is Oakland. Suzy's family all live in Los Angel\
Prompt Engineering for Large Language Models
The ChatGPT 4 output is:
First, we create the indexed list of unique relationships:
brother, 1
located in, 2
sister, 1
sibling, 1
found at, 2
father, 1
location, 2
mother, 1
relative, 1
Now, we rewrite the original text using the relation indices:
Sam has a 1 Mike. Suzy has a 1 Sally. Suszy has a 1 Jill, and a 1 Bill.
Mike is 2 San Diego. Sam is 2 Oakland. Suzy's family all 2 Los Angeles.
In the first sentence Sam has a 1 Mike the number 1 refers to brother.
Disambiguate Relationship Names and Rewrite Text, Only Output
Rewritten Text
Here we output just results in JSON format.
Prompt:
We want to disambiguate relationship names and then rewrite text using canonical rel\
ationship names.
Given a list of relationships like:
brother
located in
sister
sibling
found at
father
location
mother
relative
Prompt Engineering for Large Language Models
Then generate a list with an index for each unique relationship, like:
brother, <1>
located in, <2>
sister, <1>
sibling, <1>
found at, <2>
father, <1>
location, <2>
mother, <1>
relative, <1>
And then rewrite the original text using the relation indices, and list relation nam\
es with indices. Only output rewritten text and names with indices.
Task: process the following text outputing JSON format:
Sam has a brother Mike. Suzy has a sister Sally. Suzy has a mother Jill, and a fathe\
r Bill.
Mike is in San Diego. Sam is located is Oakland. Suzy's family all live in Los Angel\
The ChatGPT 4 output is:
"relationships": {
"brother": "<1>",
"located in": "<2>",
"sister": "<1>",
"sibling": "<1>",
"found at": "<2>",
"father": "<1>",
"location": "<2>",
"mother": "<1>",
"relative": "<1>"
"rewritten_text": "Sam has a brother Mike. Suzy has a sister Sally. Suzy has a mot\
her Jill, and a father Bill.\nMike is <2> San Diego. Sam is <2> Oakland. Suzy's fami\
ly all live <2> Los Angeles."
Prompt Engineering for Large Language Models
Topic Modeling for Document Classification
In this example we show a complete template that includes document text. To use this in your
programs, replace the document text with a token string that you can replace with the text for the
document you are topic modeling.
Prompt:
Topic modeling involves reading text and assigning a topic name (also known as a doc\
ument classification).
The topics that you know how to classify are:
sports
health
chemistry
economy
politics
The output format will be:
Known topics: <list of all topics>
Topic: <topic of text>
Task: what is the best topic name for the following text:
John and Sam went to the playing field yesterday. Sam sprained his ankle.
The result is:
Known topics: sports, health, chemistry, economy, politics
Topic: sports
In this prompt template, I used placeholder document text John and Sam went to the playing field
yesterday. Sam sprained his ankle. that you would replace with the longer text from the document
you want to determine the topic or classification of.
In this template, I like to ask the LLM to repeat the list of topics (or classifications) it knows.
Prompt Engineering Wrapup
When you experiment with the examples in this chapter you can both use the ChatGPT web interface
to modify and develop prompts and you can also revisit the chapter on OpenAI APIs and experiment
Prompt Engineering for Large Language Models
with prompts programmatically. My personal preference is to perform initial experiments with the
ChatGPT web interface but to do most of the iterative prompt development in Emacs with a SLIME
I mentioned earlier Andrew Ng’s and Isa Fulford’s one hour prompt engineering class. The devote
time and provide good advice on interactive developement. Their examples use the Python language.
Using Common Lisp with
Wolfram/One
If you use Wolfram/One¹⁰⁹ then the material in this short chapter may interest you. The interface
that I wrote is simple: I use uiop:run-program to spawn a new process to run the Wolfram Language
command line tool that writes results to a temporary file. I then use uiop:read-file-string to read
the results and parse them into a convenient form for use.
Before we build and use an interface to Wolfram/One, let’s look at two screen shots of the
Wolfram/One interface with examples that we will run later in Common Lisp. The first example
finds entities in text:
Using Wolfram/One to find entities in text
The second example uses a deep learning model to answer a question given text containing the
answer to the question:
Using Wolfram/One to answer natural language questions
Here is the package.lisp file for this example:
¹⁰⁹https://www.wolfram.com/wolfram-one/
Using Common Lisp with Wolfram/One
(defpackage #:wolfram
(:use #:cl #:uiop)
(:export #:wolfram #:cleanup-lists
#:find-answer-in-text #:entities))
And the wolfram.asd file:
(asdf:defsystem #:wolfram
:description "Wolfram Language interface experiments"
:author "Mark Watson <markw@markwatson.com>"
:license "Apache 2"
:depends-on (#:uiop #:cl-json #:myutils)
:components ((:file "package")
(:file "wolfram")))
The implementation in Wolfram.lisp is simple enough. In lines 6-8 I create a Common Lisp path
object in /tmp (and absolute pathname is required) and then use file-namestring to get just the file
name as a string. In lines 8-10 we are creating an operating system shell and running the Wolfram
Language command line tool with arguments to execute the query and write the results to the
temporary file. In lines 11-15 we read the contents of the temporary file, delete the file, and decode
the returned string as JSON data.
The Data returned form calling the Wolfram Language command line tool contains excess structure
that we don’t need (a sample of the raw returned data is shown later) so the function cleanup-lists
shown in lines 17-19 discards heads of lists when the first value in a list or sublist is Rule or List. The
function recursive-remove seen in lines 20-24 will remove all occurrences of an item from a nested
(in-package #:wolfram)
;; General query utilities
(defun wolfram (statement)
(let ((temp-file-path
(file-namestring (uiop:tmpize-pathname "/tmp/wolfram"))))
(uiop:run-program (concatenate 'string "wolframscript -code 'Export[\""
temp-file-path "\"," statement
",\"ExpressionJSON\"]'"))
(let* ((ret (uiop:read-file-string temp-file-path)))
(delete-file temp-file-path)
(with-input-from-string (s (myutils:replace-all
(myutils:replace-all ret "\"'" "\"") "'\"" "\""))
(json:decode-json s)))))
Using Common Lisp with Wolfram/One
(defun cleanup-lists (r)
(cdr (recursive-remove "Rule" (recursive-remove "List" r))))
(defun recursive-remove (item tree)
(if (atom tree)
(mapcar (lambda (nested-list) (recursive-remove item nested-list))
(remove item tree :test #'equal))))
;; Higher level utilities for specific types of queries
(defun entities (text)
(let* ((noquotes (myutils:replace-all (myutils:replace-all text "\"" " ") "'" " "))
(query2
(concatenate
'string "TextCases['" noquotes
{'City', 'Country', 'Date', 'Person'} ->"
" {'String', 'Interpretation', 'Probability'}]"))
(query (myutils:replace-all query2 "'" "\"")))
(remove-if #'(lambda (a) (null (cadr a)))
(cleanup-lists (wolfram query)))))
(defun find-answer-in-text (text question)
(let* ((nqtext (myutils:replace-all (myutils:replace-all text "\"" " ") "'" " "))
(nqquestion (myutils:replace-all
(myutils:replace-all question "\"" " ") "'" " "))
(query2 (concatenate 'string "FindTextualAnswer['" nqtext
"', '" nqquestion "']"))
(query (myutils:replace-all query2 "'" "\"")))
(wolfram query)))
The last two functions in the last code listing, entities and find-answer-in-text are higher level
functions intended to work with the Wolfram Language procedures TextCases (see Wolfram
documentation for TextCases¹¹⁰) and FindTextualAnswer (see Wolfram documentation for Find-
TextualAnswer¹¹¹).
The functions cleanup-lists and recursive-remove can be used to clean up results. First, we will
just call function wolfram and show the raw results:
¹¹⁰https://reference.wolfram.com/language/ref/TextCases.html
¹¹¹https://reference.wolfram.com/language/ref/FindTextualAnswer.html
Using Common Lisp with Wolfram/One
$ sbcl
* (ql:quickload "wolfram")
To load "wolfram":
Load 1 ASDF system:
wolfram
; Loading "wolfram"
[package myutils].................................
[package wolfram]
("wolfram")
* (setf example "TextCases['NYC, Los Angeles, and Chicago are the largest cities in \
the USA in 2018 according to Pete Wilson.', {'City', 'Country', 'Date', 'Person'} ->\
{'String', 'Interpretation', 'Probability'}]")
"TextCases['NYC, Los Angeles, and Chicago are the largest cities in the USA in 2018 \
according to Pete Wilson.', {'City', 'Country', 'Date', 'Person'} -> {'String', 'Int\
erpretation', 'Probability'}]"
* (setf example-str (myutils:replace-all
example "'" "\""))
"TextCases[\"NYC, Los Angeles, and Chicago are the largest cities in the USA in 2018\
according to Pete Wilson.\", {\"City\", \"Country\", \"Date\", \"Person\"} -> {\"St\
ring\", \"Interpretation\", \"Probability\"}]"
* (setf results (wolfram:wolfram example-str))
* (pprint results)
("Association"
("Rule" "City"
("List"
("List" "NYC" ("Entity" "City" ("List" "NewYork" "NewYork" "UnitedStates"))
0.75583166)
("List" "Los Angeles"
("Entity" "City" ("List" "LosAngeles" "California" "UnitedStates"))
0.84206486)
("List" "Chicago"
("Entity" "City" ("List" "Chicago" "Illinois" "UnitedStates"))
0.91092855)))
("Rule" "Country"
("List" ("List" "USA" ("Entity" "Country" "UnitedStates") 0.9285077)))
("Rule" "Date"
("List"
("List" "2018" ("DateObject" ("List" 2018) "Year" "Gregorian" -7.0)
0.8364356)))
("Rule" "Person"
("List"
("List" "Pete Wilson" ("Entity" "Person" "PeteWilson::s7259") 0.9274548))))
Using Common Lisp with Wolfram/One
Now we clean up the output:
* (defvar results-cleaned (wolfram:cleanup-lists results))
* (pprint results-cleaned)
(("City"
(("NYC" ("Entity" "City" ("NewYork" "NewYork" "UnitedStates")) 0.75583166)
("Los Angeles" ("Entity" "City" ("LosAngeles" "California" "UnitedStates"))
0.84206486)
("Chicago" ("Entity" "City" ("Chicago" "Illinois" "UnitedStates"))
0.91092855)))
("Country" (("USA" ("Entity" "Country" "UnitedStates") 0.9285077)))
("Date" (("2018" ("DateObject" (2018) "Year" "Gregorian" -7.0) 0.8364356)))
("Person" (("Pete Wilson" ("Entity" "Person" "PeteWilson::s7259") 0.9274548))))
Next we will try the two higher-level utility functions. The first example shows finding entities in
CL-USER 21 > (pprint
(wolfram:entities "Sedona Arizona is home to Mark Louis Watson"))
(("City"
(("Sedona" ("Entity" "City" ("Sedona" "Arizona" "UnitedStates")) 0.8392784)))
("Person" (("Mark Louis Watson" "Mark Louis Watson" 0.9023427))))
The second example uses a Wolfram pre-trained deep learning model for question answering:
CL-USER 22 > (pprint
(wolfram::find-answer-in-text "International Business Machines Corpor\
ation (IBM) is an American multinational technology company headquartered in Armonk,\
New York, with operations in over 170 countries. The company began in 1911, founded\
in Endicott, New York, as the Computing-Tabulating-Recording Company (CTR) and was \
renamed \"International Business Machines\" in 1924. IBM is incorporated in New York\
"where is IBM is headquartered?"))
"Armonk, New York"
If you use Wolfram/One then these examples should get you started wrapping other Wolfram
Language functionality for use in your Common Lisp applications.
Book Wrapup
Congratulations for finishing this book!
I love programming in Lisp languages with concise code and a bottom-up approach to development.
I hope you now also share this enthusiasm with me.
Common Lisp is sometimes criticised as not having as many useful libraries as some newer languages
like Python and Java, and this is a valid criticism. That said, I hope the wide variety of examples
in this book will convince you that Common Lisp is a good choice for many types of programming
projects.
I would like to thank you for reading my book and I hope that you enjoyed it. As I mentioned in the
Introduction I have been using Common Lisp since the mid-1980s, and other Lisp dialects for longer
than that. I have always found something almost magical developing in Lisp. Being able to extend
the language with macros and using the development technique of building a mini-language in Lisp
customized for an application enables programmers to be very efficient in their work. I have usually
found that this bottom-up development style helps me deal with software complexity because the
lower level functions tend to get well tested while the overall system being developed is not yet
too complex to fully understand. Later in the development process these lower level functions and
utilities almost become part of the programming language and the higher level application logic is
easier to understand because you have fewer lines of code to fit inside your head during development.
I think that unless a programmer works in very constrained application domains, it often makes sense
to be a polyglot programmer. I have tried, especially in the new material for this fourth edition, to
give you confidence that Common Lisp is good for both general software development language and
also as “glue” to tie different systems together.
Thank you for buying and reading my book!
Mark Watson
